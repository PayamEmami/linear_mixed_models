<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction to Mixed Models - Mixed models (mathematical details)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./lmm.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./math.html">Mixed models (mathematical details)</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Introduction to Mixed Models</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./environment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Environment</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Simple linear regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lmm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mixed models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./math.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Mixed models (mathematical details)</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#ordinary-least-squares-regression" id="toc-ordinary-least-squares-regression" class="nav-link active" data-scroll-target="#ordinary-least-squares-regression">Ordinary Least Squares regression</a>
  <ul class="collapse">
  <li><a href="#mathematical-formulation" id="toc-mathematical-formulation" class="nav-link" data-scroll-target="#mathematical-formulation">Mathematical Formulation</a></li>
  <li><a href="#correlated-residuals" id="toc-correlated-residuals" class="nav-link" data-scroll-target="#correlated-residuals">Correlated residuals</a></li>
  </ul></li>
  <li><a href="#generalized-least-squares-gls" id="toc-generalized-least-squares-gls" class="nav-link" data-scroll-target="#generalized-least-squares-gls">Generalized Least Squares (GLS)</a>
  <ul class="collapse">
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example</a></li>
  <li><a href="#structure-of-v" id="toc-structure-of-v" class="nav-link" data-scroll-target="#structure-of-v">Structure of V</a>
  <ul class="collapse">
  <li><a href="#design-matrix" id="toc-design-matrix" class="nav-link" data-scroll-target="#design-matrix">Design matrix</a></li>
  </ul></li>
  <li><a href="#structure-of-v-cont." id="toc-structure-of-v-cont." class="nav-link" data-scroll-target="#structure-of-v-cont.">Structure of V (cont.)</a></li>
  </ul></li>
  <li><a href="#estimation-of-parameters" id="toc-estimation-of-parameters" class="nav-link" data-scroll-target="#estimation-of-parameters">Estimation of parameters</a>
  <ul class="collapse">
  <li><a href="#bias-of-mle" id="toc-bias-of-mle" class="nav-link" data-scroll-target="#bias-of-mle">Bias of MLE</a></li>
  <li><a href="#restricted-maximum-likelihood-reml" id="toc-restricted-maximum-likelihood-reml" class="nav-link" data-scroll-target="#restricted-maximum-likelihood-reml">Restricted Maximum Likelihood (REML)</a></li>
  <li><a href="#other-methods-of-fitting" id="toc-other-methods-of-fitting" class="nav-link" data-scroll-target="#other-methods-of-fitting">Other methods of fitting</a></li>
  </ul></li>
  <li><a href="#best-linear-unbiased-estimators-blue" id="toc-best-linear-unbiased-estimators-blue" class="nav-link" data-scroll-target="#best-linear-unbiased-estimators-blue">Best Linear Unbiased Estimators (BLUE)</a></li>
  <li><a href="#random-effects" id="toc-random-effects" class="nav-link" data-scroll-target="#random-effects">Random Effects</a></li>
  <li><a href="#final-mixed-model-form" id="toc-final-mixed-model-form" class="nav-link" data-scroll-target="#final-mixed-model-form">Final mixed model form</a></li>
  <li><a href="#p-value-for-mixed-models-satterthwaite-approximation" id="toc-p-value-for-mixed-models-satterthwaite-approximation" class="nav-link" data-scroll-target="#p-value-for-mixed-models-satterthwaite-approximation">p-value for mixed models (Satterthwaite approximation)</a>
  <ul class="collapse">
  <li><a href="#how-rare-is-this-value" id="toc-how-rare-is-this-value" class="nav-link" data-scroll-target="#how-rare-is-this-value">How rare is this value?!</a></li>
  <li><a href="#standard-normal-distribution" id="toc-standard-normal-distribution" class="nav-link" data-scroll-target="#standard-normal-distribution">Standard normal distribution</a></li>
  <li><a href="#putting-it-all-together-t" id="toc-putting-it-all-together-t" class="nav-link" data-scroll-target="#putting-it-all-together-t">Putting it all together (T)</a></li>
  <li><a href="#t-statistics-for-ols" id="toc-t-statistics-for-ols" class="nav-link" data-scroll-target="#t-statistics-for-ols">t-statistics for OLS</a>
  <ul class="collapse">
  <li><a href="#degrees-of-freedom-of-gls" id="toc-degrees-of-freedom-of-gls" class="nav-link" data-scroll-target="#degrees-of-freedom-of-gls">degrees of freedom of GLS</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Mixed models (mathematical details)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In this section we are going to explain some of the key mathematical properties of mixed model. Before we begin please note that as said before mixed models can become fairly complex, with a variety of parametrization techniques that go beyond the scope and time frame of this chapter. Here I assume that the reader has relatively a good understanding of Ordinary Least Squares regression (OLS) as we are going to use that to build mixed models.</p>
<section id="ordinary-least-squares-regression" class="level1">
<h1>Ordinary Least Squares regression</h1>
<p>Ordinary Least Squares (OLS) is a fundamental method for linear regression analysis. It aims to find the line (or hyperplane) that best fits a set of data points by minimizing the sum of the squared differences (residuals) between observed values and the values predicted by the linear model.</p>
<section id="mathematical-formulation" class="level2">
<h2 class="anchored" data-anchor-id="mathematical-formulation">Mathematical Formulation</h2>
<p>Consider a dataset with <span class="math inline">\(n\)</span> observations and <span class="math inline">\(p\)</span> predictors. The model can be expressed as:</p>
<p><span class="math display">\[
y = X\beta + \epsilon
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(y\)</span> is an <span class="math inline">\(n \times 1\)</span> vector of observed values.</li>
<li><span class="math inline">\(X\)</span> is an <span class="math inline">\(n \times p\)</span> matrix of predictor variables, often augmented with a column of ones to include the intercept in the model.</li>
<li><span class="math inline">\(\beta\)</span> is a <span class="math inline">\(p \times 1\)</span> vector of coefficients that we want to estimate.</li>
<li><span class="math inline">\(\epsilon\)</span> is an <span class="math inline">\(n \times 1\)</span> vector of the model’s residuals or errors.</li>
</ul>
<p>The objective of OLS is to minimize the sum of squared residuals, given by:</p>
<p><span class="math display">\[
S(\beta) = (y - X\beta)^\top (y - X\beta)
\]</span></p>
<p>To find the value of <span class="math inline">\(\beta\)</span> that minimizes <span class="math inline">\(S(\beta)\)</span>, set the gradient of <span class="math inline">\(S\)</span> with respect to <span class="math inline">\(\beta\)</span> to zero. This yields the normal equations:</p>
<p><span class="math display">\[
X^\top X \beta = X^\top y
\]</span></p>
<p>Assuming <span class="math inline">\(X^\top X\)</span> is non-singular (invertible), the solution for <span class="math inline">\(\beta\)</span> is:</p>
<p><span class="math display">\[
\beta = (X^\top X)^{-1} X^\top y
\]</span></p>
<p>This solution provides the best linear unbiased estimates of the coefficients under the <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">Gauss-Markov theorem</a>, provided the residuals <span class="math inline">\(\epsilon\)</span> are uncorrelated with constant variance (homoscedasticity) and mean zero.</p>
<p>OLS has some highly desired statistical properties:</p>
<ul>
<li><strong>Unbiasedness:</strong> <span class="math inline">\(E(\beta) = \beta\)</span>, meaning that the expected value of the OLS estimator equals the true parameter value.</li>
<li><strong>Efficiency:</strong> Under the assumptions of the Gauss-Markov theorem, the OLS estimator has the smallest variance among all linear unbiased estimators.</li>
<li><strong>Consistency:</strong> As the sample size <span class="math inline">\(n\)</span> approaches infinity, the OLS estimator converges in probability to the true parameter value, assuming the predictors <span class="math inline">\(X\)</span> are non-stochastic.</li>
</ul>
</section>
<section id="correlated-residuals" class="level2">
<h2 class="anchored" data-anchor-id="correlated-residuals">Correlated residuals</h2>
<p>The classical OLS method is designed under the assumption that the residuals <span class="math inline">\(\epsilon\)</span> are uncorrelated and homoscedastic. However, when these assumptions are violated, particularly in the presence of correlated residuals, OLS estimates remain unbiased but lose efficiency, meaning they no longer have the smallest possible variance among all linear unbiased estimators.</p>
<p>Consider the linear model: <span class="math display">\[
y = X\beta + \epsilon, \quad \text{where} \quad \epsilon \sim N(0, \sigma^2 V)
\]</span></p>
<p>Assuming that the residuals are not correlated, <span class="math inline">\(V\)</span> is an identity (The diagonal entries are 1, the rest are zero). However, when the correlation exist we are dealing with a more complex covariance matrix <span class="math inline">\(V\)</span>. The presence of correlated residuals necessitates a transformation of the model so that the assumptions of OLS are effectively restored. This transformation can be achieved through the use of a matrix <span class="math inline">\(C\)</span>, such that when applied to the residuals, results in uncorrelated residuals with uniform variance.</p>
<p>So formally what we are interested in is:</p>
<p><span class="math display">\[
\text{Cov}(C\epsilon) =  \sigma^2 I
\]</span> So let’s start working with <span class="math inline">\(\text{Cov}(C\epsilon)\)</span>. Please remember that the mean of the residuals are assumed to be zero. Therefore we can write the covariance equation as <span class="math display">\[
\text{Cov}(C\epsilon) = C\epsilon(C\epsilon)^T
\]</span> If we rewrite this and apply the transpose we end up with</p>
<p><span class="math display">\[
\text{Cov}(C\epsilon) = C\epsilon\epsilon^TC^T
\]</span> As we said the original covariance of <span class="math inline">\(\epsilon\)</span> is <span class="math inline">\(\sigma^2 V\)</span> so we can replace <span class="math inline">\(\epsilon\epsilon^T\)</span> by <span class="math inline">\(\sigma^2 V\)</span>:</p>
<p><span class="math display">\[
\text{Cov}(C\epsilon) = C\sigma^2 VC^T = \sigma^2 I
\]</span> If we define both side by <span class="math inline">\(\sigma^2\)</span></p>
<p><span class="math display">\[
C VC^T =  I
\]</span> We assume that C exhibits complex conjugate symmetry then</p>
<p><span class="math display">\[
\text{Cov}(C\epsilon) = C VC
\]</span></p>
<p>We take <span class="math inline">\(C\)</span> to the other side of the equation:</p>
<p><span class="math display">\[
CV= C^{-1}
\]</span> We do the same thing for the other <span class="math inline">\(C\)</span>:</p>
<p><span class="math display">\[
V= C^{-1}C^{-1}
\]</span> Multiply two <span class="math inline">\(C^{-1}\)</span>:</p>
<p><span class="math display">\[
V= C^{-2}
\]</span> Take the inverse of both side <span class="math display">\[
V^{-1}= C^{2}
\]</span> and finally we take the square root of both side:</p>
<p><span class="math display">\[
C=V^{-1/2}
\]</span> So let’s put this back in the original equation <span class="math display">\[
\text{Cov}(C\epsilon) = C\sigma^2 VC^T = V^{-1/2}\sigma^2 VV^{-1/2} = \sigma^2 I
\]</span> Now that we have our transformation matrix <span class="math inline">\(V^{-1/2}\)</span>, we should apply it to the OLS equation. In order to effectively “whitens” the errors we are going to premultiply OLS equation with the transformation <span class="math inline">\(V^{-1/2}\)</span>:</p>
<p><span class="math display">\[
V^{-1/2} y = V^{-1/2} X \beta + V^{-1/2} \epsilon
\]</span></p>
<p>With the transformed model <span class="math inline">\(V^{-1/2} y = V^{-1/2} X \beta + V^{-1/2} \epsilon\)</span> and the transformed errors having covariance <span class="math inline">\(I\)</span>, we can now apply OLS directly:</p>
<p><span class="math display">\[
\beta_{\text{transformed OLS}} = \left( (V^{-1/2} X)^\top (V^{-1/2} X) \right)^{-1} (V^{-1/2} X)^\top (V^{-1/2} y)
\]</span></p>
<p>This is equivalent to estimating beta for Generalized Least Squares (GLS):</p>
<p><span class="math display">\[
\beta_{\text{GLS}} = (X^\top V^{-1} X)^{-1} X^\top V^{-1} y
\]</span> with the following minimization problem:</p>
<p><span class="math display">\[
\min_{\beta} (y - X\beta)^\top V^{-1} (y - X\beta)
\]</span> We can also try to verify our dervitation of beta by expanding the quadratic form: <span class="math inline">\((y - X\beta)^\top V^{-1} (y - X\beta) = y^\top V^{-1} y - \beta^\top X^\top V^{-1} y - y^\top V^{-1} X \beta + \beta^\top X^\top V^{-1} X \beta\)</span></p>
<p>Taking the derivative of this expression with respect to <span class="math inline">\(\beta\)</span> and setting it to zero gives: <span class="math display">\[
-2X^\top V^{-1} y + 2X^\top V^{-1} X \beta = 0
\]</span></p>
<p><span class="math display">\[
X^\top V^{-1} X \beta = X^\top V^{-1} y
\]</span></p>
<p>Solving for <span class="math inline">\(\beta\)</span> yields: <span class="math display">\[
\beta = (X^\top V^{-1} X)^{-1} X^\top V^{-1} y
\]</span> This is again the GLS estimator, confirming that it minimizes the quadratic form.</p>
</section>
</section>
<section id="generalized-least-squares-gls" class="level1">
<h1>Generalized Least Squares (GLS)</h1>
<p>Generalized Least Squares (GLS) is a statistical technique used to estimate the unknown parameters in a linear regression model when the error terms exhibit heteroscedasticity (varying variance) or are correlated. Unlike ordinary least squares (OLS) which assumes that the error terms are homoscedastic (constant variance) and uncorrelated, GLS adjusts for these irregularities by pre-multiplying the model by the inverse square root of the covariance matrix of the errors, <span class="math inline">\(V^{-1/2}\)</span>, or directly incorporating the inverse of the covariance matrix, <span class="math inline">\(V^{-1}\)</span>, into the estimation process. This transformation normalizes the errors, leading to more efficient and unbiased estimates even in the presence of non-ideal error structures. GLS is particularly useful in time series analysis, panel data models, and other contexts where observations are not independent.</p>
<section id="example" class="level2">
<h2 class="anchored" data-anchor-id="example">Example</h2>
<p>Before going forward we want to try whether the equation we derived actually work on not.</p>
<p>Let’s get back to our previous example:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>simulate_grouped_trend <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">group_count =</span> <span class="dv">5</span>, <span class="at">points_per_group =</span> <span class="dv">10</span>, <span class="at">global_slope =</span> <span class="sc">-</span><span class="dv">10</span>, <span class="at">global_intercept =</span> <span class="dv">30</span>, <span class="at">group_slope =</span> <span class="dv">2</span>, <span class="at">noise_sd =</span> <span class="dv">50</span>,<span class="at">noise_sd2=</span><span class="dv">2</span>) {</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co"># Setting a seed for reproducibility</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Initialize an empty data frame to store the simulated data</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">numeric</span>(), <span class="at">y =</span> <span class="fu">numeric</span>())</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Loop to create each group</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>group_count) {</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    x_start <span class="ot">&lt;-</span> <span class="dv">12</span> <span class="sc">+</span> (i <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> (<span class="dv">10</span> <span class="sc">/</span> group_count) <span class="co"># Stagger the start of x for each group</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> <span class="fu">runif</span>(points_per_group, <span class="at">min =</span> x_start, <span class="at">max =</span> x_start <span class="sc">+</span> (<span class="dv">10</span> <span class="sc">/</span> group_count))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply a local positive trend within the group, but maintain the global negative trend</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    local_intercept <span class="ot">&lt;-</span> global_intercept <span class="sc">+</span> global_slope <span class="sc">*</span> (x_start <span class="sc">+</span> (<span class="dv">10</span> <span class="sc">/</span> (<span class="dv">2</span> <span class="sc">*</span> group_count))) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> noise_sd)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> local_intercept <span class="sc">+</span> group_slope[i] <span class="sc">*</span> (x <span class="sc">-</span> x_start) <span class="sc">+</span> <span class="fu">rnorm</span>(points_per_group, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> noise_sd2)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combine this group with the overall dataset</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    group_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y,<span class="at">group=</span>i)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    data <span class="ot">&lt;-</span> <span class="fu">rbind</span>(data, group_data)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(data)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># generate simulated data</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>data_int <span class="ot">&lt;-</span> <span class="fu">simulate_grouped_trend</span>(<span class="at">group_count =</span> <span class="dv">4</span>,<span class="at">points_per_group =</span> <span class="dv">10</span>,<span class="at">global_slope =</span> <span class="sc">-</span><span class="dv">2</span>,<span class="at">global_intercept =</span> <span class="dv">100</span>,<span class="at">group_slope =</span> <span class="fu">c</span>(<span class="dv">6</span>,<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">1</span>),<span class="at">noise_sd =</span> <span class="dv">5</span>,<span class="at">noise_sd2=</span><span class="dv">2</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># set group to factor</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>data_int<span class="sc">$</span>group <span class="ot">&lt;-</span> <span class="fu">factor</span>(data_int<span class="sc">$</span>group)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the data</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(data_int<span class="sc">$</span>x,data_int<span class="sc">$</span>y,<span class="at">xlab=</span><span class="st">"Age"</span>,<span class="at">ylab=</span><span class="st">"Protein expression"</span>,<span class="at">col=</span>data_int<span class="sc">$</span>group,<span class="at">pch=</span><span class="fu">as.numeric</span>(data_int<span class="sc">$</span>group))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-scatter-plot" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scatter-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="math_files/figure-html/fig-scatter-plot-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scatter-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: A scatter plot of Age vs.&nbsp;Protein expression
</figcaption>
</figure>
</div>
</div>
</div>
<p>Let’s say that we know the structure of V using some prior knowledge (we will cover how V is exactly derived later).</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a diagonal matrix 'var.d' with equal diagonal elements set to 378.6062 (covariance). The dimensions are based on the number of levels in 'data_int$group'.</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>var.d <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="fl">378.6062</span>, <span class="at">ncol =</span> <span class="fu">length</span>(<span class="fu">levels</span>(data_int<span class="sc">$</span>group)), <span class="at">nrow=</span><span class="fu">length</span>(<span class="fu">levels</span>(data_int<span class="sc">$</span>group)))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Transpose of the model matrix for the 'group' variable in the dataset 'data_int', excluding intercept (hence '~0+').</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>Zt <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">model.matrix</span>(<span class="sc">~</span><span class="dv">0</span><span class="sc">+</span>data_int<span class="sc">$</span>group))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the variance-covariance matrix 'V' for the random effects using the matrix 'Zt' and the variance diagonal matrix 'var.d'.</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>V <span class="ot">&lt;-</span> (<span class="fu">t</span>(Zt) <span class="sc">%*%</span> var.d <span class="sc">%*%</span> Zt)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the diagonal elements of the variance-covariance matrix 'V' to 384.5376 (total variance).</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="fu">diag</span>(V) <span class="ot">&lt;-</span> <span class="fl">384.5376</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct the model matrix 'X' for the fixed effect 'x' using data from 'data_int'.</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span>x, <span class="at">data=</span>data_int)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the response variable 'y' from 'data_int'.</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> data_int<span class="sc">$</span>y</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the mixed model solution using generalized least squares, solving the equation (X'V^-1X)^-1X'V^-1y for the fixed effects estimates.</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                 [,1]
(Intercept) 29.910468
x            2.556151</code></pre>
</div>
</div>
<p>Let’s compare this to when we use mixed model:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lme4)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>model_lmm <span class="ot">&lt;-</span> <span class="fu">lmer</span>(y <span class="sc">~</span> <span class="dv">1</span><span class="sc">+</span>x<span class="sc">+</span>(<span class="dv">1</span><span class="sc">|</span>group), <span class="at">data =</span> data_int)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(model_lmm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: y ~ 1 + x + (1 | group)
   Data: data_int
REML criterion at convergence: 201.3982
Random effects:
 Groups   Name        Std.Dev.
 group    (Intercept) 19.458  
 Residual              2.435  
Number of obs: 40, groups:  group, 4
Fixed Effects:
(Intercept)            x  
     29.911        2.556  </code></pre>
</div>
</div>
<p>We have calculated the exact same coefficients using both approaches. We can now see how <span class="math inline">\(V\)</span> changes our data using <span class="math inline">\(V^{-1/2} y = V^{-1/2} X \beta + V^{-1/2} \epsilon\)</span>. But before that we need to cover another small algebraic operation <em>Square root of a matrix</em> or specifically how to calculate <em>V^{-1/2}</em>.</p>
<p>Assuming <span class="math inline">\(V\)</span> is a symmetric matrix, it can be decomposed into: <span class="math display">\[
V = P \Lambda P^T
\]</span> where:</p>
<ul>
<li><span class="math inline">\(P\)</span> is an orthogonal matrix whose columns are the eigenvectors of <span class="math inline">\(V\)</span>.</li>
<li><span class="math inline">\(\Lambda\)</span> is a diagonal matrix whose diagonal elements are the eigenvalues of <span class="math inline">\(V\)</span>, denoted as <span class="math inline">\(\lambda_i\)</span> (i.e., <span class="math inline">\(\Lambda = \text{diag}(\lambda_1, \lambda_2, ..., \lambda_n)\)</span>).</li>
</ul>
<p>For <span class="math inline">\(V\)</span> to have a real square root, all its eigenvalues should be non-negative (<span class="math inline">\(\lambda_i \geq 0\)</span> for all <span class="math inline">\(i\)</span>). This is because the square root of a negative number is not defined in the real numbers, which would lead to complex values.</p>
<p>The square root of the diagonal matrix <span class="math inline">\(\Lambda\)</span> is straightforward to compute. The square root of a diagonal matrix is another diagonal matrix <span class="math inline">\(\Lambda^{1/2}\)</span> with diagonal entries equal to the square roots of the diagonal entries of <span class="math inline">\(\Lambda\)</span>. Thus: <span class="math display">\[
\Lambda^{1/2} = \text{diag}(\sqrt{\lambda_1}, \sqrt{\lambda_2}, ..., \sqrt{\lambda_n})
\]</span></p>
<p>The square root of the matrix <span class="math inline">\(V\)</span>, denoted <span class="math inline">\(V^{1/2}\)</span>, can now be constructed using the eigenvectors and the square root of the eigenvalues: <span class="math display">\[
V^{1/2} = P \Lambda^{1/2} P^T
\]</span> This expression utilizes the original matrix of eigenvectors <span class="math inline">\(P\)</span>, the square root of the eigenvalue matrix <span class="math inline">\(\Lambda^{1/2}\)</span>, and the transpose of <span class="math inline">\(P\)</span>.</p>
<p>To verify that <span class="math inline">\(V^{1/2}\)</span> is the square root of <span class="math inline">\(V\)</span>, compute <span class="math inline">\(V^{1/2} V^{1/2}\)</span> and check that it equals <span class="math inline">\(A\)</span>: <span class="math display">\[
V^{1/2} V^{1/2} = (P \Lambda^{1/2} P^T) (P \Lambda^{1/2} P^T)
\]</span> Using the orthogonality of <span class="math inline">\(P\)</span> (i.e., <span class="math inline">\(P^T P = I\)</span> where <span class="math inline">\(I\)</span> is the identity matrix):</p>
<p><span class="math display">\[
V^{1/2} V^{1/2} = P \Lambda^{1/2} (P^T P) \Lambda^{1/2} P^T = P \Lambda^{1/2} \Lambda^{1/2} P^T = P \Lambda P^T = V
\]</span> So, <span class="math inline">\(V^{1/2}\)</span> is the square root of <span class="math inline">\(V\)</span>.</p>
<p>Let’s create a small function in R to calculate the square root of a matrix:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>matrix_sqrt <span class="ot">&lt;-</span> <span class="cf">function</span>(Sigma) {</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Ensure the matrix is symmetric</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="sc">!</span><span class="fu">all.equal</span>(Sigma, <span class="fu">t</span>(Sigma))) {</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">stop</span>(<span class="st">"The input matrix must be symmetric."</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute the eigenvalues and eigenvectors</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  eigen_decomp <span class="ot">&lt;-</span> <span class="fu">eigen</span>(Sigma)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Extract eigenvalues and eigenvectors</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  eigenvalues <span class="ot">&lt;-</span> eigen_decomp<span class="sc">$</span>values</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>  eigenvectors <span class="ot">&lt;-</span> eigen_decomp<span class="sc">$</span>vectors</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Check for negative eigenvalues</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">any</span>(eigenvalues <span class="sc">&lt;</span> <span class="dv">0</span>)) {</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">stop</span>(<span class="st">"The matrix must be positive definite."</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute Lambda^1/2</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>  Lambda_half <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(eigenvalues))</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute Sigma^1/2</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>  Sigma_half <span class="ot">&lt;-</span> eigenvectors <span class="sc">%*%</span> Lambda_half <span class="sc">%*%</span> <span class="fu">t</span>(eigenvectors)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">Sigma_half=</span>Sigma_half,<span class="at">eigenvectors=</span>eigenvectors,<span class="at">eigenvalues=</span>eigenvalues))</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now what we are going to do is to transform <span class="math inline">\(X\)</span> (the design matrix) and <span class="math inline">\(y\)</span> (our response) using the <span class="math inline">\(V^{1/2}\)</span> and only perform OLS on it:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>V_inv_sqrt <span class="ot">&lt;-</span> <span class="fu">matrix_sqrt</span>(V)<span class="sc">$</span>Sigma_half</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>X_transformed <span class="ot">&lt;-</span> V_inv_sqrt<span class="sc">%*%</span>X</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>y_transformed <span class="ot">&lt;-</span> V_inv_sqrt<span class="sc">%*%</span>y</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">lm</span>(y_transformed<span class="sc">~</span><span class="dv">0</span><span class="sc">+</span>X_transformed))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y_transformed ~ 0 + X_transformed)

Coefficients:
X_transformed(Intercept)            X_transformedx  
                  29.910                     2.556  </code></pre>
</div>
</div>
<p>You see that this again gave us the exact same coefficients as mixed model.</p>
<p>Let’s look at the original data and compare it with the transformed one:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X[,<span class="dv">2</span>],y,<span class="at">col=</span>data_int<span class="sc">$</span>group,<span class="at">main=</span><span class="st">"Original data"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(model_lmm<span class="sc">@</span>beta)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X_transformed[,<span class="dv">2</span>],y_transformed,<span class="at">col=</span>data_int<span class="sc">$</span>group,<span class="at">main=</span><span class="st">"Transformed data"</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="at">x=</span>X_transformed[,<span class="dv">2</span>],<span class="at">y=</span><span class="fu">predict</span>(<span class="fu">lm</span>(y_transformed<span class="sc">~</span><span class="dv">0</span><span class="sc">+</span>X_transformed)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-scatter-plot-transformed" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scatter-plot-transformed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="math_files/figure-html/fig-scatter-plot-transformed-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scatter-plot-transformed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: A scatter plot of of transformed X and Y
</figcaption>
</figure>
</div>
</div>
</div>
<p>It is clear that the difference between groups has been removed in the transformed data. We can appreciate this procedure more if we try to increase the variance between the groups and see what happens to the transformation:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the model</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gganimate)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>global_intercept_slop <span class="ot">&lt;-</span> model_lmm<span class="sc">@</span>beta</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>max_cov <span class="ot">&lt;-</span> <span class="fl">378.6062</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>total_variance <span class="ot">&lt;-</span> <span class="fl">384.5376</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Animation Data Preparation</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (co <span class="cf">in</span> <span class="fu">seq</span>(<span class="dv">0</span>, max_cov, <span class="at">length.out =</span> <span class="dv">20</span>)) {</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>  var.d <span class="ot">&lt;-</span> <span class="fu">diag</span>(co, <span class="at">ncol =</span> <span class="fu">length</span>(<span class="fu">levels</span>(data_int<span class="sc">$</span>group)), <span class="at">nrow =</span> <span class="fu">length</span>(<span class="fu">levels</span>(data_int<span class="sc">$</span>group)))</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>  Zt <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">model.matrix</span>(<span class="sc">~</span><span class="dv">0</span> <span class="sc">+</span> data_int<span class="sc">$</span>group))</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>  V <span class="ot">&lt;-</span> (<span class="fu">t</span>(Zt) <span class="sc">%*%</span> var.d <span class="sc">%*%</span> Zt)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">diag</span>(V) <span class="ot">&lt;-</span> total_variance</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>  V_inv_sqrt <span class="ot">&lt;-</span> <span class="fu">matrix_sqrt</span>(V)<span class="sc">$</span>Sigma_half</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span>x, <span class="at">data =</span> data_int)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> data_int<span class="sc">$</span>y</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>  X_transformed <span class="ot">&lt;-</span> V_inv_sqrt <span class="sc">%*%</span> X</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>  y_transformed <span class="ot">&lt;-</span> V_inv_sqrt <span class="sc">%*%</span> y</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>  transformed_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">X_transformed =</span> X_transformed[, <span class="dv">2</span>], y_transformed, <span class="at">Group =</span> data_int<span class="sc">$</span>group, <span class="at">co =</span> co)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>  results[[<span class="fu">length</span>(results) <span class="sc">+</span> <span class="dv">1</span>]] <span class="ot">&lt;-</span> transformed_data</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine all data frames for animation</span></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>animation_data <span class="ot">&lt;-</span> <span class="fu">do.call</span>(rbind, results)</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the animation using ggplot2 and gganimate</span></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(animation_data, <span class="fu">aes</span>(<span class="at">x =</span> X_transformed, <span class="at">y =</span> y_transformed, <span class="at">color =</span> Group)) <span class="sc">+</span></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()<span class="sc">+</span> <span class="fu">stat_ellipse</span>(<span class="fu">aes</span>(<span class="at">x=</span>X_transformed, <span class="at">y=</span>y_transformed,<span class="at">color=</span>Group),<span class="at">type =</span> <span class="st">"norm"</span>)<span class="sc">+</span></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Variance scaling factor: {closest_state}"</span>, <span class="at">x =</span> <span class="st">"Transformed X"</span>, <span class="at">y =</span> <span class="st">"Transformed Y"</span>) <span class="sc">+</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transition_states</span>(co, <span class="at">transition_length =</span> <span class="dv">2</span>, <span class="at">state_length =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ease_aes</span>(<span class="st">'linear'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-scatter-plot-transformed-animation1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scatter-plot-transformed-animation1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="math_files/figure-html/fig-scatter-plot-transformed-animation1-1.gif" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scatter-plot-transformed-animation1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: A scatter plot of of transformed X and Y
</figcaption>
</figure>
</div>
</div>
</div>
<p>By now it should be clear that as the variance across the group is low that transformation does basically nothing (because there is nothing to correct for) but as we increase this variance we keep rescaling and streching the data so that they are aligned and therefore the differences between the groups are minimized.</p>
<p>So this transformation ensures that each dimension (principal component) of the space now has equal variance (specifically, variance of 1), removing the effect of the original variances encoded by <span class="math inline">\(V\)</span>. If the components of the data were correlated (as indicated by the off-diagonal elements of <span class="math inline">\(V\)</span>), the transformation effectively decorrelates these components. In the new space, the covariance between any pair of different components of <span class="math inline">\(V^{-1/2} data\)</span> is zero. Thus, geometrically, <span class="math inline">\(V^{-1/2} data\)</span> transforms the data into a form where the scale differences and correlations introduced by <span class="math inline">\(V\)</span> are neutralized, making the transformed data suitable for analysis under standard assumptions of independence and uniform scale.</p>
<p>The last thing to note here is that here we are dealing with a random intercept model, the covariance matrix <span class="math inline">\(V\)</span> used for the transformation typically involves scaling and centering the group data without any rotation. This is because the model structure we are using doesn’t incorporate random slopes or other types of covariance structures that might induce rotation. Instead, it predominantly influences the scaling and translation of the groups in our dataset.</p>
<p>In a random intercept model, each group has its own intercept but shares the slope (if any), leading to vertical shifts in the data but maintaining the same orientation (no rotation) regarding the predictor axis. So geometrically:</p>
<ol type="1">
<li><p><strong>Scaling</strong>: As we increase the variance parameter (<code>co</code> in our model), the amount of scaling increases. This scaling affects how tightly or loosely the data points within each group are clustered around their group mean.</p></li>
<li><p><strong>Centering</strong>: The transformation tends to draw group data towards a central axis or point because each group’s variance contributes to pulling the group’s mean towards the overall mean of the dataset.</p></li>
</ol>
<p>In terms of matrix operations: - The matrix <span class="math inline">\(V\)</span> in this case is primarily influencing the variance within each group and does not introduce any off-diagonal terms that would typically cause a rotation. Rotation in the data space would occur if there were covariances between different dimensions (such as would be introduced with random slopes), which would manifest as off-diagonal elements in <span class="math inline">\(V\)</span>.</p>
<p>In order to true see what the transformation does, we now incorporate a random slope and show it affects the transformation here.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the model</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>model_lmm <span class="ot">&lt;-</span> <span class="fu">lmer</span>(y <span class="sc">~</span> <span class="dv">1</span><span class="sc">+</span>x<span class="sc">+</span>(<span class="dv">1</span><span class="sc">+</span>x<span class="sc">|</span>group), <span class="at">data =</span> data_int)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># custome function to extract the V matrix</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>getV1 <span class="ot">&lt;-</span> <span class="cf">function</span>(x,<span class="at">var1=</span><span class="fl">344.65503</span>,<span class="at">var2=</span><span class="fl">5.141458</span>,<span class="at">cov=</span><span class="sc">-</span><span class="fl">39.26625</span>,<span class="at">times=</span><span class="dv">4</span>){</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    Zt <span class="ot">&lt;-</span> <span class="fu">getME</span>(x, <span class="st">"Zt"</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>  vr <span class="ot">&lt;-</span> <span class="fu">sigma</span>(x)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>  original_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(var1, cov, cov, var2), <span class="at">nrow=</span><span class="dv">2</span>, <span class="at">byrow=</span><span class="cn">TRUE</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>var.d <span class="ot">&lt;-</span> <span class="fu">bdiag</span>(<span class="fu">rep</span>(<span class="fu">list</span>(original_matrix),times))</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>  var.b <span class="ot">&lt;-</span>(<span class="fu">t</span>(Zt) <span class="sc">%*%</span> var.d <span class="sc">%*%</span> Zt)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>  sI <span class="ot">&lt;-</span> vr <span class="sc">*</span> Matrix<span class="sc">::</span><span class="fu">Diagonal</span>(<span class="fu">nobs</span>(x)) <span class="co">#for a sparse matrix</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>  var.y <span class="ot">&lt;-</span> var.b <span class="sc">+</span> sI</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Animation Data Preparation</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>varr1<span class="ot">&lt;-</span><span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">344.65503</span>, <span class="at">length.out =</span> <span class="dv">20</span>)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>varr2<span class="ot">&lt;-</span><span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">5.141458</span>, <span class="at">length.out =</span> <span class="dv">20</span>)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>corr<span class="ot">&lt;-</span><span class="fu">seq</span>(<span class="dv">0</span>, <span class="sc">-</span><span class="fl">39.26625</span>, <span class="at">length.out =</span> <span class="dv">20</span>)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>) {</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>  V<span class="ot">&lt;-</span><span class="fu">getV1</span>(<span class="at">x =</span> model_lmm,<span class="at">var2 =</span> varr2[ii],<span class="at">cov =</span> corr[ii],<span class="at">var1 =</span> varr1[<span class="dv">20</span>])</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>  V_inv_sqrt <span class="ot">&lt;-</span> <span class="fu">matrix_sqrt</span>(V)<span class="sc">$</span>Sigma_half</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span>x, <span class="at">data =</span> data_int)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> data_int<span class="sc">$</span>y</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>  X_transformed <span class="ot">&lt;-</span> V_inv_sqrt <span class="sc">%*%</span> X</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>  y_transformed <span class="ot">&lt;-</span> V_inv_sqrt <span class="sc">%*%</span> y</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>  transformed_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">X_transformed =</span> X_transformed[, <span class="dv">2</span>], y_transformed, <span class="at">Group =</span> data_int<span class="sc">$</span>group, <span class="at">co =</span> varr2[ii])</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>  results[[<span class="fu">length</span>(results) <span class="sc">+</span> <span class="dv">1</span>]] <span class="ot">&lt;-</span> transformed_data</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>  V<span class="ot">&lt;-</span><span class="fu">getV1</span>(<span class="at">x =</span> model_lmm,<span class="at">var2 =</span> varr2[<span class="dv">20</span>],<span class="at">cov =</span> corr[<span class="dv">20</span>],<span class="at">var1 =</span> varr1[<span class="dv">20</span>])</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>  V_inv_sqrt <span class="ot">&lt;-</span> <span class="fu">matrix_sqrt</span>(V)<span class="sc">$</span>Sigma_half</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span>x, <span class="at">data =</span> data_int)</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> data_int<span class="sc">$</span>y</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>  X_transformed <span class="ot">&lt;-</span> V_inv_sqrt <span class="sc">%*%</span> X</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>  y_transformed <span class="ot">&lt;-</span> V_inv_sqrt <span class="sc">%*%</span> y</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>  transformed_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">X_transformed =</span> X_transformed[, <span class="dv">2</span>], y_transformed, <span class="at">Group =</span> data_int<span class="sc">$</span>group, <span class="at">co =</span> varr2[ii])</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine all data frames for animation</span></span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>animation_data <span class="ot">&lt;-</span> <span class="fu">do.call</span>(rbind, results)</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(animation_data, <span class="fu">aes</span>(<span class="at">x =</span> X_transformed, <span class="at">y =</span> y_transformed, <span class="at">color =</span> Group)) <span class="sc">+</span></span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()<span class="sc">+</span> <span class="fu">stat_ellipse</span>(<span class="fu">aes</span>(<span class="at">x=</span>X_transformed, <span class="at">y=</span>y_transformed,<span class="at">color=</span>Group),<span class="at">type =</span> <span class="st">"norm"</span>)<span class="sc">+</span></span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Variance of slope scaling factor: {closest_state}"</span>, <span class="at">x =</span> <span class="st">"Transformed X"</span>, <span class="at">y =</span> <span class="st">"Transformed Y"</span>) <span class="sc">+</span></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transition_states</span>(co, <span class="at">transition_length =</span> <span class="dv">1</span>, <span class="at">state_length =</span> <span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ease_aes</span>(<span class="st">'linear'</span>)<span class="sc">+</span><span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>,<span class="at">se =</span> F)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-scatter-plot-transformed-animation2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scatter-plot-transformed-animation2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="math_files/figure-html/fig-scatter-plot-transformed-animation2-1.gif" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scatter-plot-transformed-animation2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.4: A scatter plot of of transformed X and Y
</figcaption>
</figure>
</div>
</div>
</div>
<p>In the plot above, i have already applied the random intercept transformation to make it a bit easier to see the rotation.</p>
<p>The slope transformation basically draws the overall slope towards the global one and aligns different groups.</p>
<p>So so summarize <span class="math inline">\(V\)</span> in GLS transforms the data into a form where the scale differences and correlations introduced by grouping structure (or any other known structure) are neutralized, making the transformed data suitable for analysis under standard assumptions of independence and uniform scale.</p>
</section>
<section id="structure-of-v" class="level2">
<h2 class="anchored" data-anchor-id="structure-of-v">Structure of V</h2>
<p>I want to remind you here is that the purpose of matrix <span class="math inline">\(V\)</span> is to encode the correlation structure within each group in the data. Given this, as we said before, in an OLS model, it is typically assumed that the residuals are independently and identically distributed (i.i.d.) with a variance of <span class="math inline">\(\sigma^2\)</span>. This again implies that the covariance matrix of the residuals is: <span class="math display">\[
\text{Cov}(\epsilon) =  \sigma^2 I
\]</span> where <span class="math inline">\(\mathbf{I}\)</span> is the identity matrix, indicating no correlation between residuals of different observations.</p>
<p>Suppose we now believe that there is an underlying group structure that impacts the variance of the observations. This is a very important statement. If we don’t believe or know that there is a grouping structure in our experiment as prior knowledge, then our model has the total residual variance <span class="math inline">\(\sigma^2\)</span>. In this context even if the true covariance matrix of the residuals is not diagonal, we will have no way of addressing this simply because we don’t know what grouping structure has caused the dependencies between the residuals. So from now on, we are going to assume that we know there is a grouping in our data so that the difference between these groups whether it is on the slope or intercept adds an additional variance component. For simplicity we will be only talking about random intercept from now but the concept will apply to any sort of structure we decide to add to the model.</p>
<p>So let’ say that we think there is a grouping structure where each group has its specific baseline level of <span class="math inline">\(y\)</span>. The differences between these baselines adds an additional variance component <span class="math inline">\(\sigma_d^2\)</span> that reflects the variability across the groups beyond what is captured by the base residual variance <span class="math inline">\(\sigma^2\)</span>. As a result we our total variance is now <span class="math inline">\(\sigma_d^2+\sigma^2\)</span>. In this case, <span class="math inline">\(\sigma^2\)</span> is now the variance of residuals after accounting for grouping structure and <span class="math inline">\(\sigma_d^2\)</span> is the variance of between the groups.</p>
<p>It should be obvious to see that the relation between between <span class="math inline">\(\sigma_d^2\)</span> and <span class="math inline">\(\sigma^2\)</span> should effectively reflect how tightly the data points are clustered within each group. If the variance within groups (i.e., <span class="math inline">\(\sigma^2\)</span>) is relatively small compared to the variance between groups (i.e., <span class="math inline">\(\sigma_d^2\)</span>), it indicates that the groups are distinctly different from each other, and that the grouping structure is a significant factor in explaining the overall variability in the data. This scenario would suggest strong clustering within groups, meaning that the observations within each group are similar to each other but differ significantly from observations in other groups. Conversely, if <span class="math inline">\(\sigma^2\)</span> is large relative to <span class="math inline">\(\sigma_d^2\)</span>, it suggests that the within-group differences are significant, and the grouping structure might not be as influential in explaining the variance in the data.</p>
<p>This relationship helps us define intra-class correlation coefficient (ICC). <span class="math display">\[
\text{ICC} = \frac{\sigma_d^2}{\sigma_d^2 + \sigma^2}
\]</span></p>
<p>In our case ICC describes the degree to which units within the same group resemble each other. It is considered a type of correlation, but it applies to data organized into groups rather than to data consisting of paired observations.</p>
<p>The ICC formula calculates the proportion of total variance (<span class="math inline">\(\sigma_d^2 + \sigma^2\)</span>) that is due to differences between groups. It essentially measures the ratio of between-group variance to the total variance in the data.</p>
<ul>
<li>If <span class="math inline">\(\text{ICC}\)</span> is close to 1, it indicates that most of the variation in the data is due to differences between groups (high between-group variability).</li>
<li>If <span class="math inline">\(\text{ICC}\)</span> is close to 0, it indicates that most of the variation in the data is due to differences within group (high within-group variability or noise).</li>
</ul>
<p>We can clarify this using a simple example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-scatter-plot-ICC</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "A scatter plot of of X and Y showing the effect of ICC"</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the model</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gganimate)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>simulate_data <span class="ot">&lt;-</span> <span class="cf">function</span>(n_groups, n_per_group, slope, icc, sigma_within) {</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Parameters</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">&lt;-</span> <span class="dv">0</span>  <span class="co"># Overall intercept (can be set to other values as needed)</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute the variance components</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>  sigma_between <span class="ot">&lt;-</span> icc <span class="sc">*</span> sigma_within <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> icc)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Simulate random effects for each group (intercepts)</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>  group_effects <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> n_groups, <span class="at">mean =</span> mu, <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma_between))</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Prepare data frame</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>  data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">group =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_groups, <span class="at">each =</span> n_per_group),</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="fu">rep</span>(<span class="fu">runif</span>(<span class="at">n =</span> n_per_group, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">10</span>), <span class="at">times =</span> n_groups)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Add the fixed effect (slope) and random effects</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>  data<span class="sc">$</span>y <span class="ot">&lt;-</span> slope <span class="sc">*</span> data<span class="sc">$</span>x <span class="sc">+</span> <span class="fu">rep</span>(group_effects, <span class="at">each =</span> n_per_group) <span class="sc">+</span> </span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>            <span class="fu">rnorm</span>(<span class="at">n =</span> n_groups <span class="sc">*</span> n_per_group, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma_within))</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(data)</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>results<span class="ot">&lt;-</span><span class="fu">list</span>()</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>data_int <span class="ot">&lt;-</span> <span class="fu">simulate_data</span>(<span class="dv">4</span>,<span class="dv">10</span>,<span class="dv">10</span>,<span class="fl">0.1</span>,<span class="dv">100</span>)</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>ICCs<span class="ot">&lt;-</span><span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="at">length.out =</span> <span class="dv">20</span>)</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (ICC <span class="cf">in</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="at">length.out =</span> <span class="dv">20</span>)) {</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a> data_int <span class="ot">&lt;-</span> <span class="fu">simulate_data</span>(<span class="dv">4</span>,<span class="dv">10</span>,<span class="dv">10</span>,ICC,<span class="dv">100</span>)</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>  transformed_data <span class="ot">&lt;-</span> <span class="fu">cbind</span>(data_int,<span class="at">ICC=</span>ICC)</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>  results[[<span class="fu">length</span>(results) <span class="sc">+</span> <span class="dv">1</span>]] <span class="ot">&lt;-</span> transformed_data</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>animation_data <span class="ot">&lt;-</span> <span class="fu">do.call</span>(rbind, results)</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>animation_data<span class="sc">$</span>group<span class="ot">&lt;-</span><span class="fu">as.factor</span>(animation_data<span class="sc">$</span>group)</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(animation_data, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> group)) <span class="sc">+</span></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()<span class="sc">+</span> <span class="fu">stat_ellipse</span>(<span class="fu">aes</span>(<span class="at">x=</span>x, <span class="at">y=</span>y,<span class="at">color=</span>group),<span class="at">type =</span> <span class="st">"norm"</span>)<span class="sc">+</span></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"ICC: {closest_state}"</span>, <span class="at">x =</span> <span class="st">"X"</span>, <span class="at">y =</span> <span class="st">"Y"</span>) <span class="sc">+</span></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transition_states</span>(ICC, <span class="at">transition_length =</span> <span class="dv">1</span>, <span class="at">state_length =</span> <span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ease_aes</span>(<span class="st">'linear'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="math_files/figure-html/unnamed-chunk-9-1.gif" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It should be clear that as ICC goes up, the data points starts forming more distinct clusters thus increasing the correlation. We have now a method to measure the correlation within a cluster of data points. We can now proceed with defining the structure of <span class="math inline">\(V\)</span>.</p>
<p>We start with the diagonal elements. The diagonal elements must reflect the total variance for each observation within the group, which includes both the baseline residual variance and the additional variance due to the group effect. Thus: <span class="math display">\[
\text{Diagonal elements} = \sigma^2 + \sigma_d^2
\]</span> The off-diagonal elements are a little bit more involved. Remember that, off-diagonal elements show the <strong>covariance</strong> between data points within each group.</p>
<p>We have so far calculated the correlation:</p>
<p><span class="math display">\[
\text{ICC}=\rho = \frac{\sigma_d^2}{\sigma_d^2 + \sigma^2}
\]</span></p>
<p>Recal that <span class="math inline">\(\rho = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}\)</span>. We can use this to transform ICC into covariance:</p>
<p><span class="math display">\[
\text{Cov}(W_{ij}, W_{ik}) = \rho \times \sigma_{W_{ij}} \times \sigma_{W_{ik}}
\]</span></p>
<p>Also recall that each observation within a group shares the same variance components therefore <span class="math inline">\(\sigma_{W_{ij}} = \sigma_{W_{ik}}\)</span>. Our total variance was <span class="math inline">\(\sqrt{\sigma^2 + \sigma_d^2}\)</span> so</p>
<p><span class="math display">\[
\sigma_{W_{ij}} = \sigma_{W_{ik}} = \sqrt{\sigma^2 + \sigma_d^2}
\]</span></p>
<p>Substitute the expressions for <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\sigma_{W_{ij}}\)</span> into the covariance formula: <span class="math display">\[
\text{Cov}(W_{ij}, W_{ik}) = \frac{\sigma_d^2}{\sigma^2 + \sigma_d^2} \times (\sigma^2 + \sigma_d^2)
\]</span> <span class="math display">\[
\text{Cov}(W_{ij}, W_{ik}) = \sigma_d^2
\]</span> Here, the factor <span class="math inline">\(\sigma^2 + \sigma_d^2\)</span> in the denominator and the standard deviations cancel out, leaving <span class="math inline">\(\sigma_d^2\)</span>.</p>
<p>Given the calculations and the framework we’ve discussed, we can now construct the covariance matrix <span class="math inline">\(V\)</span> for our generalized least squares model.</p>
<p>In the context of GLS where we recognize the presence of group effects, the covariance matrix <span class="math inline">\(V\)</span> takes on a block diagonal form. Each block corresponds to a group of data points that share a common variance structure due to their grouping. This form is important as it reflects the within-group correlations while assuming independence between groups.</p>
<p>The matrix <span class="math inline">\(V\)</span> is composed of blocks <span class="math inline">\(V_i\)</span> along the diagonal, where each block <span class="math inline">\(V_i\)</span> is an <span class="math inline">\(n_i \times n_i\)</span> matrix for group <span class="math inline">\(i\)</span> (assuming <span class="math inline">\(n_i\)</span> observations per group). Each block <span class="math inline">\(V_i\)</span> can be described as follows: <span class="math display">\[
V_i = \begin{bmatrix}
\sigma^2 + \sigma_d^2 &amp; \sigma_d^2 &amp; \cdots &amp; \sigma_d^2 \\
\sigma_d^2 &amp; \sigma^2 + \sigma_d^2 &amp; \cdots &amp; \sigma_d^2 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma_d^2 &amp; \sigma_d^2 &amp; \cdots &amp; \sigma^2 + \sigma_d^2
\end{bmatrix}
\]</span></p>
<p>Or more compactly as</p>
<p><span class="math display">\[V_i = \sigma^2 \mathbf{I} + \sigma_d^2 \mathbf{11'}\]</span></p>
<p>and factor out <span class="math inline">\(\sigma^2\)</span> it gives us a general form of <span class="math display">\[
\sigma^2 (\mathbf{I} + \frac{\sigma_d^2}{\sigma^2} \mathbf{11'})
\]</span> Giving us the form of <span class="math display">\[
\sigma^2V_i = \begin{bmatrix}
1 + \sigma_d^2/\sigma^2 &amp; \sigma_d^2/\sigma^2 &amp; \cdots &amp; \sigma_d^2/\sigma^2 \\
\sigma_d^2/\sigma^2 &amp; 1 + \sigma_d^2/\sigma^2 &amp; \cdots &amp; \sigma_d^2/\sigma^2 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma_d^2/\sigma^2 &amp; \sigma_d^2/\sigma^2 &amp; \cdots &amp; 1 + \sigma_d^2/\sigma^2
\end{bmatrix}
\]</span></p>
<p>Each diagonal element of <span class="math inline">\(V_i\)</span> is <span class="math inline">\(\sigma^2 + \sigma_d^2\)</span>, representing the total variance for each observation within the group. This includes both the baseline residual variance <span class="math inline">\(\sigma^2\)</span>) and the additional variance <span class="math inline">\(\sigma_d^2\)</span> due to the group effect. Each off-diagonal element is <span class="math inline">\(\sigma_d^2\)</span>, which we derived from the intra-class correlation <span class="math inline">\(\rho\)</span>. These elements represent the covariance between any two different observations within the same group, quantifying how much observations within a group are expected to covary due to the shared group effect.</p>
<p>It is important to note that again <span class="math inline">\(V\)</span> has the block diagonal form in which each block is <span class="math inline">\(V_i\)</span>. The block diagonal form of <span class="math inline">\(V\)</span> is appropriate because it reflects that: - Observations within the same group are correlated due to shared group effects, as captured by the off-diagonal elements. - Observations from different groups are assumed to be independent of each other, justifying the separation into different blocks for each group.</p>
<p>We will now finish construction of <span class="math inline">\(V\)</span> bases on <span class="math inline">\(V_i\)</span>. Please note that the stucture of <span class="math inline">\(V_i\)</span> (<span class="math inline">\(\sigma^2 (\mathbf{I} + \frac{\sigma_d^2}{\sigma^2} \mathbf{11'})\)</span>) seems to be identical for any <span class="math inline">\(i\)</span>. This is actually true for our simple cases where we have only a single varying baseline (<span class="math inline">\(intercept\)</span>) between the groups. In this case we can manually count how many data points are in each group and just create a matrix a looks like this:</p>
<p><span class="math display">\[
V=\begin{bmatrix}
V_1 &amp; 0 &amp; 0\\
0 &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; V_n
\end{bmatrix}\
\]</span> where <span class="math inline">\(n\)</span> is the number of groups. We practicaly just copy and paste <span class="math inline">\(\sigma^2 (\mathbf{I} + \frac{\sigma_d^2}{\sigma^2} \mathbf{11'})\)</span> in each diagonal entire. This however is not an elegant solution. If we have more complex grouping structure with many components, we will have a hard time doing that manually. Here we can use deign matrix. You should be fimilar with a deign matrix but i will try to give a very short overview.</p>
<section id="design-matrix" class="level3">
<h3 class="anchored" data-anchor-id="design-matrix">Design matrix</h3>
<p>The design matrix, <span class="math inline">\(Z\)</span>, specifies how grouping and effects are associated with observations. Essentially, <span class="math inline">\(Z\)</span> allocates the influence of effects to the observational data.</p>
<p>Each row of <span class="math inline">\(Z\)</span> corresponds to an observation in the dataset. Each column of <span class="math inline">\(Z\)</span> represents a an effect (like intercept or slope). The number of columns in <span class="math inline">\(Z\)</span> is equal to the number of effects in the model. The elements of <span class="math inline">\(Z\)</span> indicate the degree to which a given effect influences a particular observation. These elements can be binary (0 or 1) in simple cases or continuous values representing the strength or magnitude of the effect.</p>
<p>To illustrate the use of <span class="math inline">\(Z\)</span> more concretely, let’s consider a couple of examples from typical applications:</p>
<p>Suppose we have a study measuring blood pressure in patients from different clinics, where each clinic is considered a group. If we model the intercept for each clinic, indicating that each clinic might inherently have higher or lower blood pressure readings:</p>
<p>Each row corresponds to a patient’s measurement, and there is one column for each clinic. Each element in <span class="math inline">\(Z\)</span> is: - <span class="math inline">\(1\)</span> if the observation (patient) belongs to the clinic (group) - <span class="math inline">\(0\)</span> otherwise</p>
<p><strong>Form of <span class="math inline">\(Z\)</span></strong>:</p>
<p>If there are three clinics and six patients (two in each clinic), <span class="math inline">\(Z\)</span> might look like: <span class="math display">\[
Z = \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1
\end{bmatrix}
\]</span> Each column represents a clinic, and each row assigns a patient to their respective clinic.</p>
<p>For multiple individuals, <span class="math inline">\(Z\)</span> would block these vectors accordingly, potentially stacking them if each has a separate slope effect.</p>
<p>If we had model both intercept and slope (let’s say age) together we might end up having:</p>
<p><span class="math display">\[
Z = \begin{bmatrix}
1 &amp; 30 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
1 &amp; 35 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 20 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 21 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 43 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 53
\end{bmatrix}
\]</span></p>
<p>The design matrix <span class="math inline">\(Z\)</span> is tailored to the structure of the study and the specific effects being modeled. Here we are going to automate building <span class="math inline">\(V\)</span> using design matrix.</p>
<p>Please Note that Z has also a block diagonal form. Each block <span class="math inline">\(Z_i\)</span> continues the matrix format by isolating specific subgroups or experimental conditions, which allows for different intercepts and slopes to be applied to separate segments of the data. This design is particularly useful for mixed-effects models where we might want to estimate both fixed effects (common to all groups) and random effects (varying by group or individual).</p>
<p>For example if we consider the matrix <span class="math inline">\(V\)</span>:</p>
<p><span class="math display">\[
Z = \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1
\end{bmatrix}
\]</span> The structure suggests that we are dealing with three distinct groups. Here’s how each block <span class="math inline">\(Z_i\)</span> in the block diagonal matrix <span class="math inline">\(Z\)</span> is formed and its role in the model:</p>
<ul>
<li><strong>Block <span class="math inline">\(Z_i\)</span></strong>: <span class="math display">\[
Z_i = \begin{bmatrix}
1  \\
1
\end{bmatrix}
\]</span> This block corresponds to the group <span class="math inline">\(i\)</span>. In the second example where <span class="math inline">\(Z\)</span> is:</li>
</ul>
<p><span class="math display">\[
Z = \begin{bmatrix}
1 &amp; 30 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
1 &amp; 35 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 20 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 21 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 43 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 53
\end{bmatrix}
\]</span></p>
<ul>
<li><strong>First Block <span class="math inline">\(Z_1\)</span></strong>: <span class="math display">\[
Z_1 = \begin{bmatrix}
1 &amp; 30 \\
1 &amp; 35
\end{bmatrix}
\]</span></li>
</ul>
<p>This block corresponds to the first group. The first column of ones represents the intercept, and the second column represents the slope with respect to the covariate ‘age’. This group could represent a specific demographic or experimental condition.</p>
<ul>
<li><p><strong>Second Block <span class="math inline">\(Z_2\)</span></strong>: <span class="math display">\[
Z_2 = \begin{bmatrix}
1 &amp; 20 \\
1 &amp; 21
\end{bmatrix}
\]</span></p>
<p>Similar to the first, this block represents another group but with different age values. The structure again includes an intercept and a slope component, indicating a similar analysis but for a different subset of the population or a different experimental condition.</p></li>
<li><p><strong>Third Block <span class="math inline">\(Z_3\)</span></strong>: <span class="math display">\[
Z_3 = \begin{bmatrix}
1 &amp; 43 \\
1 &amp; 53
\end{bmatrix}
\]</span></p>
<p>This block deals with yet another group, possibly older individuals based on the age values shown. The model structure for this group is consistent with the others, providing the ability to compare across groups while accounting for within-group variations.</p></li>
</ul>
</section>
</section>
<section id="structure-of-v-cont." class="level2">
<h2 class="anchored" data-anchor-id="structure-of-v-cont.">Structure of V (cont.)</h2>
<p>Given that we now know about the design matrix, we can go ahead and reformulate the stucture of <span class="math inline">\(V\)</span> base on the design matrix. Please again assume that we are only dealing with intercept here so our design matrix looks like</p>
<p><span class="math display">\[
Z = \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1
\end{bmatrix}
\]</span></p>
<p>and <span class="math inline">\(Z_i\)</span>:</p>
<p><span class="math display">\[
  Z_i = \begin{bmatrix}
  1  \\
  1
  \end{bmatrix}
  \]</span> If you remember the stucutre of <span class="math inline">\(V_i=\sigma^2 (\mathbf{I} + \frac{\sigma_d^2}{\sigma^2} \mathbf{11'})\)</span>.</p>
<p>If <span class="math inline">\(Z_i\)</span> is only a column matrix of <span class="math inline">\(1\)</span> then <span class="math inline">\(Z_iZ^{T}_{i}=\mathbf{11'}\)</span>. Let’s define matrix <span class="math inline">\(D\)</span> as a block diagonal matrix where each block, <span class="math inline">\(D_i\)</span>, represents the variance components associated with the intercepts and possibly slopes for each group. Given the structure of <span class="math inline">\(V_i = \sigma^2 (\mathbf{I} + \frac{\sigma_d^2}{\sigma^2} \mathbf{11'})\)</span>, and assuming <span class="math inline">\(Z_i\)</span> is structured to capture different types of effects (like intercept and slope as previously mentioned), the form of <span class="math inline">\(D\)</span> and each <span class="math inline">\(D_i\)</span> needs to reflect the appropriate variance scaling for these effects.</p>
<p>The matrix <span class="math inline">\(D\)</span> is structured as:</p>
<p><span class="math display">\[
D = \begin{bmatrix}
D_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; D_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; D_N
\end{bmatrix}
\]</span> where each block <span class="math inline">\(D_i\)</span> is specific to a group and contains the variance components for the intercept and slope within that group. Assuming a simple case where each group has one intercept each <span class="math inline">\(Z_i\)</span> block consists of one columns for intercept.</p>
$$ D_i =
<span class="math display">\[\begin{bmatrix}
\frac{\sigma_{\text{int},i}^2}{\sigma^2}

\end{bmatrix}\]</span>
<p>$$</p>
<p>Therefore</p>
<p><span class="math display">\[
D = \begin{bmatrix}
\frac{\sigma_{\text{int},1}^2}{\sigma^2} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \frac{\sigma_{\text{int},2}^2}{\sigma^2} &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \frac{\sigma_{\text{int},N}^2}{\sigma^2}
\end{bmatrix}
\]</span></p>
<p>Since we consider all the groups to share the same variance covariance component:</p>
<p><span class="math display">\[
D=\begin{bmatrix}
\frac{\sigma_{\text{int}}^2}{\sigma^2} &amp; 0 &amp; 0\\
0 &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; \frac{\sigma_{\text{int}}^2}{\sigma^2}
\end{bmatrix}\
\]</span></p>
<p>If we had intercept and slope, each <span class="math inline">\(D_i\)</span> could be represented as a 2x2 matrix, because each <span class="math inline">\(Z_i\)</span> block consists of two columns (one for intercepts and one for slopes).</p>
<p><span class="math display">\[
D_i = \begin{bmatrix}
\frac{\sigma_{\text{int}}^2}{\sigma^2} &amp; 0 \\
0 &amp; \frac{\sigma_{\text{slope}}^2}{\sigma^2}
\end{bmatrix}
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
D=\begin{bmatrix}
\begin{bmatrix}
\frac{\sigma_{\text{int}}^2}{\sigma^2} &amp; 0 \\
0 &amp; \frac{\sigma_{\text{slope}}^2}{\sigma^2}
\end{bmatrix} &amp; 0 &amp; 0\\
0 &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp;  \begin{bmatrix}
\frac{\sigma_{\text{int}}^2}{\sigma^2} &amp; 0 \\
0 &amp; \frac{\sigma_{\text{slope}}^2}{\sigma^2}
\end{bmatrix}
\end{bmatrix}\
\]</span></p>
<p>Give <span class="math inline">\(D\)</span> and <span class="math inline">\(Z_i\)</span> we can now write <span class="math inline">\(V_i=\sigma^2 (\mathbf{I} + \frac{\sigma_d^2}{\sigma^2} \mathbf{11'})\)</span> as</p>
<p><span class="math display">\[
V_i=\sigma^2 (\mathbf{I} + Z_iDZ_i^T)
\]</span> The complete matrix form of <span class="math inline">\(V\)</span> can be written as</p>
<p><span class="math display">\[
V=\sigma^2 (\mathbf{I} + ZDZ^T)
\]</span></p>
</section>
</section>
<section id="estimation-of-parameters" class="level1">
<h1>Estimation of parameters</h1>
<p>Given everything we have said so far, we now have complete ingredients to perform GLS. This includes our response variable <span class="math inline">\(y\)</span>, design matrix for the effect of interest <span class="math inline">\(X\)</span> (fixed effect), a design matrix for grouping structure in the data <span class="math inline">\(Z\)</span> and variance covariance matrix <span class="math inline">\(D\)</span> which encodes <span class="math inline">\(\sigma^2\)</span> (residual variance) and <span class="math inline">\(\sigma_d^2\)</span> (variance of the grouping structure) which gives us <span class="math inline">\(V=\sigma^2 (\mathbf{I} + ZDZ^T)\)</span>.</p>
<p>What we have assumed so far is that we know <span class="math inline">\(V\)</span> but in reality <span class="math inline">\(V\)</span> is unknown. Since <span class="math inline">\(V\)</span> is from using <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\frac{\sigma_d^2}{\sigma^2}\)</span>, we need to estimate these two which we are going to denote as <span class="math inline">\(\theta=[\sigma^2,\frac{\sigma_d^2}{\sigma^2}]=[\sigma^2,\lambda]\)</span>.</p>
<p>To begin, in order to find the parameters for our model, we employ Maximum Likelihood Estimation (MLE). The goal is to estimate the <span class="math inline">\(\theta\)</span> that best fit our model to the observed data.</p>
<p>The GLS model assumes that the response variable <span class="math inline">\(y\)</span> is linearly dependent on multiple predictors, captured in matrix <span class="math inline">\(X\)</span>, with random errors <span class="math inline">\(\epsilon\)</span> that are normally distributed but with a non-constant variance structure. This variance structure is modeled by <span class="math inline">\(\sigma^2 V\)</span>, where <span class="math inline">\(V\)</span> depends on another matrix <span class="math inline">\(Z\)</span> and a variance-covariance matrix <span class="math inline">\(D\)</span>. The model can be written as:</p>
<p><span class="math display">\[
y = X\beta + \epsilon, \quad \text{where} \quad \epsilon \sim N(0, \sigma^2 V).
\]</span> Since <span class="math inline">\(\epsilon \sim N(0, \sigma^2 V)\)</span>, and <span class="math inline">\(\epsilon = y - X\beta\)</span>, we can say that <span class="math inline">\(y\)</span>, the response variable, is also normally distributed. This follows from the properties of the normal distribution where a linear transformation of a normally distributed variable is also normally distributed. Thus:</p>
<p><span class="math display">\[
y \sim N(X\beta, \sigma^2 V).
\]</span> The probability density function for a multivariate normal distribution with mean <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span> is given by:</p>
<p><span class="math display">\[
f(x) = \frac{1}{(2\pi)^{k/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu)\right),
\]</span> We substitute our model notation and it gives us</p>
<p><span class="math display">\[
L(\theta; y) = \frac{1}{(2\pi)^{n/2} |V|^{1/2}} \exp\left(-\frac{1}{2} (y - X\beta)^T (V)^{-1} (y - X\beta)\right),
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of observations in <span class="math inline">\(y\)</span>. Here <span class="math inline">\(|V|\)</span> denotes the determinant of <span class="math inline">\(V\)</span>.</p>
<p>Taking the natural logarithm of the likelihood function simplifies calculations and helps in deriving estimators. The log-likelihood (<span class="math inline">\(\ell\)</span>) is:</p>
<p><span class="math display">\[
\ell(\theta; y) = -\frac{n}{2} \log(2\pi) - \frac{1}{2} \log |V| - \frac{1}{2} (y - X\beta)^T V^{-1} (y - X\beta).
\]</span></p>
<p>What we know from GLS (see above) is that</p>
<p><span class="math display">\[
   \beta = (X^T V^{-1} X)^{-1} X^T V^{-1} y.
\]</span> Ignore the constant <span class="math inline">\(-n \log(2\pi)\)</span> as it does not depend on <span class="math inline">\(\beta\)</span> or <span class="math inline">\(\theta\)</span>.</p>
<ol start="3" type="1">
<li>Substituting <span class="math inline">\(\tilde{\boldsymbol{\beta}}\)</span> and acknowledging <span class="math inline">\(V\)</span> as <span class="math inline">\(\tilde{\boldsymbol{V}}(\boldsymbol{\theta})\)</span>gives: <span class="math display">\[
-2\ell(\boldsymbol{\theta};\boldsymbol{Y}) = \log|\tilde{\boldsymbol{V}}(\boldsymbol{\theta})| + (Y-X\tilde{\boldsymbol{\beta}}(\boldsymbol{\theta}))^T \tilde{\boldsymbol{V}}(\boldsymbol{\theta})^{-1} (Y-X\tilde{\boldsymbol{\beta}}(\boldsymbol{\theta})).
\]</span> Given this function we can go ahead and write a small piece of code to calculate this.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>simulate_grouped_trend <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">group_count =</span> <span class="dv">5</span>, <span class="at">points_per_group =</span> <span class="dv">10</span>, <span class="at">global_slope =</span> <span class="sc">-</span><span class="dv">10</span>, <span class="at">global_intercept =</span> <span class="dv">30</span>, <span class="at">group_slope =</span> <span class="dv">2</span>, <span class="at">noise_sd =</span> <span class="dv">50</span>,<span class="at">noise_sd2=</span><span class="dv">2</span>) {</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co"># Setting a seed for reproducibility</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Initialize an empty data frame to store the simulated data</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">numeric</span>(), <span class="at">y =</span> <span class="fu">numeric</span>())</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Loop to create each group</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>group_count) {</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    x_start <span class="ot">&lt;-</span> <span class="dv">12</span> <span class="sc">+</span> (i <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> (<span class="dv">10</span> <span class="sc">/</span> group_count) <span class="co"># Stagger the start of x for each group</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> <span class="fu">runif</span>(points_per_group, <span class="at">min =</span> x_start, <span class="at">max =</span> x_start <span class="sc">+</span> (<span class="dv">10</span> <span class="sc">/</span> group_count))</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply a local positive trend within the group, but maintain the global negative trend</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    local_intercept <span class="ot">&lt;-</span> global_intercept <span class="sc">+</span> global_slope <span class="sc">*</span> (x_start <span class="sc">+</span> (<span class="dv">10</span> <span class="sc">/</span> (<span class="dv">2</span> <span class="sc">*</span> group_count))) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> noise_sd)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> local_intercept <span class="sc">+</span> group_slope[i] <span class="sc">*</span> (x <span class="sc">-</span> x_start) <span class="sc">+</span> <span class="fu">rnorm</span>(points_per_group, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> noise_sd2)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combine this group with the overall dataset</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    group_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y,<span class="at">group=</span>i)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    data <span class="ot">&lt;-</span> <span class="fu">rbind</span>(data, group_data)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(data)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co"># generate simulated data</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>data_int <span class="ot">&lt;-</span> <span class="fu">simulate_grouped_trend</span>(<span class="at">group_count =</span> <span class="dv">4</span>,<span class="at">points_per_group =</span> <span class="dv">10</span>,<span class="at">global_slope =</span> <span class="sc">-</span><span class="dv">2</span>,<span class="at">global_intercept =</span> <span class="dv">100</span>,<span class="at">group_slope =</span> <span class="fu">c</span>(<span class="dv">6</span>,<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">1</span>),<span class="at">noise_sd =</span> <span class="dv">5</span>,<span class="at">noise_sd2=</span><span class="dv">2</span>)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="co"># set group to factor</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>data_int<span class="sc">$</span>group <span class="ot">&lt;-</span> <span class="fu">factor</span>(data_int<span class="sc">$</span>group)</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a><span class="co"># theta is [sigmal,lambda]</span></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Design matrix of random effect</span></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span><span class="dv">0</span><span class="sc">+</span>data_int<span class="sc">$</span>group)</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct the model matrix 'X' for the fixed effect 'x' using data from 'data_int'.</span></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span>x, <span class="at">data=</span>data_int)</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="co"># get groups (random effects)</span></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>groups <span class="ot">&lt;-</span> <span class="fu">unique</span>(data_int<span class="sc">$</span>group)</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>log1 <span class="ot">&lt;-</span> <span class="cf">function</span>(theta) {</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>  D <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="at">x =</span> theta[<span class="dv">2</span>],<span class="at">ncol =</span> <span class="fu">length</span>(groups),<span class="at">nrow =</span> <span class="fu">length</span>(groups))</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>  I <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">1</span>,<span class="at">nrow =</span> <span class="fu">nrow</span>(X),<span class="at">ncol =</span> <span class="fu">nrow</span>(X))</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>  V<span class="ot">&lt;-</span>theta[<span class="dv">1</span>]<span class="sc">*</span>(I<span class="sc">+</span>Z<span class="sc">%*%</span>D<span class="sc">%*%</span><span class="fu">t</span>(Z))</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>  Beta <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> y</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(<span class="fu">log</span>(<span class="fu">det</span>(V)) <span class="sc">+</span> <span class="fu">t</span>(y <span class="sc">-</span> X <span class="sc">%*%</span> Beta) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> (y <span class="sc">-</span> X <span class="sc">%*%</span> Beta)))</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>theta<span class="ot">&lt;-</span>optimization<span class="sc">::</span><span class="fu">optim_nm</span>(<span class="at">fun =</span> log1, <span class="at">k =</span> <span class="dv">2</span>, <span class="at">start =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="at">maximum=</span> <span class="cn">FALSE</span>, <span class="at">tol =</span> <span class="fl">0.0000000000001</span>)<span class="sc">$</span>par</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Residual = "</span>,<span class="fu">sqrt</span>(theta[<span class="dv">1</span>]),<span class="st">" group = "</span>, <span class="fu">sqrt</span>(<span class="fu">prod</span>(theta)),<span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Residual =  2.40449  group =  16.61623 </code></pre>
</div>
</div>
<p>if we compare the results to that of <code>lmer</code> we see they are identical:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">VarCorr</span>(<span class="fu">lmer</span>(y<span class="sc">~</span>x<span class="sc">+</span>(<span class="dv">1</span><span class="sc">|</span>group),<span class="at">data=</span>data_int,<span class="at">REML =</span> <span class="cn">FALSE</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> Groups   Name        Std.Dev.
 group    (Intercept) 16.6162 
 Residual              2.4045 </code></pre>
</div>
</div>
<section id="bias-of-mle" class="level2">
<h2 class="anchored" data-anchor-id="bias-of-mle">Bias of MLE</h2>
<p>While Maximum Likelihood Estimators are efficient and consistent under many conditions, they can be biased, particularly in estimating the variance in a model.</p>
<p>Consider a random sample <span class="math inline">\(y_1, y_2, \ldots, y_n\)</span> from a normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span>. We want to estimate the variance <span class="math inline">\(\sigma^2\)</span> using MLE.</p>
<p>The probability density function for a single observation <span class="math inline">\(y_i\)</span> given <span class="math inline">\(\sigma^2\)</span> is:</p>
<p><span class="math display">\[
f(y_i | \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(y_i - \mu)^2}{2\sigma^2}\right).
\]</span></p>
<p>The likelihood function for all observations <span class="math inline">\(y_1, \ldots, y_n\)</span>, assuming independence, is the product of individual densities:</p>
<p><span class="math display">\[
L(\sigma^2 | y_1, \ldots, y_n) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(y_i - \mu)^2}{2\sigma^2}\right).
\]</span></p>
<p>The log-likelihood function, <span class="math inline">\(\ell(\sigma^2)\)</span>, is the natural logarithm of the likelihood function:</p>
<p><span class="math display">\[
\ell(\sigma^2) = \sum_{i=1}^n \log\left(\frac{1}{\sqrt{2\pi \sigma^2}}\right) - \sum_{i=1}^n \frac{(y_i - \mu)^2}{2\sigma^2}
= -\frac{n}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \mu)^2.
\]</span></p>
<p>To find the maximum likelihood estimator (MLE) for <span class="math inline">\(\sigma^2\)</span>, differentiate <span class="math inline">\(\ell(\sigma^2)\)</span> with respect to <span class="math inline">\(\sigma^2\)</span> and set this derivative to zero:</p>
<p><span class="math display">\[
\frac{d\ell(\sigma^2)}{d\sigma^2} = -\frac{n}{2\sigma^2} + \frac{\sum_{i=1}^n (y_i - \mu)^2}{2(\sigma^2)^2} = 0.
\]</span></p>
<p>Solving for <span class="math inline">\(\sigma^2\)</span>, we rearrange and simplify:</p>
<p><span class="math display">\[
\frac{n}{2\sigma^2} = \frac{\sum_{i=1}^n (y_i - \mu)^2}{2(\sigma^2)^2}
\Rightarrow \sigma^2 = \frac{1}{n} \sum_{i=1}^n (y_i - \mu)^2.
\]</span></p>
<p>When <span class="math inline">\(\mu\)</span> is estimated from the data, the sample mean <span class="math inline">\(\overline{y}\)</span> is:</p>
<p><span class="math display">\[
\overline{y} = \frac{1}{n} \sum_{i=1}^n y_i.
\]</span></p>
<p>The MLE for <span class="math inline">\(\sigma^2\)</span> when the mean is estimated from the data becomes:</p>
<p><span class="math display">\[
\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (y_i - \overline{y})^2,
\]</span></p>
<p>We now need to take the expectation of To find the expectation of <span class="math inline">\(\hat{\sigma}^2\)</span>. To do that we express it in terms of <span class="math inline">\(y_i - \mu\)</span> and <span class="math inline">\(\overline{y} - \mu\)</span>**</p>
<p>First, rewrite the squared term using the identity <span class="math inline">\(a - b = (a - c) + (c - b)\)</span> where <span class="math inline">\(a = y_i\)</span>, <span class="math inline">\(b = \overline{y}\)</span>, and <span class="math inline">\(c = \mu\)</span>: <span class="math display">\[
y_i - \overline{y} = (y_i - \mu) - (\overline{y} - \mu).
\]</span> Thus, the squared term becomes:</p>
<p><span class="math display">\[
(y_i - \overline{y})^2 = [(y_i - \mu) - (\overline{y} - \mu)]^2.
\]</span> Expanding this, we get:</p>
<p><span class="math display">\[
(y_i - \overline{y})^2 = (y_i - \mu)^2 - 2(y_i - \mu)(\overline{y} - \mu) + (\overline{y} - \mu)^2.
\]</span></p>
<p>Summing over all <span class="math inline">\(i\)</span> from 1 to <span class="math inline">\(n\)</span>, we obtain: <span class="math display">\[
\sum_{i=1}^n (y_i - \overline{y})^2 = \sum_{i=1}^n (y_i - \mu)^2 - 2 \sum_{i=1}^n (y_i - \mu)(\overline{y} - \mu) + \sum_{i=1}^n (\overline{y} - \mu)^2.
\]</span> Since <span class="math inline">\(\overline{y} - \mu\)</span> does not depend on <span class="math inline">\(i\)</span>, it can be factored out of the sums:</p>
<p><span class="math display">\[
\sum_{i=1}^n (y_i - \overline{y})^2 = \sum_{i=1}^n (y_i - \mu)^2 - 2(\overline{y} - \mu) \sum_{i=1}^n (y_i - \mu) + n(\overline{y} - \mu)^2.
\]</span> Given that <span class="math inline">\(\sum_{i=1}^n (y_i - \mu) = n(\overline{y} - \mu)\)</span>, the middle term simplifies to:</p>
<p><span class="math display">\[
-2n(\overline{y} - \mu)^2,
\]</span> and the equation becomes:</p>
<p><span class="math display">\[
\sum_{i=1}^n (y_i - \overline{y})^2 = \sum_{i=1}^n (y_i - \mu)^2 - n(\overline{y} - \mu)^2.
\]</span></p>
<p>We take the expectation: <span class="math display">\[
E\left[\hat{\sigma}^2\right] = E\left[\frac{1}{n} \sum_{i=1}^n (y_i - \overline{y})^2\right] = \frac{1}{n} E\left[\sum_{i=1}^n (y_i - \mu)^2 - n(\overline{y} - \mu)^2\right].
\]</span> Since <span class="math inline">\(E[\sum_{i=1}^n (y_i - \mu)^2] = n\sigma^2\)</span> and <span class="math inline">\(E[n(\overline{y} - \mu)^2] = \sigma^2\)</span> (by the properties of variance), we get:</p>
<p><span class="math display">\[
E[\hat{\sigma}^2] = \frac{1}{n} [n\sigma^2 - \sigma^2] = \frac{n-1}{n} \sigma^2.
\]</span></p>
<p>Thus, the bias <span class="math inline">\(B(\hat{\sigma}^2)\)</span> is:</p>
<p><span class="math display">\[
B(\hat{\sigma}^2) = E(\hat{\sigma}^2) - \sigma^2 = \frac{n-1}{n} \sigma^2 - \sigma^2 = \frac{n-1-n}{n} \sigma^2 = \frac{-1}{n} \sigma^2.
\]</span></p>
</section>
<section id="restricted-maximum-likelihood-reml" class="level2">
<h2 class="anchored" data-anchor-id="restricted-maximum-likelihood-reml">Restricted Maximum Likelihood (REML)</h2>
<p>As said before MLE has a significant limitation: it tends to underestimate variance components due to its non-adjustment for the degrees of freedom consumed by the fixed effects. This underestimation arises because MLE maximizes a likelihood function using all available data, which inadvertently causes an overfitting of fixed effects, leading to biased and sometimes misleading estimates of variance.</p>
<p>This bias in MLE is known to result from too large a denominator in the variance formula, which does not account for the degrees of freedom used up by estimating the mean (fixed effects). Intuitively, this comes from how the sample mean is based on the same data, making the data points appear more clustered around the mean than they actually are, hence underestimating the true variability that a correct denominator would account for.</p>
<p>To address this issue, Restricted Maximum Likelihood (REML) was developed. REML refines the approach used by MLE by first fitting the fixed effects and then adjusting the likelihood function used to estimate variance components, specifically compensating for the degrees of freedom used in estimating the fixed effects. This adjustment ensures that the estimation of variance components is based on the correct amount of independent information.</p>
<p>A key aspect of REML is the process of “integrating out” the fixed effects. This approach involves focusing on the likelihood of the residuals after removing the influence of the fixed effects, rather than the likelihood of the entire dataset. By integrating out these effects, REML effectively separates the estimation of variance parameters from the estimation of the fixed effects, which can reduce bias in the variance estimates. The integration is achieved through a mathematical transformation where the fixed effects are marginalized over (integrated out), usually requiring the calculation of integrals over these parameters. This process results in a likelihood function that depends only on the random effects and the variance components, which REML then maximizes to obtain more accurate and less biased estimates.</p>
<p>Let’s have a look at our previous likelyhood function</p>
<p><span class="math display">\[
\log\left[\ell(\theta; y)\right] = -\frac{n}{2} \log(2\pi) - \frac{1}{2} \log |V| +  \log\left[ e^{-\frac{\mathbf{(Y-X\beta)^TV}^{-1}(\mathbf{Y-X\beta})}{2}}\right]
\]</span></p>
<p>Integrating out the fixed effects in REML involves focusing on the part of the log-likelihood that pertains only to the variance components, adjusting for the estimation of <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
\log\left[\int\mathrm{L}(\beta, V)d\beta\right] = -\frac{1}{2}\log(2\pi) - \frac{1}{2}\log(\lvert \mathbf{V} \rvert) + \log\left[\int e^{-\frac{\mathbf{(Y-X\beta)^TV}^{-1}(\mathbf{Y-X\beta})}{2}}d\beta\right]
\]</span></p>
<p>Let’s start from <span class="math inline">\((\mathbf{Y} - \mathbf{X}\beta)^T V^{-1} (\mathbf{Y} - \mathbf{X}\beta)\)</span></p>
<p>Expanding this, we get: <span class="math display">\[
= \mathbf{Y}^T V^{-1} \mathbf{Y} - \mathbf{Y}^T V^{-1} \mathbf{X}\beta - \beta^T \mathbf{X}^T V^{-1} \mathbf{Y} + \beta^T \mathbf{X}^T V^{-1} \mathbf{X}\beta
\]</span></p>
<p>To rewrite this in the form of a completed square, it’s helpful to recognize the general form: <span class="math inline">\(\beta^T \mathbf{A} \beta - 2\beta^T \mathbf{b}\)</span> Here, <span class="math inline">\(\mathbf{A} = \mathbf{X}^T V^{-1} \mathbf{X}\)</span> and <span class="math inline">\(\mathbf{b} = \mathbf{X}^T V^{-1} \mathbf{Y}\)</span>.</p>
<p>We can rewrite it by completing the square in <span class="math inline">\(\beta\)</span>: <span class="math display">\[
\beta^T \mathbf{X}^T V^{-1} \mathbf{X} \beta - 2\beta^T \mathbf{X}^T V^{-1} \mathbf{Y}
\]</span> <span class="math display">\[
= (\beta - \mathbf{A}^{-1}\mathbf{b})^T \mathbf{A} (\beta - \mathbf{A}^{-1}\mathbf{b}) - \mathbf{b}^T \mathbf{A}^{-1} \mathbf{b}
\]</span> Where <span class="math inline">\(\mathbf{A}^{-1} = (\mathbf{X}^T V^{-1} \mathbf{X})^{-1}\)</span>.</p>
<p>Substituting Back: Thus, substituting back <span class="math inline">\(\mathbf{A}^{-1}\mathbf{b}\)</span> and rearranging, we find: <span class="math display">\[
= (\beta - (\mathbf{X}^T V^{-1} \mathbf{X})^{-1} \mathbf{X}^T V^{-1} \mathbf{Y})^T \mathbf{X}^T V^{-1} \mathbf{X} (\beta - (\mathbf{X}^T V^{-1} \mathbf{X})^{-1} \mathbf{X}^T V^{-1} \mathbf{Y}) + \mathbf{Y}^T V^{-1} \mathbf{Y} - \mathbf{b}^T \mathbf{A}^{-1} \mathbf{b}
\]</span> Simplifying: <span class="math display">\[
= (\beta - \mathbf{A} \mathbf{Y})^T \mathbf{X}^T V^{-1} \mathbf{X} (\beta - \mathbf{A} \mathbf{Y}) + \mathbf{Y}^T V^{-1} \mathbf{Y} - \mathbf{Y}^T V^{-1} \mathbf{X} \mathbf{A} \mathbf{Y}
\]</span> where <span class="math inline">\(\mathbf{A} = (\mathbf{X}^T V^{-1} \mathbf{X})^{-1} \mathbf{X}^T V^{-1}\)</span>.</p>
<p>The final expression for the quadratic form then becomes:</p>
<p><span class="math display">\[
(\beta - \mathbf{A} \mathbf{Y})^T \mathbf{X}^T V^{-1} \mathbf{X} (\beta - \mathbf{A} \mathbf{Y}) + \mathbf{Y}^T (V^{-1} - V^{-1} \mathbf{X} \mathbf{A}) \mathbf{Y}
\]</span></p>
<p>Given: <span class="math display">\[
\int e^{-\frac{(\mathbf{Y} - \mathbf{X}\beta)^T V^{-1} (\mathbf{Y} - \mathbf{X}\beta)}{2}} d\beta
\]</span></p>
<p>From the earlier completion of the square, we know the integral’s argument can be expressed as: <span class="math display">\[
(\beta - \mathbf{A} \mathbf{Y})^T \mathbf{X}^T V^{-1} \mathbf{X} (\beta - \mathbf{A} \mathbf{Y}) + \mathbf{Y}^T (V^{-1} - V^{-1} \mathbf{X} \mathbf{A}) \mathbf{Y}
\]</span></p>
<p>The integral of a Gaussian (quadratic form in the exponent) over <span class="math inline">\(\beta\)</span> is: <span class="math display">\[
e^{-\frac{1}{2} \mathbf{Y}^T (V^{-1} - V^{-1} \mathbf{X} \mathbf{A}) \mathbf{Y}} \cdot \int e^{-\frac{1}{2} (\beta - \mathbf{A} \mathbf{Y})^T \mathbf{X}^T V^{-1} \mathbf{X} (\beta - \mathbf{A} \mathbf{Y})} d\beta
\]</span></p>
<p>For the integral over <span class="math inline">\(\beta\)</span>, since it’s a Gaussian integral, it simplifies to: <span class="math display">\[
(2\pi)^{\frac{k}{2}} \left|\mathbf{X}^T V^{-1} \mathbf{X}\right|^{-\frac{1}{2}}
\]</span></p>
<p>Where <span class="math inline">\(k\)</span> is the number of parameters (dimension of <span class="math inline">\(\beta\)</span>).</p>
<p>Substituting back: <span class="math display">\[
\log\left[e^{-\frac{1}{2} \mathbf{Y}^T (V^{-1} - V^{-1} \mathbf{X} \mathbf{A}) \mathbf{Y}} (2\pi)^{\frac{k}{2}} \left|\mathbf{X}^T V^{-1} \mathbf{X}\right|^{-\frac{1}{2}}\right]
\]</span> <span class="math display">\[
= -\frac{1}{2} \mathbf{Y}^T (V^{-1} - V^{-1} \mathbf{X} \mathbf{A}) \mathbf{Y} + \frac{k}{2} \log(2\pi) - \frac{1}{2} \log\left|\mathbf{X}^T V^{-1} \mathbf{X}\right|
\]</span></p>
<p><span class="math display">\[
\log\left[\int\mathrm{L}(\beta, V)d\beta\right] = -\frac{1}{2}\log(2\pi) - \frac{1}{2}\log(\lvert \mathbf{V} \rvert) + \left(-\frac{1}{2} \mathbf{Y}^T (V^{-1} - V^{-1} \mathbf{X} \mathbf{A}) \mathbf{Y} + \frac{k}{2} \log(2\pi) - \frac{1}{2} \log\left|\mathbf{X}^T V^{-1} \mathbf{X}\right|\right)
\]</span> Therefore:</p>
<p><span class="math display">\[
-2\ell_{REML}(\boldsymbol{\theta};\boldsymbol{Y}) = \log |V| + \log |\mathbf{X}^T V^{-1} \mathbf{X}| + \mathbf{Y}^T V^{-1} \mathbf{Y} - \mathbf{Y}^T V^{-1} \mathbf{X} \mathbf{A} \mathbf{Y}
\]</span> where <span class="math inline">\(\mathbf{A} = (\mathbf{X}^T V^{-1} \mathbf{X})^{-1} \mathbf{X}^T V^{-1}\)</span></p>
<p>Rewriting the last part sing complete squares and simplifying (<span class="math inline">\(\mathbf{A} \mathbf{Y} = \tilde{\boldsymbol{\beta}}(\boldsymbol{\theta})\)</span>):</p>
<p><span class="math display">\[
\mathbf{Y}^T V^{-1} \mathbf{Y} - \mathbf{Y}^T V^{-1} \mathbf{X} \mathbf{A} \mathbf{Y} = (\mathbf{Y} - \mathbf{X}\tilde{\boldsymbol{\beta(\boldsymbol{\theta})}})^T V^{-1} (\mathbf{Y} - \mathbf{X}\tilde{\boldsymbol{\beta(\boldsymbol{\theta})}})
\]</span> Combining the rearranged terms, the full expression is: <span class="math display">\[
-2\ell_{REML}(\boldsymbol{\theta};\boldsymbol{Y}) = \log |\tilde{\boldsymbol{V}}(\boldsymbol{\theta})| + \log |\boldsymbol{X}^T \tilde{\boldsymbol{V}}^{-1} \boldsymbol{X}| + (\boldsymbol{Y} - \boldsymbol{X}\tilde{\boldsymbol{\beta}}(\boldsymbol{\theta}))^T \tilde{\boldsymbol{V}}^{-1} (\boldsymbol{Y} - \boldsymbol{X}\tilde{\boldsymbol{\beta}}(\boldsymbol{\theta}))
\]</span> This is our final equation for REML and can be implemented easily:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># theta is [sigmal,lambda]</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Design matrix of random effect</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span><span class="dv">0</span><span class="sc">+</span>data_int<span class="sc">$</span>group)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct the model matrix 'X' for the fixed effect 'x' using data from 'data_int'.</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span>x, <span class="at">data=</span>data_int)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co"># get groups (random effects)</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>groups <span class="ot">&lt;-</span> <span class="fu">unique</span>(data_int<span class="sc">$</span>group)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>log_reml <span class="ot">&lt;-</span> <span class="cf">function</span>(theta) {</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>  D <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="at">x =</span> theta[<span class="dv">2</span>],<span class="at">ncol =</span> <span class="fu">length</span>(groups),<span class="at">nrow =</span> <span class="fu">length</span>(groups))</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>  I <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">1</span>,<span class="at">nrow =</span> <span class="fu">nrow</span>(X),<span class="at">ncol =</span> <span class="fu">nrow</span>(X))</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>  V<span class="ot">&lt;-</span>theta[<span class="dv">1</span>]<span class="sc">*</span>(I<span class="sc">+</span>Z<span class="sc">%*%</span>D<span class="sc">%*%</span><span class="fu">t</span>(Z))</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>  Beta <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> y</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(<span class="fu">log</span>(<span class="fu">det</span>(V)) <span class="sc">+</span> <span class="fu">log</span>(<span class="fu">det</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> X))<span class="sc">+</span> <span class="fu">t</span>(y <span class="sc">-</span> X <span class="sc">%*%</span> Beta) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> (y <span class="sc">-</span> X <span class="sc">%*%</span> Beta)))</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>theta<span class="ot">&lt;-</span>optimization<span class="sc">::</span><span class="fu">optim_nm</span>(<span class="at">fun =</span> log_reml, <span class="at">k =</span> <span class="dv">2</span>, <span class="at">start =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="at">maximum=</span> <span class="cn">FALSE</span>, <span class="at">tol =</span> <span class="fl">0.0000000000001</span>)<span class="sc">$</span>par</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Residual = "</span>,<span class="fu">sqrt</span>(theta[<span class="dv">1</span>]),<span class="st">" group = "</span>, <span class="fu">sqrt</span>(<span class="fu">prod</span>(theta)),<span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Residual =  2.435465  group =  19.45779 </code></pre>
</div>
</div>
<p>which gives the same results as</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">VarCorr</span>(<span class="fu">lmer</span>(y<span class="sc">~</span>x<span class="sc">+</span>(<span class="dv">1</span><span class="sc">|</span>group),<span class="at">data=</span>data_int,<span class="at">REML =</span> <span class="cn">TRUE</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> Groups   Name        Std.Dev.
 group    (Intercept) 19.4578 
 Residual              2.4355 </code></pre>
</div>
</div>
</section>
<section id="other-methods-of-fitting" class="level2">
<h2 class="anchored" data-anchor-id="other-methods-of-fitting">Other methods of fitting</h2>
<p>In practice, when dealing with this kind of models, we do not directly optimize the log-likelihood function due to its complexity, particularly when random effects are included. Instead, more tractable approaches such as the Expectation-Maximization (EM) algorithm, Newton-Raphson, and quasi-likelihood methods are commonly employed. These techniques facilitate the optimization process by breaking it down into simpler steps or by approximating the log-likelihood function. For instance, the EM algorithm efficiently handles missing data or latent variables by iteratively estimating expected values and maximizing the likelihood. Newton-Raphson and related methods, on the other hand, provide robust mechanisms for dealing with non-linear optimization through successive approximations. These methods are essential for ensuring convergence and computational feasibility in the analysis of mixed models.</p>
</section>
</section>
<section id="best-linear-unbiased-estimators-blue" class="level1">
<h1>Best Linear Unbiased Estimators (BLUE)</h1>
<p>Now that we have estimated the components of <span class="math inline">\(V\)</span>, we can show that <span class="math inline">\(B\)</span> is in fact Best Linear Unbiased Estimators (BLUE). This involves proving that the GLS estimator is both unbiased and has the smallest variance among all linear unbiased estimators.</p>
<p>Recall the GLS estimator is defined as: <span class="math display">\[
\hat{\beta}_{GLS} = (X^T V^{-1} X)^{-1} X^T V^{-1} y
\]</span> where <span class="math inline">\(V\)</span> is the known covariance matrix of the errors <span class="math inline">\(e\)</span> in the model <span class="math inline">\(y = X\beta + e\)</span>.</p>
<p>The model equation is: <span class="math display">\[
y = X\beta + e
\]</span> where <span class="math inline">\(e\)</span> is the error term with <span class="math inline">\(E(e) = 0\)</span> and <span class="math inline">\(Var(e) = \sigma^2 V\)</span>.</p>
<p>Plug the expression for <span class="math inline">\(y\)</span> into the GLS estimator: <span class="math display">\[
\hat{\beta}_{GLS} = (X^T V^{-1} X)^{-1} X^T V^{-1} (X\beta + e)
\]</span> Break the expression into two parts by distributing <span class="math inline">\(X^T V^{-1}\)</span>: <span class="math display">\[
\hat{\beta}_{GLS} = (X^T V^{-1} X)^{-1} X^T V^{-1} X\beta + (X^T V^{-1} X)^{-1} X^T V^{-1} e
\]</span> Notice that <span class="math inline">\(X^T V^{-1} X\)</span> and <span class="math inline">\((X^T V^{-1} X)^{-1} X^T V^{-1} X\)</span> simplify as follows: <span class="math display">\[
(X^T V^{-1} X)^{-1} X^T V^{-1} X = I
\]</span> where <span class="math inline">\(I\)</span> is the identity matrix, thus: <span class="math display">\[
\hat{\beta}_{GLS} = \beta + (X^T V^{-1} X)^{-1} X^T V^{-1} e
\]</span> To show unbiasedness, we take the expectation of <span class="math inline">\(\hat{\beta}_{GLS}\)</span>: <span class="math display">\[
E(\hat{\beta}_{GLS}) = E(\beta + (X^T V^{-1} X)^{-1} X^T V^{-1} e)
\]</span> <span class="math display">\[
E(\hat{\beta}_{GLS}) = \beta + E((X^T V^{-1} X)^{-1} X^T V^{-1} e)
\]</span> Since <span class="math inline">\(E(e) = 0\)</span>: <span class="math display">\[
E((X^T V^{-1} X)^{-1} X^T V^{-1} e) = (X^T V^{-1} X)^{-1} X^T V^{-1} E(e) = 0
\]</span> Therefore: <span class="math display">\[
E(\hat{\beta}_{GLS}) = \beta
\]</span></p>
<p>The expectation of the GLS estimator is equal to the true parameter vector <span class="math inline">\(\beta\)</span>, so <span class="math inline">\(\hat{\beta}_{GLS}\)</span> is an unbiased estimator of <span class="math inline">\(\beta\)</span>. That means covariance structure <span class="math inline">\(V\)</span> of the errors do not introduce any bias into the estimator.</p>
<p>Lastly we need to show that variance of the GLS estimator <span class="math inline">\(\beta_{GLS}\)</span> is lower than or equal to the variance of any other linear unbiased estimator.</p>
<p>Let’s first derive the variance of <span class="math inline">\(\beta_{GLS}\)</span></p>
<p>Recall that <span class="math display">\[
   \hat{\beta}_{GLS} = (X^T V^{-1} X)^{-1} X^T V^{-1} y
\]</span></p>
<p>Since <span class="math inline">\(y = X\beta + e\)</span> and the errors <span class="math inline">\(e\)</span> have covariance <span class="math inline">\(\sigma^2 V\)</span>, the variance of <span class="math inline">\(y\)</span> is: <span class="math display">\[
   Var(y) = \sigma^2 V
\]</span> Give the linear transformation rule for variance (<span class="math inline">\(Var(Ay) = A Var(y) A^T\)</span>), which states that for any linear transformation <span class="math display">\[
   Var(\hat{\beta}_{GLS}) = Var\left((X^T V^{-1} X)^{-1} X^T V^{-1} y\right)
\]</span> <span class="math display">\[
   = (X^T V^{-1} X)^{-1} X^T V^{-1} Var(y) V^{-1} X (X^T V^{-1} X)^{-1}
\]</span> <span class="math display">\[
   = (X^T V^{-1} X)^{-1} X^T V^{-1} (\sigma^2 V) V^{-1} X (X^T V^{-1} X)^{-1}
\]</span> Simplifying: <span class="math display">\[
   = \sigma^2 (X^T V^{-1} X)^{-1} X^T V^{-1} V V^{-1} X (X^T V^{-1} X)^{-1} = \sigma^2 (X^T V^{-1} X)^{-1} (X^T V^{-1} X) (X^T V^{-1} X)^{-1}
\]</span> <span class="math display">\[
   = \sigma^2 (X^T V^{-1} X)^{-1}
\]</span></p>
<p>The variance of the GLS estimator <span class="math inline">\(\hat{\beta}_{GLS} = Var(\hat{\beta}_{GLS}) = \sigma^2 (X^T V^{-1} X)^{-1}\)</span></p>
<p>Any other linear unbiased estimator <span class="math inline">\(\hat{\beta}_L\)</span> can be expressed as: <span class="math display">\[
\hat{\beta}_L = CY
\]</span> where <span class="math inline">\(C\)</span> must satisfy <span class="math inline">\(CX = I\)</span> for unbiasedness. Specifically, we can define <span class="math inline">\(C\)</span> as: <span class="math display">\[
C = (X^TV^{-1}X)^{-1}X^TV^{-1} + M
\]</span> where <span class="math inline">\(M\)</span> is a matrix such that <span class="math inline">\(MX = 0\)</span>. This ensures that <span class="math inline">\(\hat{\beta}_L\)</span> is unbiased: <span class="math display">\[
E[\hat{\beta}_L] = \beta + MXE[\beta] = \beta
\]</span> since <span class="math inline">\(E[y] = X\beta\)</span>.</p>
<p>Given the model <span class="math inline">\(y = X\beta + e\)</span> with <span class="math inline">\(Var(e) = \sigma^2 V\)</span>, the variance of <span class="math inline">\(y\)</span> is <span class="math inline">\(\sigma^2 V\)</span>, and hence: <span class="math display">\[
Var(\hat{\beta}_L) = C \sigma^2 V C^T
\]</span></p>
<p>Substituting <span class="math inline">\(C\)</span> into the variance formula, we get: <span class="math display">\[
Var(\hat{\beta}_L) = \left((X^TV^{-1}X)^{-1}X^TV^{-1} + M\right) \sigma^2 V \left((X^TV^{-1}X)^{-1}X^TV^{-1} + M\right)^T
\]</span> Expanding and using <span class="math inline">\(MX = 0\)</span>, we have: <span class="math display">\[
Var(\hat{\beta}_L) = \sigma^2 \left((X^TV^{-1}X)^{-1} + MVM^T\right)
\]</span></p>
<p>Since <span class="math inline">\(\sigma^2 (X^T V^{-1} X)^{-1}=Var(\hat{\beta}_{GLS})\)</span> then <span class="math inline">\(Var(\hat{\beta}_L) = Var(\hat{\beta}_{GLS}) + \sigma^2 MVM^T\)</span></p>
<p><span class="math inline">\(MVM^T\)</span> is a positive semi-definite matrix since <span class="math inline">\(V\)</span> is positive definite (or semi-definite if <span class="math inline">\(V\)</span> is not strictly positive). This implies <span class="math inline">\(\sigma^2 MVM^T \geq 0\)</span> which gives us <span class="math inline">\(Var(\hat{\beta}_L) \geq Var(\hat{\beta}_{GLS})\)</span>.</p>
<p>This shows that any deviation from the GLS estimator in the form of an additional matrix <span class="math inline">\(M\)</span> results in an increase in variance. Therefore, the GLS estimator <span class="math inline">\(\hat{\beta}_{GLS}\)</span> is the Best Linear Unbiased Estimator (BLUE) for models where the errors have a covariance structure described by <span class="math inline">\(\sigma^2 V\)</span>.</p>
</section>
<section id="random-effects" class="level1">
<h1>Random Effects</h1>
<p>I want to briefly review the components we’ve discussed so far before proceeding with random effects</p>
<ol type="1">
<li><p><strong>Design Matrix, <span class="math inline">\(Z\)</span></strong>: This matrix is called the design matrix for the random effect and encodes how each observation in our dataset is related to other observations within the same group or category. It effectively organizes our data according to predetermined groupings.</p></li>
<li><p><strong>Covariance Matrix, <span class="math inline">\(D\)</span></strong>: <span class="math inline">\(D\)</span> is the covariance matrix associated with the groupings defined by <span class="math inline">\(Z\)</span>. It quantifies the variance within each group as well as the covariance between groups.</p></li>
<li><p><strong>Error Terms</strong>: Often denoted as <span class="math inline">\(\sigma^2\)</span>, this represents the variance of the error terms in our models. It captures the amount of variability in our observations that isn’t explained by the model.</p></li>
</ol>
<p>What we tried to model so far is that we believe there is certain grouping in the data which we encoded by by <span class="math inline">\(Z\)</span>. What we believe is that for one or more of our primary effects just as intercept, slope etc there exist variabilities that are specific to these groups and not fully captured by these primary effects alone. This variability, which may come from unique characteristics or influences within each group. We modeled this variability using <span class="math inline">\(D\)</span>. If we suspected that only a single effect such as intercept was affected by groups, <span class="math inline">\(D\)</span> will be diagonal, meaning that it will have a single value repeating on its diagonal element. If we suspect 2 or more effects are random, then <span class="math inline">\(D\)</span> is block diagona. Each block’s size corresponds to the number of effects considered to be random within a group, effectively capturing the covariance between these effects across different groupings.</p>
<p>I guess you realized that our aim has never been to <strong>estimate</strong> the exact influence of each single group on the effects but rather to say that they are from a distribution that captures the inherent variability among the groups. If we have thousands of data points, we might as well consider estimating directly each group’s effect; however, such an approach can quickly become unwieldy and prone to overfitting, especially in cases where groups are many but each group has few observations. By using a model that incorporates random effects, we effectively balance the need for specificity in modeling group influences against the practical constraints of model complexity and the risk of fitting noise rather than signal. Also have this in mind that, we might not be interested in the effects of each group but rather just quantifying the influence collectively would be sufficient.</p>
<p>Having said that, we know that these groupings affect the observed data, yet unlike fixed effects, random effects are not directly observable. They represent underlying influences or variables that affect the outcome but are not included as direct measurements in the data. For example, in a study on student performance across schools, random effects might include inherent school qualities like teaching methods or school climate, which are not directly measured but still influence student outcomes. Despite being unseen, we can predict these random effects. This is different to directly estimating a parameter in that prediction focuses on the likely values of random variables for specific instances or groups, given the observed data and the model structure. We are going to denote these prediction as <span class="math inline">\(u\)</span> here.</p>
<p>We we said is that <span class="math inline">\(u\)</span> is coming from a distribution <span class="math inline">\(D\)</span> as covariance parameter and mean of zero. so <span class="math inline">\(u \sim N(0, \sigma^2D)\)</span> and we also said that our residuals are distributed as <span class="math inline">\(e \sim N(0, \sigma^2)\)</span>.</p>
<p>The errors <span class="math inline">\(e\)</span> and the random effects <span class="math inline">\(u\)</span> are assumed independent.</p>
<p>Given the assumptions and linearity, the observations <span class="math inline">\(y\)</span> are also normally distributed with: - Mean: <span class="math inline">\(E(y) = X\beta\)</span> (since <span class="math inline">\(E(u) = 0\)</span> and <span class="math inline">\(E(e) = 0\)</span>) - Covariance: <span class="math inline">\(Var(y) = Z\sigma^2DZ^T + \sigma^2I\)</span></p>
<p>Please note that this is because our total variation is variation of <span class="math inline">\(Zu\)</span> and <span class="math inline">\(e\)</span> so <span class="math inline">\(Var(Zu) = ZDZ^T\)</span> and <span class="math inline">\(Var(e) = \sigma^2\)</span>, utilizing the independence of <span class="math inline">\(u\)</span> and <span class="math inline">\(e\)</span>.</p>
<p>Therefore <span class="math inline">\(y \sim N(X\beta, \sigma^2(I+ZDZ^T))\)</span></p>
<p>When we have a pair of random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and we know the outcome of <span class="math inline">\(Y\)</span>, the conditional expectation <span class="math inline">\(E(X | Y)\)</span> serves as the best predictor of <span class="math inline">\(X\)</span> based on <span class="math inline">\(Y\)</span>, in the mean squared error sense. This is because it minimizes the expected squared difference between the predicted values and the actual values of <span class="math inline">\(X\)</span>, compared to any other function of <span class="math inline">\(Y\)</span>.</p>
<p>The conditional expectation of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span> in a bivariate normal distribution can be expressed as:</p>
<p><span class="math display">\[
E(X | Y = y) = \mu_X + \frac{\sigma_{XY}}{\sigma_Y^2} (y - \mu_Y)
\]</span></p>
<p>To apply conditional expectation, recognize that <span class="math inline">\(u\)</span> and <span class="math inline">\(y\)</span> have a joint normal distribution. We can reformulate conditional expectation in a form suitable for our model:</p>
<p><span class="math display">\[
E(u | y) = E(u) + Cov(u, y)Var(y)^{-1}(y - E(y))
\]</span> - <span class="math inline">\(E(u) = 0\)</span> and <span class="math inline">\(E(y) = X\beta\)</span> - <span class="math inline">\(Cov(u, y)\)</span> needs to be calculated: <span class="math display">\[Cov(u, y) = Cov(u, Zu + e)\]</span> <span class="math display">\[= Cov(u, Zu) + Cov(u, e)\]</span> <span class="math display">\[= \sigma^2DZ^T + 0\]</span> <span class="math display">\[= \sigma^2DZ^T\]</span> The last equality is because <span class="math inline">\(Cov(u, e) = 0\)</span> due to independence and <span class="math inline">\(Cov(u, Zu) = \sigma^2DZ^T\)</span> because</p>
<p><span class="math display">\[
   \text{Cov}(u, Zu) = E[(u - E[u])(Zu - E[Zu])^T]
\]</span></p>
<p>Given that <span class="math inline">\(E[u] = 0\)</span> (since <span class="math inline">\(u\)</span> is assumed to have mean zero), the expression simplifies to: <span class="math display">\[
   \text{Cov}(u, Zu) = E[u(Zu)^T]
\]</span></p>
<p><span class="math display">\[
   E[u(Zu)^T] = E[u(u^T Z^T)]
\]</span> Here, <span class="math inline">\(u^T Z^T\)</span> is simply a rearrangement of the multiplication. T</p>
<p>The inner operation <span class="math inline">\(u(u^T Z^T)\)</span> results in a matrix where each element is a product of elements from <span class="math inline">\(u\)</span> and <span class="math inline">\(u^T\)</span>, transformed by <span class="math inline">\(Z^T\)</span>.</p>
<p><span class="math display">\[
   E[uu^T] = \sigma^2D
\]</span></p>
<p>So, applying <span class="math inline">\(Z^T\)</span> to each element effectively results in:</p>
<p><span class="math display">\[
   E[u(u^T Z^T)] = E[uu^T]Z^T = \sigma^2DZ^T
\]</span></p>
<p>Using the formula for conditional expectation in multivariate normal distributions: <span class="math display">\[E(u | y) = 0 + \sigma^2DZ^T(Z\sigma^2DZ^T + \sigma^2)^{-1}(y - X\beta)\]</span> <span class="math display">\[= \sigma^2DZ^T V^{-1} (y - X\beta)\]</span> where <span class="math inline">\(V = Z\sigma^2DZ^T + \sigma^2\)</span> is the covariance matrix of <span class="math inline">\(y\)</span>.</p>
<p>The final predictor <span class="math inline">\(\hat{u}\)</span> is: <span class="math display">\[\hat{u} = \sigma^2DZ^T V^{-1} (y - X\beta)\]</span></p>
<p>Let’s give this a try to see if it gives us correct results:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Design matrix of random effect</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span><span class="dv">0</span><span class="sc">+</span>data_int<span class="sc">$</span>group)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct the model matrix 'X' for the fixed effect 'x' using data from 'data_int'.</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span>x, <span class="at">data=</span>data_int)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># get groups (random effects)</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>groups <span class="ot">&lt;-</span> <span class="fu">unique</span>(data_int<span class="sc">$</span>group)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>log_reml <span class="ot">&lt;-</span> <span class="cf">function</span>(theta) {</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>  D <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="at">x =</span> theta[<span class="dv">2</span>],<span class="at">ncol =</span> <span class="fu">length</span>(groups),<span class="at">nrow =</span> <span class="fu">length</span>(groups))</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>  I <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">1</span>,<span class="at">nrow =</span> <span class="fu">nrow</span>(X),<span class="at">ncol =</span> <span class="fu">nrow</span>(X))</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>  V<span class="ot">&lt;-</span>theta[<span class="dv">1</span>]<span class="sc">*</span>(I<span class="sc">+</span>Z<span class="sc">%*%</span>D<span class="sc">%*%</span><span class="fu">t</span>(Z))</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>  Beta <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> y</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(<span class="fu">log</span>(<span class="fu">det</span>(V)) <span class="sc">+</span> <span class="fu">log</span>(<span class="fu">det</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> X))<span class="sc">+</span> <span class="fu">t</span>(y <span class="sc">-</span> X <span class="sc">%*%</span> Beta) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> (y <span class="sc">-</span> X <span class="sc">%*%</span> Beta)))</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>theta<span class="ot">&lt;-</span>optimization<span class="sc">::</span><span class="fu">optim_nm</span>(<span class="at">fun =</span> log_reml, <span class="at">k =</span> <span class="dv">2</span>, <span class="at">start =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="at">maximum=</span> <span class="cn">FALSE</span>, <span class="at">tol =</span> <span class="fl">0.0000000000001</span>)<span class="sc">$</span>par</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>D <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="at">x =</span> theta[<span class="dv">2</span>],<span class="at">ncol =</span> <span class="fu">length</span>(groups),<span class="at">nrow =</span> <span class="fu">length</span>(groups))</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>I <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">1</span>,<span class="at">nrow =</span> <span class="fu">nrow</span>(X),<span class="at">ncol =</span> <span class="fu">nrow</span>(X))</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>V<span class="ot">&lt;-</span>theta[<span class="dv">1</span>]<span class="sc">*</span>(I<span class="sc">+</span>Z<span class="sc">%*%</span>D<span class="sc">%*%</span><span class="fu">t</span>(Z))</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>Beta <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> y</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>u <span class="ot">&lt;-</span> theta[<span class="dv">1</span>]<span class="sc">*</span>D<span class="sc">%*%</span><span class="fu">t</span>(Z)<span class="sc">%*%</span><span class="fu">solve</span>(V)<span class="sc">%*%</span>(y<span class="sc">-</span>X<span class="sc">%*%</span>Beta)</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(u)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           [,1]
[1,]  26.705634
[2,]   1.337016
[3,] -11.314929
[4,] -16.727721</code></pre>
</div>
</div>
<p>compare this to <code>lmer</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">ranef</span>(<span class="fu">lmer</span>(y<span class="sc">~</span>x<span class="sc">+</span>(<span class="dv">1</span><span class="sc">|</span>group),<span class="at">data=</span>data_int,<span class="at">REML =</span> <span class="cn">TRUE</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$group
  (Intercept)
1   26.705636
2    1.337016
3  -11.314930
4  -16.727722

with conditional variances for "group" </code></pre>
</div>
</div>
<p>We get identical results! These values are also called to be Best Linear Unbiased Prediction (BLUP) because Unbiasedness (<span class="math inline">\(E(E(u | y)) = E(u) = 0\)</span>) and minization of the variance of the prediction error <span class="math inline">\(E[(u - \hat{u})^2]\)</span> among all linear unbiased predictors. We are not going to prove them here as we did similar analysis up there fore BLUEs.</p>
</section>
<section id="final-mixed-model-form" class="level1">
<h1>Final mixed model form</h1>
<p>So all together, we now know that each observation in our data can be described by our fixed effects and predicted random effects plus some random noise.</p>
<p>Putting all together we have</p>
<p><span class="math display">\[y = X\beta + Zu + e\]</span></p>
<p>This describes our data model, where:</p>
<ul>
<li><span class="math inline">\(y\)</span> is the vector of observed data.</li>
<li><span class="math inline">\(\beta\)</span> represents fixed effects, which may or may not be of interest for the derivation.</li>
<li><span class="math inline">\(u\)</span> represents the random effects we wish to predict.</li>
<li><span class="math inline">\(e\)</span> is the error term.</li>
<li><span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are known design matrices.</li>
</ul>
<p>We also assume: - <span class="math inline">\(u \sim N(0, \sigma^2D)\)</span>, where <span class="math inline">\(D\)</span> is the covariance matrix of the random effects. - <span class="math inline">\(e \sim N(0, \sigma^2)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is the covariance matrix of the residuals. - <span class="math inline">\(u\)</span> and <span class="math inline">\(e\)</span> are independent.</p>
<p>This is the final form of the mixed model which we derived starting from OLS.</p>
</section>
<section id="p-value-for-mixed-models-satterthwaite-approximation" class="level1">
<h1>p-value for mixed models (Satterthwaite approximation)</h1>
<p>This should be clear by now that mixed models are inherently more complex than OLS. This complexity goes also to statistical testing. In this section, I want to write a bit about statistical testing both from the classical perspective but also from linear models and mixed models. Again we are not going to go through all details but just demonstrate some of the key concepts.</p>
<section id="how-rare-is-this-value" class="level2">
<h2 class="anchored" data-anchor-id="how-rare-is-this-value">How rare is this value?!</h2>
<p>Let’s assume we have a specific value from our dataset that we’re curious about its rarity. The only way to accurately assess how rare this value is involves understanding where it falls within an overall distribution of data. This compares the value against a bunch of expected values. Ff it deviates significantly from what’s typical, then we can consider it rare. To do this, we first need to define the distribution that best describes our dataset. A common assumption to make about a dataset is that it follows a normal distribution. Many natural phenomena and human-made processes tend to distribute their values in a way that can be symmetrically modeled around a central value, with fewer occurrences appearing as one moves further away from this central point.</p>
<p>If we assume that our data follows a normal distribution, we can use its mathematical properties to check the rarity of specific observations within our dataset.</p>
<p>In probability, the “density” of a value in a continuous distribution refers to the relative likelihood of this value occurring. For a normal distribution values closer to the mean are more densely packed and so more likely to occur. As we move away from the mean, the density decreases, indicating that these values are less likely and rarer.</p>
<p>The density function doesn’t give the probability of the event occurring at exactly a specific point (which in continuous distributions is technically zero), but rather it helps us understand how dense the distribution is around that point, which is indicative of the probability of observing values in a small interval around that point.</p>
<p>The probability density function for a normal distribution is given by:</p>
<p><span class="math display">\[
f(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\]</span></p>
<p>Here: - <span class="math inline">\(x\)</span> is the value for which we are calculating the probability density. - <span class="math inline">\(\mu\)</span> is the mean of the distribution. - <span class="math inline">\(\sigma\)</span> is the standard deviation, which measures the dispersion of the data around the mean. - <span class="math inline">\(\sigma^2\)</span> is the variance.</p>
<p>To determine how rare or unusual a particular value <span class="math inline">\(x\)</span> is within a normal distribution, we need to calculate the probability of observing <span class="math inline">\(x\)</span> or a value more extreme (larger or smaller, depending on the context). This is typically done using the cumulative distribution function (CDF), which gives the probability that a normally distributed random variable is less than or equal to a certain value.</p>
<p>CDF is represented by the integral of the probability density function from negative infinity to $ x $. Mathematically, it’s expressed as:</p>
<p><span class="math display">\[
F(x | \mu, \sigma^2) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(t-\mu)^2}{2\sigma^2}} dt
\]</span></p>
<p>For instance, if we want to calculate the probability of observing a value less than or equal to 60 from a normal distribution with a mean of 50 and a standard deviation of 10:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Value to calculate the cumulative probability for</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">60</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the probability</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>probability <span class="ot">&lt;-</span> <span class="fu">pnorm</span>(x, <span class="at">mean =</span> mu, <span class="at">sd =</span> sigma)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(probability)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.8413447</code></pre>
</div>
</div>
<p>I guess you realized that we have an infinite number of normal distributions defined by particular parameters (<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>). However, it is often nice to have a standard one from which we can derive a table of probabilities. This distribution is called the <strong>standard normal distribution</strong>.</p>
</section>
<section id="standard-normal-distribution" class="level2">
<h2 class="anchored" data-anchor-id="standard-normal-distribution">Standard normal distribution</h2>
<p>The standard normal distribution is a specific instance of the normal distribution where the mean (<span class="math inline">\(\mu\)</span>) is 0 and the standard deviation (<span class="math inline">\(\sigma\)</span>) is 1. This standardization simplifies many statistical methods and allows us to use a single, universally applicable table (commonly called the Z-table) for finding probabilities</p>
<p>To convert any normal distribution to the standard normal distribution, values are transformed into what are known as Z-scores. This transformation process is called <strong>standardization</strong>. A Z-score tells us how many standard deviations away a point is from the mean of the distribution.</p>
<p>The formula to calculate the Z-score of a value <span class="math inline">\(x\)</span> is:</p>
<p><span class="math display">\[Z = \frac{(x - \mu)}{\sigma}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(x\)</span> is the value from the original distribution.</li>
<li><span class="math inline">\(\mu\)</span> is the mean of the original distribution.</li>
<li><span class="math inline">\(\sigma\)</span> is the standard deviation of the original distribution.</li>
</ul>
<p>Let’s consider an example where we calculate the Z-scores:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample data values</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>data_values <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">40</span>, <span class="dv">55</span>, <span class="dv">60</span>, <span class="dv">45</span>, <span class="dv">70</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Mean and standard deviation of the dataset</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Z-scores</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>z_scores <span class="ot">&lt;-</span> (data_values <span class="sc">-</span> mu) <span class="sc">/</span> sigma</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Output Z-scores</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(z_scores)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -1.0  0.5  1.0 -0.5  2.0</code></pre>
</div>
</div>
<p>We can now use a standard normal distribution for finding probabilities.</p>
<p>For any random variable we can write the transformation as</p>
<p><span class="math display">\[Z = \frac{(X - \mu)}{\sigma}\]</span></p>
<p>Now suppose that we went and collected a few samples and measured its mean <span class="math inline">\(\bar{X}\)</span> and we want to check how likely is that to get a value larger or smaller than <span class="math inline">\(\bar{X}\)</span>. We can do a <span class="math inline">\(z\)</span> transformation but with a small difference.</p>
<p>There is however a small difference here. Eventhough the sample can be used to calculate unbiased estimates of the population property, the sample estimate will not be perfect. The standard deviation of the sampling distribution is called the standard error or <span class="math inline">\(SE\)</span>. If each <span class="math inline">\(X_i\)</span> is normally distributed with mean <span class="math inline">\(\mu\)</span> (mean of population) and standard deviation <span class="math inline">\(\sigma\)</span>, then:</p>
<ul>
<li><strong>Expected Value</strong>: <span class="math inline">\(E[\overline{X}] = \mu\)</span></li>
<li><strong>Variance</strong>: Since <span class="math inline">\(X_i\)</span> are independent, the variance of the sum is the sum of the variances: <span class="math inline">\(Var(\overline{X}) = Var\left(\frac{1}{n} \sum_{i=1}^n X_i\right) = \frac{1}{n^2} \sum_{i=1}^n Var(X_i) = \frac{1}{n^2} \cdot n \sigma^2 = \frac{\sigma^2}{n}\)</span></li>
</ul>
<p>The standard error of the mean is:</p>
<p><span class="math inline">\(SEM = \frac{\sigma}{\sqrt{n}}\)</span></p>
<p>So we change the definiton of <span class="math inline">\(Z\)</span> for sample mean to</p>
<p><span class="math display">\[Z = \frac{(\bar{X} - \mu)}{\frac{\sigma}{\sqrt{n}}}\]</span> Which can be used to derive the probability. However, there is a challenge! So far, we have assumed that we know the variance of the population. In many practical situations <span class="math inline">\(\sigma\)</span> is not known and must be estimated from the sample data we call it <span class="math inline">\(SE\)</span>.</p>
<p><span class="math display">\[
SE = \frac{s}{\sqrt{n}}
\]</span></p>
<p>Here, <span class="math inline">\(s\)</span> is the sample standard deviation, and <span class="math inline">\(n\)</span> is the sample size. The standard error provides a measure of how much error is expected in the estimate of the mean. As the sample size increases, the standard error decreases, indicating a more accurate estimate of the population mean.</p>
<p>Our standarized value now becomes</p>
<p><span class="math display">\[
\frac{\bar{X} - \mu}{SE}
\]</span></p>
<p>There is again a small little question here: Is this actually normal distribution?</p>
<p>Let’s see if we can write <span class="math inline">\(\frac{\bar{X} - \mu}{SE}\)</span> in terms of distributions that we already know such as <span class="math inline">\(Z\)</span>.</p>
<p>We know that</p>
<p><span class="math display">\[
\frac{\bar{X} - \mu}{SE}=\frac{\overline{X} - \mu}{S / \sqrt{n}}
\]</span> Where <span class="math inline">\(S\)</span> is the standard deviation of samples.</p>
<p>Recall that <span class="math inline">\(Z = \frac{\overline{X} - \mu}{\sigma/\sqrt{n}}\)</span>.</p>
<p>So we can write</p>
<p><span class="math display">\[
\overline{X} - \mu = Z \cdot \frac{\sigma}{\sqrt{n}}
\]</span></p>
<p>Plugging this into the expression <span class="math inline">\(\frac{\overline{X} - \mu}{S / \sqrt{n}}\)</span>, we have:</p>
<p><span class="math display">\[
\frac{\overline{X} - \mu}{S / \sqrt{n}} = \frac{Z \cdot \frac{\sigma}{\sqrt{n}}}{S / \sqrt{n}} = Z \cdot \frac{\sigma}{S}
\]</span></p>
<p>Now we should work our way around <span class="math inline">\(\frac{\sigma}{S}\)</span> and write it in a form that has know distribution.</p>
<p>Remember that sample mean <span class="math inline">\(\overline{X}\)</span> and sample variance <span class="math inline">\(S^2\)</span> are defined as follows: <span class="math display">\[
\overline{X} = \frac{1}{n} \sum_{i=1}^1 X_i
\]</span> <span class="math display">\[
S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2
\]</span></p>
<p>First, let’s express <span class="math inline">\(X_i - \overline{X}\)</span>: <span class="math display">\[
X_i - \overline{X} = \left( X_i - \mu \right) - \left( \overline{X} - \mu \right)
\]</span> <span class="math display">\[
= (X_i - \mu) - \left( \frac{1}{n} \sum_{j=1}^n (X_j - \mu) \right)
\]</span></p>
<p>Now consider the sum of squared deviations from the mean: <span class="math display">\[
\sum_{i=1}^n (X_i - \overline{X})^2 = \sum_{i=1}^n \left[ (X_i - \mu) - (\overline{X} - \mu) \right]^2
\]</span> Using the property of variance and the independence of <span class="math inline">\(X_i\)</span>, we know: <span class="math display">\[
\sum_{i=1}^n (X_i - \overline{X})^2 = \sum_{i=1}^n (X_i - \mu)^2 - n(\overline{X} - \mu)^2
\]</span></p>
<p>Since <span class="math inline">\(X_i\)</span> are i.i.d. <span class="math inline">\(N(\mu, \sigma^2)\)</span>, <span class="math inline">\((X_i - \mu)/\sigma\)</span> are i.i.d. <span class="math inline">\(N(0, 1)\)</span>, we know that: <span class="math display">\[
\sum_{i=1}^n \left( \frac{X_i - \mu}{\sigma} \right)^2 \sim \chi^2_n
\]</span></p>
<p>However, we are interested in <span class="math inline">\(\sum_{i=1}^n (X_i - \overline{X})^2\)</span>. Note that <span class="math inline">\(\overline{X}\)</span> itself is <span class="math inline">\(N(\mu, \sigma^2/n)\)</span>.</p>
<p>By Cochran’s theorem, the sum of squared deviations can be split into independent parts: <span class="math display">\[
\sum_{i=1}^n (X_i - \mu)^2 = \sum_{i=1}^n (X_i - \overline{X})^2 + n(\overline{X} - \mu)^2
\]</span> where <span class="math inline">\(\sum_{i=1}^n (X_i - \overline{X})^2\)</span> and <span class="math inline">\(n(\overline{X} - \mu)^2\)</span> are independent.</p>
<p>Now, <span class="math inline">\(\sum_{i=1}^n (X_i - \overline{X})^2\)</span> follows a chi-squared distribution with <span class="math inline">\(n-1\)</span> degrees of freedom: <span class="math display">\[
\sum_{i=1}^n (X_i - \overline{X})^2 \sim \sigma^2 \chi^2_{n-1}
\]</span></p>
<p>We have: <span class="math display">\[
S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2
\]</span> So, we can write: <span class="math display">\[
(n-1)S^2 = \sum_{i=1}^n (X_i - \overline{X})^2
\]</span></p>
<p>So: <span class="math display">\[
\frac{(n-1)S^2}{\sigma^2} = \frac{\sum_{i=1}^n (X_i - \overline{X})^2}{\sigma^2} \sim \chi^2_{n-1}
\]</span></p>
<p>Thus, we have shown that: <span class="math display">\[
\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}
\]</span> Let’s call this <span class="math inline">\(U = \frac{(n-1)S^2}{\sigma^2}\sim \chi^2_{n-1}\)</span>.</p>
<p>We can express <span class="math inline">\(\sigma\)</span> in terms of <span class="math inline">\(S\)</span> and <span class="math inline">\(U\)</span> using the relation <span class="math inline">\(\sigma^2 = \frac{(n-1)S^2}{U}\)</span>, then:</p>
<p><span class="math display">\[
\sigma = S \sqrt{\frac{n-1}{U}}
\]</span> Substituting back into the equation, we get:</p>
<p><span class="math display">\[
Z \cdot \frac{S \sqrt{\frac{n-1}{U}}}{S} = Z \cdot \sqrt{\frac{n-1}{U}} = \frac{Z}{\sqrt{\frac{U}{n-1}}}
\]</span> Therefore:</p>
<p><span class="math display">\[\frac{\overline{X} - \mu}{S / \sqrt{n}} = \frac{Z}{\sqrt{\frac{U}{n-1}}}\]</span></p>
</section>
<section id="putting-it-all-together-t" class="level2">
<h2 class="anchored" data-anchor-id="putting-it-all-together-t">Putting it all together (T)</h2>
<p>Based on</p>
<p><span class="math display">\[
\frac{\overline{X} - \mu}{S / \sqrt{n}} = \frac{Z}{\sqrt{\frac{U}{n-1}}}
\]</span></p>
<p>Let’s just assume we call <span class="math inline">\(n-1=d\)</span> (degrees of freedom) so</p>
<p><span class="math display">\[
\frac{Z}{\sqrt{\frac{U}{n-1}}}=\frac{Z}{\sqrt{\frac{U}{d}}}\sim \chi^2_{n}
\]</span></p>
<p>We know that a random variable <span class="math inline">\(Z\)</span>) that is standard normally distributed, denoted <span class="math inline">\(Z \sim N(0, 1)\)</span>, has a probability density function (PDF): <span class="math display">\[
  f_Z(z) = \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}}
  \]</span></p>
<p>Also a chi-squared distribution with <span class="math inline">\(d\)</span> degrees of freedom, denoted <span class="math inline">\(U \sim \chi^2_d\)</span>, can be expressed as the sum of squares of <span class="math inline">\(d\)</span> independent standard normal variables. Its PDF is given by:</p>
<p><span class="math display">\[
  f_U(u) = \frac{u^{(d/2)-1} e^{-u/2}}{2^{d/2} \Gamma(d/2)}
\]</span></p>
<p>Let’s make things a bit tidy here by defining:</p>
<p><span class="math display">\[
   T = \frac{Z}{\sqrt{\frac{U}{d}}} = \frac{Z}{\sqrt{V}}
\]</span></p>
<p>where <span class="math inline">\(V = \frac{U}{d}\)</span>.</p>
<p>We now have to find PDF of <span class="math inline">\(T\)</span> which a joint PDF if weassume that <span class="math inline">\(Z\)</span> and <span class="math inline">\(V\)</span> are independent. The joint PDF of <span class="math inline">\(Z\)</span> and <span class="math inline">\(V\)</span>, <span class="math inline">\(f_{Z,V}(z,v)\)</span>, is the product of their marginal PDFs:</p>
<p><span class="math display">\[
f_{Z,V}(z,v) = f_Z(z) f_V(v) = \left( \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}} \right) \left( \frac{d^{d/2} v^{d/2 - 1} e^{-dv/2}}{2^{d/2} \Gamma(d/2)} \right)
\]</span></p>
<p>Directly working with <span class="math inline">\(f_{Z,V}(z, v)\)</span> to find the PDF of <span class="math inline">\(T\)</span> involves solving an integral where <span class="math inline">\(T\)</span> is a function of both <span class="math inline">\(Z\)</span> and <span class="math inline">\(V\)</span>. Instead we are going to write things in terms of <span class="math inline">\(T\)</span> using change variables method.</p>
<p>Let’s start with</p>
<p><span class="math display">\[
Z = T\sqrt{W}, \quad V = W
\]</span></p>
<p>where <span class="math inline">\(W\)</span> is just a placeholder to help in calculation. This is an easier form to work with but the problem is when working with transformation of random variables, the volume that the probability density covers can change. In order to conserve the total probability between the orignal and the transformed space we need to derive how much we have to strech or compress the transformed space. determinant of Jacobian gives us a way to calculate this scaling factor.</p>
<p>Given:</p>
<p><span class="math display">\[
Z = T\sqrt{W}, \quad V = W
\]</span></p>
<p>To find the Jacobian determinant, we compute the partial derivatives:</p>
<p><span class="math display">\[
\begin{align*}
\frac{\partial Z}{\partial T} &amp;= \sqrt{W} \\
\frac{\partial Z}{\partial W} &amp;= \frac{T}{2\sqrt{W}} \\
\frac{\partial V}{\partial T} &amp;= 0 \\
\frac{\partial V}{\partial W} &amp;= 1
\end{align*}
\]</span></p>
<p>These partial derivatives form the Jacobian matrix:</p>
<p><span class="math display">\[
J = \begin{bmatrix}
\sqrt{W} &amp; \frac{T}{2\sqrt{W}} \\
0 &amp; 1
\end{bmatrix}
\]</span></p>
<p>The determinant of this matrix is:</p>
<p><span class="math display">\[
\text{det}(J) = \sqrt{W} \times 1 - 0 \times \frac{T}{2\sqrt{W}} = \sqrt{W}
\]</span></p>
<p>Using the Jacobian, we can transform the joint PDF of <span class="math inline">\(Z\)</span> and <span class="math inline">\(V\)</span> into the joint PDF of $T ) and <span class="math inline">\(W\)</span>:</p>
<p><span class="math display">\[
f_{T,W}(t, w) = f_{Z,V}(t\sqrt{w}, w) \times |\text{det}(J)|
\]</span></p>
<p>Given <span class="math inline">\(f_{Z,V}(z, v) = f_Z(z) f_V(v)\)</span>, we substitute <span class="math inline">\(z = t\sqrt{w}\)</span> and <span class="math inline">\(v = w\)</span>, and include the Jacobian:</p>
<p><span class="math display">\[
f_{T,W}(t, w) = f_Z(t\sqrt{w}) f_V(w) \sqrt{w}
\]</span></p>
<p>which becomes</p>
<p><span class="math display">\[
f_{T,W}(t, w)  = \left( \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2 w}{2}} \right) \left( \frac{d^{d/2} w^{d/2 - 1} e^{-dw/2}}{2^{d/2} \Gamma(d/2)} \right) \cdot \sqrt{w}
\]</span></p>
<p>Finally, to find <span class="math inline">\(f_T(t)\)</span>, the marginal PDF of <span class="math inline">\(T\)</span>, we integrate out <span class="math inline">\(W\)</span>:</p>
<p><span class="math display">\[
f_T(t) = \int_0^\infty f_{T,W}(t, w) \, dw
\]</span></p>
<p>I’m not going to solve this here but the results of this integration is</p>
<p><span class="math display">\[
f_T(t) = \frac{\Gamma\left(\frac{d+1}{2}\right)}{\sqrt{d\pi} \Gamma\left(\frac{d}{2}\right)} \left(1 + \frac{t^2}{d}\right)^{-\frac{d+1}{2}}
\]</span></p>
<p>This gives us the Student’s t-distribution, matching the formula for the <a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">PDF of the t-distribution</a>.</p>
<p>We came all the way here to just to prove that</p>
<p><span class="math display">\[
   \frac{Z}{\sqrt{\frac{U}{d}}} \sim t_d
\]</span> with <span class="math inline">\(d\)</span> degrees of freedom and <span class="math inline">\(Z \sim N(0,1)\)</span> and <span class="math inline">\(U \sim chi^2_d\)</span>.</p>
</section>
<section id="t-statistics-for-ols" class="level2">
<h2 class="anchored" data-anchor-id="t-statistics-for-ols">t-statistics for OLS</h2>
<p>Consider a linear model given by: <span class="math display">\[
Y = X\beta + \epsilon
\]</span> In order to do a t-statistics, we need to first create a hypothesis. To create such a hypothesis we need a <strong>linear contrast</strong> that is a specific linear combination of the estimated regression coefficients. It’s typically represented as <span class="math inline">\(c'\beta\)</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(c\)</span> is a known <span class="math inline">\(p \times 1\)</span> vector, specifying the coefficients for the linear combination.</li>
<li><span class="math inline">\(\beta\)</span> is the <span class="math inline">\(p \times 1\)</span> vector of true regression coefficients.</li>
</ul>
<p>The purpose of <span class="math inline">\(c\)</span> is to focus on specific hypotheses about combinations of the parameters <span class="math inline">\(\beta\)</span>. For example, if we are interested in testing whether the sum of two coefficients is zero or some other value, <span class="math inline">\(c\)</span> would be constructed accordingly to weight these coefficients. Similarly, if we have a bunch coefficients and we want to test whether one of them is different from zero or not we can construct a <span class="math inline">\(p \times 1\)</span> vector which has all the elements zero except for the coefficient that we want to test.</p>
<p><span class="math inline">\(c'\beta\)</span> in fact gives us the numerator of the t-statistics. So we have</p>
<p><span class="math display">\[
t=\frac{c'\beta}{SE}
\]</span> We still need to figure out what <span class="math inline">\(SE\)</span> is.</p>
<p>Recall that <span class="math inline">\(SE\)</span> is the standard error of <span class="math inline">\(c'\beta\)</span>. So we need to derive the variance of the estimated regression coefficients (<span class="math inline">\(\hat{\beta}\)</span>). Although we have done that before, let’s go through it one more time.</p>
<p>The variance of a linear transformation <span class="math inline">\(A\)</span> of a random vector <span class="math inline">\(b\)</span> (where <span class="math inline">\(A\)</span> is a matrix and <span class="math inline">\(b\)</span> is a vector of random variables with variance-covariance matrix <span class="math inline">\(\Sigma\)</span>) is given by: <span class="math display">\[\text{Var}(Ab) = A \Sigma A'\]</span> Applying this to <span class="math inline">\(\hat{\beta}\)</span>: <span class="math display">\[\hat{\beta} = (X'X)^{-1}X'Y\]</span> <span class="math display">\[Y = X\beta + \epsilon\]</span> <span class="math display">\[\text{Var}(\hat{\beta}) = \text{Var}((X'X)^{-1}X'Y)\]</span> Since <span class="math inline">\(Y = X\beta + \epsilon\)</span> and <span class="math inline">\(X\beta\)</span> is non-random (deterministic), the variance part comes only from <span class="math inline">\(\epsilon\)</span>: <span class="math display">\[\text{Var}(\hat{\beta}) = \text{Var}((X'X)^{-1}X'(X\beta + \epsilon))\]</span> <span class="math display">\[\text{Var}(\hat{\beta}) = \text{Var}((X'X)^{-1}X'\epsilon)\]</span> Given <span class="math inline">\(\text{Var}(\epsilon) = \sigma^2 I\)</span>: <span class="math display">\[\text{Var}(\hat{\beta}) = (X'X)^{-1}X'(\sigma^2 I)X(X'X)^{-1}\]</span> <span class="math display">\[\text{Var}(\hat{\beta}) = \sigma^2 (X'X)^{-1}X'IX(X'X)^{-1}\]</span> <span class="math display">\[\text{Var}(\hat{\beta}) = \sigma^2 (X'X)^{-1}\]</span></p>
<p>The unbiased estimator of <span class="math inline">\(\sigma^2\)</span> (variance of the errors) is given by the Residual Sum of Squares (RSS) divided by the degrees of freedom, which is typically the number of observations minus the number of parameters estimated (including intercept). If <span class="math inline">\(n\)</span> is the number of observations and <span class="math inline">\(p\)</span> is the number of parameters (including the intercept), the degrees of freedom are <span class="math inline">\(n - p\)</span>: <span class="math display">\[\hat{\sigma}^2 = \frac{\text{RSS}}{n - p}\]</span></p>
<p><span class="math display">\[\text{RSS} = (Y - \hat{Y})'(Y - \hat{Y})\]</span> Given: <span class="math display">\[\hat{Y} = X\hat{\beta} = X(X'X)^{-1}X'Y\]</span> The RSS becomes: <span class="math display">\[\text{RSS} = (Y - X(X'X)^{-1}X'Y)'(Y - X(X'X)^{-1}X'Y)\]</span> <span class="math display">\[\text{RSS} = Y'Y - Y'X(X'X)^{-1}X'Y\]</span></p>
<p>Given this the formula for t-statistics for testing coefficients of linear regression will be</p>
<p><span class="math display">\[t=\frac{c'\beta}{\frac{ Y'Y - Y'X(X'X)^{-1}X'Y}{n-p}\sqrt{(X'X)^{-1}}}\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>calculate_t_statistic <span class="ot">&lt;-</span> <span class="cf">function</span>(X, Y, c) {</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate the OLS estimates for beta</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>  beta_hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> Y</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate RSS</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>  Y_hat <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_hat</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>  rss <span class="ot">&lt;-</span> <span class="fu">sum</span>((Y <span class="sc">-</span> Y_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Degrees of freedom</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)  <span class="co"># Number of observations</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)  <span class="co"># Number of parameters</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>  df <span class="ot">&lt;-</span> n <span class="sc">-</span> p</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Variance estimate</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>  sigma_squared_hat <span class="ot">&lt;-</span> rss <span class="sc">/</span> df</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculating c'beta</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>  c_beta_hat <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(c <span class="sc">%*%</span> beta_hat)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculating the variance of c'beta_hat</span></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>  var_c_beta_hat <span class="ot">&lt;-</span> sigma_squared_hat <span class="sc">*</span> (c <span class="sc">%*%</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> c)</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>  <span class="co"># t-statistic</span></span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>  t_statistic <span class="ot">&lt;-</span> c_beta_hat <span class="sc">/</span> <span class="fu">sqrt</span>(var_c_beta_hat)</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(t_statistic)</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct the model matrix 'X' for the fixed effect 'x' using data from 'data_int'.</span></span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span>x, <span class="at">data=</span>data_int)</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>y<span class="ot">&lt;-</span>data_int<span class="sc">$</span>y</span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">data.frame</span>(<span class="at">t=</span><span class="fu">calculate_t_statistic</span>(<span class="at">X =</span> X,<span class="at">Y =</span> y,<span class="at">c =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)),<span class="at">df=</span><span class="fu">nrow</span>(X)<span class="sc">-</span><span class="fu">ncol</span>(X),</span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a>      <span class="at">pvalue=</span><span class="dv">2</span><span class="sc">*</span><span class="fu">pt</span>(<span class="fu">abs</span>(<span class="fu">calculate_t_statistic</span>(<span class="at">X =</span> X,<span class="at">Y =</span> y,<span class="at">c =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))),<span class="at">df=</span><span class="fu">nrow</span>(X)<span class="sc">-</span><span class="fu">ncol</span>(X),<span class="at">lower.tail =</span> <span class="cn">FALSE</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          t df       pvalue
1 -6.932677 38 3.033548e-08</code></pre>
</div>
</div>
<p>Compare to <code>lm</code> function</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">summary</span>(<span class="fu">lm</span>(y<span class="sc">~</span><span class="dv">0</span><span class="sc">+</span>X)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y ~ 0 + X)

Residuals:
     Min       1Q   Median       3Q      Max 
-14.1384  -4.3018   0.0277   4.2264  17.5140 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
X(Intercept) 120.1292     6.8039  17.656  &lt; 2e-16 ***
Xx            -2.7184     0.3921  -6.933 3.03e-08 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 7.238 on 38 degrees of freedom
Multiple R-squared:  0.991, Adjusted R-squared:  0.9905 
F-statistic:  2094 on 2 and 38 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<section id="degrees-of-freedom-of-gls" class="level3">
<h3 class="anchored" data-anchor-id="degrees-of-freedom-of-gls">degrees of freedom of GLS</h3>
<p>In mixed models, determining the degrees of freedom (df) is complex due to the presence of both fixed and random effects. Unlike simple linear models where df is straightforward (number of observations minus number of parameters), mixed models must account for the hierarchical structure of the data and correlations within random effects. Methods like Satterthwaite and Kenward-Roger approximations are often used to estimate df. We are again going to just use the definition of GLS and not <span class="math inline">\(y = X\beta + Zb + \epsilon\)</span> and demonstrate how Satterthwaite approximation works.</p>
<p>We know from the very first sections the estimated fixed effects <span class="math inline">\(\hat{\beta}\)</span> are given by:</p>
<p><span class="math display">\[
\hat{\beta} = (X'V^{-1}X)^{-1}X'V^{-1}y
\]</span> where <span class="math inline">\(V = ZDZ' + \sigma^2\)</span> is the variance-covariance matrix of <span class="math inline">\(y\)</span>.</p>
<p>If you remember we also proved that the variance of the <span class="math inline">\(\beta\)</span> is <span class="math inline">\((X'V^{-1}X)^{-1}\)</span>.</p>
<p>But what we are interested is the test of coeffcient <span class="math inline">\(c'\hat{\beta}\)</span>. From the OLS part above we can show that</p>
<p><span class="math display">\[
\text{Var}(c'\hat{\beta}) = c' \text{Var}(\hat{\beta}) c = c' (X'V^{-1}X)^{-1} c
\]</span></p>
<p>The test statistic for testing <span class="math inline">\(c'\beta = 0\)</span> is constructed as: <span class="math display">\[
T = \frac{c'\hat{\beta}}{\sqrt{\text{Var}(c'\hat{\beta})}}
\]</span></p>
<p>Substitute the variance expression:</p>
<p><span class="math display">\[
\text{Var}(c'\hat{\beta}) = c' (X'V^{-1}X)^{-1} c
\]</span></p>
<p>So, the test statistic becomes: <span class="math display">\[
T = \frac{c'\hat{\beta}}{\sqrt{c' (X'V^{-1}X)^{-1} c}}
\]</span></p>
<p>This test statistic follows a t-distribution with degrees of freedom that can be approximated using the Satterthwaite approximation. However unlink OLS we don’t want to use <span class="math inline">\(n-p\)</span> as degrees of freedom. We need to estimate it.</p>
<p>If you remember from the definition of T-stats we derived <span class="math inline">\(\sqrt{c' (X'V^{-1}X)^{-1} c} \sim ch^2_d\)</span> (we still don’t know <span class="math inline">\(d\)</span>).</p>
<p>However, just earlier we proved that:</p>
<p><span class="math display">\[
\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}
\]</span> Let’s call the DF here (<span class="math inline">\(n-1\)</span>) is <span class="math inline">\(d\)</span> so we have</p>
<p><span class="math display">\[
\frac{(d)S^2}{\sigma^2} \sim \chi^2_{d}
\]</span> We know the definition <span class="math inline">\(S^2=c' (X'V^{-1}X)^{-1} c\)</span>. But <span class="math inline">\(\sigma^2\)</span> is true but unknow variance of the population. Let’s call this <span class="math inline">\(\sigma^2=c' (X'V_p^{-1}X)^{-1} c\)</span></p>
<p>We can now say that <span class="math inline">\(S^2 \approx \frac{\sigma^2}{d} \chi^2_{d}\)</span>. Given this we are going to match the first two momnets of <span class="math inline">\(S^2\)</span> and <span class="math inline">\(\frac{\sigma^2}{d} \chi^2_{d}\)</span> to see if we can derive <span class="math inline">\(d\)</span></p>
<p>Let’s start with expectation (first moment) <span class="math display">\[
\mathbb{E}(S^2) = \mathbb{E}\left(\frac{\sigma^2}{d} \chi^2_{d}\right) = \frac{\sigma^2}{d} \mathbb{E}(\chi^2_{d}) = \frac{\sigma^2}{d} d = \sigma^2
\]</span> Unfortunately <span class="math inline">\(d\)</span> did not stay in this moment. Let’s go to the second moment (variance).</p>
<p><span class="math display">\[
\text{Var}(S^2) = \text{Var}\left(\frac{\sigma^2}{d} \chi^2_{d}\right) = \left(\frac{\sigma^2}{d}\right)^2 \text{Var}(\chi^2_{d}) = \left(\frac{\sigma^2}{d}\right)^2 \cdot 2d = \frac{2\sigma^4}{d}
\]</span></p>
<p>This is much better. We have <span class="math inline">\(d\)</span> left in the formula. So we have</p>
<p><span class="math display">\[
\text{Var}(S^2) =  \frac{2\sigma^4}{d}
\]</span></p>
<p>We can write:</p>
<p><span class="math display">\[
d=\frac{2\sigma^4}{Var(S^2)}
\]</span> We can estimate <span class="math inline">\(\sigma^2\)</span> as <span class="math inline">\(c' (X'V^{-1}X)^{-1} c\)</span> which is <span class="math inline">\(S^2\)</span>. so we can write</p>
<p><span class="math display">\[
d=\frac{2(c' (X'V^{-1}X)^{-1} c)^2}{Var(c' (X'V^{-1}X)^{-1} c)}
\]</span> The numerator is simple to calculate but denominator is more complex.</p>
<p>Remember that <span class="math inline">\(c' (X'V^{-1}X)^{-1} c\)</span> involes estimating <span class="math inline">\(V\)</span> which itself is a function of a parameter <span class="math inline">\(\theta\)</span>. To fix this we are going to use the delta method. The delta method can be used to approximate the variance of a function of a random variable.</p>
<p>If <span class="math inline">\(g(\theta)\)</span> is a differentiable function of <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(\theta\)</span> has an approximate normal distribution <span class="math inline">\(N(\hat{\theta}, \Sigma)\)</span> where <span class="math inline">\(\Sigma\)</span> is the covariance matrix of <span class="math inline">\(\theta\)</span>), the variance of <span class="math inline">\(g(\theta)\)</span> can be approximated as:</p>
<p><span class="math display">\[
\text{Var}(g(\theta)) \approx \nabla g(\hat{\theta})' \Sigma \nabla g(\hat{\theta})
\]</span> where <span class="math inline">\(\nabla g(\hat{\theta})\)</span> is the gradient (vector of first partial derivatives) of <span class="math inline">\(g\)</span> evaluated at $ $ and <span class="math inline">\(g(\theta) = c' (X'V^{-1}X)^{-1} c\)</span>.</p>
<p>Therefore our final df becomes</p>
<p><span class="math display">\[
d=\frac{2g(\theta)}{g(\hat{\theta})' \Sigma \nabla g(\hat{\theta})}
\]</span></p>
<p>We need to compute <span class="math inline">\(\nabla g(\theta)\)</span>, the gradient of <span class="math inline">\(g\)</span> with respect to <span class="math inline">\(\theta\)</span>. This involves differentiating <span class="math inline">\(c' (X'V^{-1}X)^{-1} c\)</span> with respect to each element of <span class="math inline">\(\theta\)</span>. We are just going to estimate this here but in practice this can be found using Jacobian. and <span class="math inline">\(\Sigma\)</span> is going to be estimated using the Hessian of the log-likelihood. Let’s see how we can implement this</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(numDeriv)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>simulate_grouped_trend <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">group_count =</span> <span class="dv">5</span>, <span class="at">points_per_group =</span> <span class="dv">10</span>, <span class="at">global_slope =</span> <span class="sc">-</span><span class="dv">10</span>, <span class="at">global_intercept =</span> <span class="dv">30</span>, <span class="at">group_slope =</span> <span class="dv">2</span>, <span class="at">noise_sd =</span> <span class="dv">50</span>,<span class="at">noise_sd2=</span><span class="dv">2</span>) {</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co"># Setting a seed for reproducibility</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Initialize an empty data frame to store the simulated data</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>  data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">numeric</span>(), <span class="at">y =</span> <span class="fu">numeric</span>())</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Loop to create each group</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>group_count) {</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    x_start <span class="ot">&lt;-</span> <span class="dv">12</span> <span class="sc">+</span> (i <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> (<span class="dv">10</span> <span class="sc">/</span> group_count) <span class="co"># Stagger the start of x for each group</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> <span class="fu">runif</span>(points_per_group, <span class="at">min =</span> x_start, <span class="at">max =</span> x_start <span class="sc">+</span> (<span class="dv">10</span> <span class="sc">/</span> group_count))</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply a local positive trend within the group, but maintain the global negative trend</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>    local_intercept <span class="ot">&lt;-</span> global_intercept <span class="sc">+</span> global_slope <span class="sc">*</span> (x_start <span class="sc">+</span> (<span class="dv">10</span> <span class="sc">/</span> (<span class="dv">2</span> <span class="sc">*</span> group_count))) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> noise_sd)</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> local_intercept <span class="sc">+</span> group_slope[i] <span class="sc">*</span> (x <span class="sc">-</span> x_start) <span class="sc">+</span> <span class="fu">rnorm</span>(points_per_group, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> noise_sd2)</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combine this group with the overall dataset</span></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    group_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y,<span class="at">group=</span>i)</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>    data <span class="ot">&lt;-</span> <span class="fu">rbind</span>(data, group_data)</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(data)</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a><span class="co"># generate simulated data</span></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>data_int <span class="ot">&lt;-</span> <span class="fu">simulate_grouped_trend</span>(<span class="at">group_count =</span> <span class="dv">4</span>,<span class="at">points_per_group =</span> <span class="dv">10</span>,<span class="at">global_slope =</span> <span class="sc">-</span><span class="dv">2</span>,<span class="at">global_intercept =</span> <span class="dv">100</span>,<span class="at">group_slope =</span> <span class="fu">c</span>(<span class="dv">6</span>,<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">1</span>),<span class="at">noise_sd =</span> <span class="dv">5</span>,<span class="at">noise_sd2=</span><span class="dv">2</span>)</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a><span class="co"># set group to factor</span></span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>data_int<span class="sc">$</span>group <span class="ot">&lt;-</span> <span class="fu">factor</span>(data_int<span class="sc">$</span>group)</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Design matrix of random effect</span></span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span><span class="dv">0</span><span class="sc">+</span>data_int<span class="sc">$</span>group)</span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct the model matrix 'X' for the fixed effect 'x' using data from 'data_int'.</span></span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span>x, <span class="at">data=</span>data_int)</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a><span class="co"># get groups (random effects)</span></span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>groups <span class="ot">&lt;-</span> <span class="fu">unique</span>(data_int<span class="sc">$</span>group)</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>y<span class="ot">&lt;-</span>data_int<span class="sc">$</span>y</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a>c<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the log-likelihood function (as negative log-likelihood)</span></span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a>log_reml <span class="ot">&lt;-</span> <span class="cf">function</span>(theta) {</span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a>  D <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="at">x =</span> theta[<span class="dv">2</span>], <span class="at">ncol =</span> <span class="fu">length</span>(groups), <span class="at">nrow =</span> <span class="fu">length</span>(groups))</span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a>  I <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">1</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(X), <span class="at">ncol =</span> <span class="fu">nrow</span>(X))</span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a>  V <span class="ot">&lt;-</span> theta[<span class="dv">1</span>] <span class="sc">*</span> (I <span class="sc">+</span> Z <span class="sc">%*%</span> D <span class="sc">%*%</span> <span class="fu">t</span>(Z))</span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a>  Beta <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> y</span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(<span class="fu">log</span>(<span class="fu">det</span>(V)) <span class="sc">+</span> <span class="fu">log</span>(<span class="fu">det</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> X)) <span class="sc">+</span> <span class="fu">t</span>(y <span class="sc">-</span> X <span class="sc">%*%</span> Beta) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> (y <span class="sc">-</span> X <span class="sc">%*%</span> Beta)))</span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-56"><a href="#cb33-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-57"><a href="#cb33-57" aria-hidden="true" tabindex="-1"></a>theta<span class="ot">&lt;-</span>optimization<span class="sc">::</span><span class="fu">optim_nm</span>(<span class="at">fun =</span> log_reml, <span class="at">k =</span> <span class="dv">2</span>, <span class="at">start =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="at">maximum=</span> <span class="cn">FALSE</span>, <span class="at">tol =</span> <span class="fl">0.0000000000001</span>)<span class="sc">$</span>par</span>
<span id="cb33-58"><a href="#cb33-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-59"><a href="#cb33-59" aria-hidden="true" tabindex="-1"></a>theta<span class="ot">&lt;-</span><span class="fu">matrix</span>(theta,<span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb33-60"><a href="#cb33-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the Hessian at the estimates</span></span>
<span id="cb33-61"><a href="#cb33-61" aria-hidden="true" tabindex="-1"></a>hessian_matrix <span class="ot">&lt;-</span> <span class="fu">hessian</span>(log_reml, theta)</span>
<span id="cb33-62"><a href="#cb33-62" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">solve</span>(hessian_matrix)</span>
<span id="cb33-63"><a href="#cb33-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-64"><a href="#cb33-64" aria-hidden="true" tabindex="-1"></a><span class="co"># define g matrix</span></span>
<span id="cb33-65"><a href="#cb33-65" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="cf">function</span>(theta) {</span>
<span id="cb33-66"><a href="#cb33-66" aria-hidden="true" tabindex="-1"></a>  D <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="at">x =</span> theta[<span class="dv">2</span>], <span class="at">ncol =</span> <span class="fu">length</span>(groups), <span class="at">nrow =</span> <span class="fu">length</span>(groups))</span>
<span id="cb33-67"><a href="#cb33-67" aria-hidden="true" tabindex="-1"></a>  I <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">1</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(X), <span class="at">ncol =</span> <span class="fu">nrow</span>(X))</span>
<span id="cb33-68"><a href="#cb33-68" aria-hidden="true" tabindex="-1"></a>  V <span class="ot">&lt;-</span> theta[<span class="dv">1</span>] <span class="sc">*</span> (I <span class="sc">+</span> Z <span class="sc">%*%</span> D <span class="sc">%*%</span> <span class="fu">t</span>(Z))</span>
<span id="cb33-69"><a href="#cb33-69" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>((c) <span class="sc">%*%</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(c))</span>
<span id="cb33-70"><a href="#cb33-70" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-71"><a href="#cb33-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-72"><a href="#cb33-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the gradient of a function by numerical approximation</span></span>
<span id="cb33-73"><a href="#cb33-73" aria-hidden="true" tabindex="-1"></a>g_grad<span class="ot">&lt;-</span><span class="fu">as.matrix</span>(<span class="fu">grad</span>(g,theta))</span>
<span id="cb33-74"><a href="#cb33-74" aria-hidden="true" tabindex="-1"></a>var_S2<span class="ot">&lt;-</span><span class="fu">t</span>(g_grad)<span class="sc">%*%</span>Sigma<span class="sc">%*%</span>g_grad</span>
<span id="cb33-75"><a href="#cb33-75" aria-hidden="true" tabindex="-1"></a>S2<span class="ot">&lt;-</span><span class="fu">g</span>(theta)</span>
<span id="cb33-76"><a href="#cb33-76" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> (<span class="dv">2</span> <span class="sc">*</span> S2<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> var_S2</span>
<span id="cb33-77"><a href="#cb33-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-78"><a href="#cb33-78" aria-hidden="true" tabindex="-1"></a>D <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="at">x =</span> theta[<span class="dv">2</span>],<span class="at">ncol =</span> <span class="fu">length</span>(groups),<span class="at">nrow =</span> <span class="fu">length</span>(groups))</span>
<span id="cb33-79"><a href="#cb33-79" aria-hidden="true" tabindex="-1"></a>I <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">1</span>,<span class="at">nrow =</span> <span class="fu">nrow</span>(X),<span class="at">ncol =</span> <span class="fu">nrow</span>(X))</span>
<span id="cb33-80"><a href="#cb33-80" aria-hidden="true" tabindex="-1"></a>V<span class="ot">&lt;-</span>theta[<span class="dv">1</span>]<span class="sc">*</span>(I<span class="sc">+</span>Z<span class="sc">%*%</span>D<span class="sc">%*%</span><span class="fu">t</span>(Z))</span>
<span id="cb33-81"><a href="#cb33-81" aria-hidden="true" tabindex="-1"></a>Beta <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(V) <span class="sc">%*%</span> y</span>
<span id="cb33-82"><a href="#cb33-82" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-83"><a href="#cb33-83" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">data.frame</span>(<span class="at">df=</span>d,<span class="at">t=</span>Beta[<span class="dv">2</span>]<span class="sc">/</span><span class="fu">sqrt</span>(S2),<span class="at">pvalue=</span><span class="dv">2</span><span class="sc">*</span><span class="fu">pt</span>(Beta[<span class="dv">2</span>]<span class="sc">/</span><span class="fu">sqrt</span>(S2),<span class="at">df=</span>d,<span class="at">lower.tail =</span> <span class="cn">FALSE</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       df       t      pvalue
1 36.7423 4.33864 0.000107513</code></pre>
</div>
</div>
<p>Compare this to <code>lmerTest</code> results:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>model<span class="ot">&lt;-</span><span class="fu">lmer</span>(y<span class="sc">~</span>x<span class="sc">+</span>(<span class="dv">1</span><span class="sc">|</span>group),<span class="at">data=</span>data_int,<span class="at">REML =</span> <span class="cn">TRUE</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(emmeans)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>trends <span class="ot">&lt;-</span> <span class="fu">emtrends</span>(model, <span class="at">var =</span> <span class="st">"x"</span>, <span class="at">lmer.df =</span> <span class="st">"satterthwaite"</span>)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">summary</span>(trends, <span class="at">infer =</span> <span class="cn">TRUE</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    x x.trend    SE   df lower.CL upper.CL t.ratio p.value
 17.1    2.56 0.589 36.7     1.36     3.75   4.339  0.0001

Degrees-of-freedom method: satterthwaite 
Confidence level used: 0.95 </code></pre>
</div>
</div>
<p>We get similar results and this concludes our chapter on math behind mixed models.</p>
</section>
</section>
</section>
<section id="summary" class="level1">
<h1>Summary</h1>
<p>In this section, we went through how mixed models works starting from OLS all the way to GLS and estimation and hypothesis testing. Please note that some of the derivations are very in efficient way of solving the equations in practice there are highly optimized methods to solve the mixed models.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./lmm.html" class="pagination-link" aria-label="Mixed models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Mixed models</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>