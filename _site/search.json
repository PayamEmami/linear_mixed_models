[
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "environment.html",
    "href": "environment.html",
    "title": "Environment",
    "section": "",
    "text": "In order to run the code in this turorial we will need to install a few packages.\n\ninstall.packages(c(\"lme4\", \"emmeans\", \"ggplot2\"))\n\n\n\n\nWe are going to use human circadian data for"
  },
  {
    "objectID": "environment.html#packages",
    "href": "environment.html#packages",
    "title": "Environment",
    "section": "",
    "text": "In order to run the code in this turorial we will need to install a few packages.\n\ninstall.packages(c(\"lme4\", \"emmeans\", \"ggplot2\"))"
  },
  {
    "objectID": "environment.html#data",
    "href": "environment.html#data",
    "title": "Environment",
    "section": "",
    "text": "We are going to use human circadian data for"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nIn this short introduction we, are going to learn about linear mixed models. We will start with the issues with modelling hierarchical data using simple linear regression. After that, we will formally introduce mixed models and show some applications of them in the context of biological data analysis.\n\n\n\n\n\n\nNote\n\n\n\nThis chapter offers an introduction to linear mixed models, focusing on their applications within biological data analysis. It is important to understand that our exploration of mixed models here is not exhaustive. The field of mixed models is extensive and complex, with a variety of techniques and considerations that go beyond the scope of this text. If you seek a comprehensive understanding of the subject, please consult additional resources and literature dedicated to statistical modelling and mixed models. Our goal is to provide a foundational understanding and illustrate the potential of mixed models in addressing hierarchical data challenges in biological research."
  },
  {
    "objectID": "lmm.html",
    "href": "lmm.html",
    "title": "Mixed models",
    "section": "",
    "text": "As briefly mentioned in the previous chapter, mixed-effects models (also known as multilevel or hierarchical models) are used introduce random effects to account for the cluster variations. This approach allows us to model the dependency structure due to clusters without having to estimate a separate parameter for each cluster effect. Mixed-effects models incorporate the clustering in the variance-covariance structure of the data, which provides a more parsimonious and interpretable model, especially when our primary interest lies in the fixed effects, such as the main effect of Age in the previous example.\nCorrelated is more common in biology than one might think. For example:\n\nRepeated Measurements: Suppose you’re measuring a patient’s blood pressure over time. Measurements from the same patient will naturally be more similar to each other than those from different patients. A mixed-effects model handles this by modeling the variation between patients.\nNested Designs: Imagine studying the effectiveness of a new drug on mice housed within different cages. Mice within the same cage might be more alike due to shared environments. A mixed-effects model accounts for both variation between mice and variation between cages.\nLongitudinal Studies: When tracking the growth of trees over time, measurements from the same tree will be more correlated than those from different trees. Here, a mixed-effects model can account for the individual tree’s growth patterns.\nMulti-omics Studies: Omics data (genomics, transcriptomics, proteomics, etc.) from the same individual will show correlations due to underlying biological pathways. A mixed-effects model can tease apart the effects of different omic layers while accounting for their relationships within a person.\nFamily-based Disease Studies: Genetic variations and environmental exposures tend to cluster within families. Mixed-effects models can effectively model these familial factors when investigating how they contribute to disease risk.\n\nMixed-effects models are a powerful tool for analyzing complex biological datasets, offering improved accuracy and providing valuable insights into the sources of variability in disease studies.\n\n\nMixed models get their name because they combine two types of effects: fixed effects and random effects. This combination is what makes them so valuable for analyzing complex data.\n\n\nThe statistical models you might be familiar with, like linear regression or ANOVA, focus on “fixed effects.” These are the direct relationships between your chosen variables and the outcome you’re interested in. Fixed effects are the factors you control or are primarily interested in. If you’re studying the effect of age on protein expression, age is your fixed effect. A fixed effect will contain all possible levels of a factor in the experiment, measuring a few specific instances of interest. The key premise of the fixed-effect model is that there is one true effect size that is common to all the studies being analyzed. This model assumes that any observed variation among study results is solely due to sampling error within studies, and not due to actual differences in effect sizes across studies. Therefore, the goal of a fixed-effect analysis is to estimate this common true effect size.\nLet’s think about a research question asking if a specific vitamin supplement reduces the duration of common cold symptoms. Multiple clinical trials have been conducted to evaluate the effect of this vitamin supplement on the duration of common cold symptoms. Each trial uses the same dosage of the supplement and has similar participant demographics (e.g., age, health status). The researchers conducting the analysis believe that, given the standardized intervention and population, the supplement should have a consistent effect across all trials.\nStudies Reviewed: - Study 1: 100 participants, average reduction in symptom duration of 1.5 days. - Study 2: 150 participants, average reduction in symptom duration of 1.4 days. - Study 3: 120 participants, average reduction in symptom duration of 1.6 days. - Study 4: 130 participants, average reduction in symptom duration of 1.3 days.\nFixed-Effect Model Assumptions in this Example:\n\nSingle True Effect Size: The assumption here is that there is one true effect size reflecting the average reduction in the duration of common cold symptoms due to the vitamin supplement. This is based on the controlled administration of the supplement and the homogeneous nature of the participant groups across the trials.\nVariation Due to Sampling Error: The slight differences in observed effects among the studies (e.g., some showing a 1.3-day reduction while others show a 1.6-day reduction) are attributed to sampling error—random variation due to the different samples of participants in each study.\nGoal: To estimate the common true effect size of the vitamin supplement on reducing the duration of cold symptoms, a fixed-effect analysis is employed. This involves calculating a weighted average of the effect sizes from the individual trials, with greater weight given to larger trials since they are less prone to sampling error.\n\nThrough the fixed-effect model analysis, it might be concluded that the specific vitamin supplement leads to an average reduction of approximately 1.4 days in the duration of common cold symptoms. This conclusion is based on the premise that the variation in results across the trials is solely due to sampling error, without accounting for potential underlying differences in participant response to the supplement, as such variability is assumed to be negligible given the standardized conditions of the trials.\nThe primary limitation of using a fixed-effect model, lies in its underlying assumptions about the homogeneity of effect sizes across studies. This model assumes that there is one true effect size that applies to all studies, and any observed variability in outcomes is solely due to sampling error. However, this assumption can be overly simplistic and may not always hold true, especially in biological and medical research where variability is the norm rather than the exception.\n\n\n\nThe random-effects offer a more flexible and realistic approach, especially in contexts where between-study variability is expected or observed. The defining feature of the random-effects model is the assumption that there is a distribution of true effect sizes across studies, and the goal is to estimate the mean of this distribution. This approach accounts for variation not only within studies (due to sampling error) but also between studies, recognizing that different studies may inherently have different true effect sizes due to various factors (e.g., differences in populations, interventions, outcomes measured).\nReturning to our scenario of evaluating the impact of a vitamin supplement on the duration of common cold symptoms, let’s consider that the clinical trials are conducted across various geographical locations, each with a distinct demographic and environmental profile. These location-based groups could inherently influence how participants respond to the supplement, independent of the supplement’s effect itself. We would introduce random intercepts for each geographical location. This means that while we are still interested in estimating the overall effect of the vitamin supplement on symptom duration, we also acknowledge that each location starts from a different baseline in terms of average symptom duration without the supplement. However, we actually don’t directly estimate these baselines but rather assume that these baselines are randomly samples from a distribution of baselines in the population with certain statistical properties. Therefore, we are not interested in each single baseline but we want to know statistical properties of the distribution of baselines.\nIn summary, the fixed-effect model’s generalization is limited to the “levels” within the experiment, meaning it applies to populations or conditions that exactly match those of the included studies. It treats the effect as if it were a fixed property of the specific scenarios tested. In contrast, the random-effects model allows for generalization to “levels beyond those that took part in the study,” acknowledging that the effect sizes are part of a distribution across a wider population. This model treats the effect as a variable property reflecting a range of possible scenarios, including those not directly observed in the analysis.\nThis might sounds too complicated or abstract. We are going to clarify this with a few examples.\n\n\n\n\nIn this section, we will try to clarify what information do we need in order to fit a linear model.\n\n\nThe first and most important thing is to understand the data we are dealing with. Before we can make any attempt to use mixed models, we need to know what kind of grouping structure we want to capture. Mixed models are particularly useful when data is collected in groups or clusters, such as measurements from different subjects, schools, geographical locations, or time points within subjects. Identifying the hierarchy or nested structure where observations are grouped within higher-level units is crucial. This grouping structure can significantly influence the correlations among observations, as data points within the same group are likely to be more similar to each other than to data points in different groups. Understanding this structure allows us to correctly model the random effects, which account for the variability at different levels of the data hierarchy, thereby improving the accuracy and interpretability of our results. By incorporating these random effects, mixed models enable us to make inferences about both the fixed effects, which are consistent across groups, and the random effects, which vary across groups.\nWithout having this information, we cannot use standard mixed models. This grouping must be categorical, not continuous, as mixed models rely on categorical variables to define the levels of the hierarchy or nested structure within the data. Categorical grouping allows us to classify observations into distinct categories or groups that share common characteristics.\nIt is obvious but worth mentioning that these grouping should not be confused with random effects.Random effects are calculated based on the assumption that data points within the same group may share more similarities with each other than with data points from different categories, thus accounting for the within-group correlation. Therefore, the identification of categorical groupings is not just a step in the analysis but a prerequisite for accurately calculating and interpreting the random effects that are fundamental to the mixed model’s approach to data analysis.\n\n\n\n\n\n\nImportant\n\n\n\nYou cannot use continuous data as grouping if it does not have levels. Without these discrete levels, it becomes impossible to define the clusters or hierarchies necessary for calculating random effects. To effectively employ mixed models, one must either use inherently categorical variables or discretize continuous variables into meaningful categories.\n\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes it is not clear what grouping of data exist. Or even what should be used as grouping. In often (but not always), the grouping is something different than what we are interested in. For example, let’s say, we have gathered data from patients with a disease and attempted to find a matched control based on certain characteristics such as age, BMI etc. The group of interest here is disease vs. healthy and a possible grouping variable for random effect would be a match ID for the a each disease and its matched control.\n\n\nThe last thing i would like to mention is that, how many levels the random effect grouping should have in order to be included in the model. Although there is no golden consensus about it but most people tend to agree that the number levels of the group factor should be more than five to be accurately modeled by mixed models. Although some argue that with even two levels, mixed model will be like classical linear regression one should be extra cautious about it.\n\n\n\nNow that we have identified the grouping structure of the data, we need to decide what effects are fixed and what effects are random with respect to the grouping structure of the data. This is again might be confusing a bit but if we start asking ourselves a few questions, it might clarify the whole concept.\nIt is important to stress that the main reason we have chosen to go with mixed models is that we believe there is a grouping in our data that somehow affects some of the effects in our study. And by saying effect we mean the influence or impact that a particular variable has on the outcome of interest.\nSo here comes the first and a key question:\n\nDo we believe a particular effect varies across the grouping structure of the data? if we do not believe that a particular effect varies across the grouping structure of the data, then we may consider modeling this effect as a fixed effect without taking care of the grouping structure.\nIf the answer to this question is yes. Then we ask ourselves another key question.\nAre we interested in estimating the effect specific to the groups? Now that we have decided to take care of the grouping structure. We might be interested in estimating the effect of interest in each of groups or even compare them across the groups. You might want to even calculate p-values for these. If this is so then you might want to model these as fixed effect for example by including them as covariate (or blocking/control variable or even interaction) in the model. Doing so however, will force the model to estimate these effect thus spending degrees of freedom.\nIf however, we are still interested in taking care of the grouping but instead of estimating the effect in each group, we are happy just to know how much variability is in the effect of interest across the groups, then we are going to consider modeling that effect as random. This will instruct the modeling application to instead of estimating effects for each group (could be hundred of groups), just come up with estimates of few parameters showing the variability. Please note that these effects can be anything from baseline, slopes, interaction etc.\nDo we have a large number of groups, and are some of these groups potentially unobserved? Random effects models are particularly useful when dealing with a large number of groups, especially when some groups in the population might not be represented in the sample. Random effects assume that the observed groups are a random sample from a larger population of groups, allowing for generalization beyond the specific groups in the study.\nHowever, as said before, instead of estimating parameters for each group, then calculate variance across the groups. If the number of levels in each group is too few (like two or something like that), the estimated variance is not going to be accurate. So you might want to actually get back and consider using classical models.\n\nBy asking these questions, we can better clarify the roles of fixed and random effects. These things are going to be further clarified when we start working on a few examples.\n\n\n\n\nWe get back our simulated data in the previous chapter and try to model it using mixed models. Before proceeding make sure that you have installed lme4 package as this is what we are going to use throughout this section.\nJust to remind you, the idea was to examine the relationship between Age and Protein expression.\n\n\nCode\nsimulate_grouped_trend &lt;- function(group_count = 5, points_per_group = 10, global_slope = -10, global_intercept = 30, group_slope = 2, noise_sd = 50) {\n  set.seed(123) # Setting a seed for reproducibility\n  \n  # Initialize an empty data frame to store the simulated data\n  data &lt;- data.frame(x = numeric(), y = numeric())\n  \n  # Loop to create each group\n  for (i in 1:group_count) {\n    x_start &lt;- 12 + (i - 1) * (10 / group_count) # Stagger the start of x for each group\n    x &lt;- runif(points_per_group, min = x_start, max = x_start + (10 / group_count))\n    \n    # Apply a local positive trend within the group, but maintain the global negative trend\n    local_intercept &lt;- global_intercept + global_slope * (x_start + (10 / (2 * group_count))) + rnorm(1, mean = 0, sd = noise_sd)\n    y &lt;- local_intercept + group_slope * (x - x_start) + rnorm(points_per_group, mean = 0, sd = noise_sd)\n    \n    # Combine this group with the overall dataset\n    group_data &lt;- data.frame(x = x, y = y,group=i)\n    data &lt;- rbind(data, group_data)\n  }\n  \n  return(data)\n}\n\n# generate simulated data\ndata_int &lt;- simulate_grouped_trend(group_count = 4,points_per_group = 10,global_slope = -2,global_intercept = 100,group_slope = 4,noise_sd = 5)\n\n# set group to factor\n\ndata_int$group &lt;- factor(data_int$group)\n# plot the data\nplot(data_int$x,data_int$y,xlab=\"Age\",ylab=\"Protein expression\",col=data_int$group,pch=as.numeric(data_int$group))\n\n\n\n\n\n\n\n\nFigure 1: A scatter plot of Age vs. Protein expression\n\n\n\n\n\nBecause there are some grouping structure in data, we decide to take care of it. Let’s say that each group is a random clinic that we decided to get some sample from. So we now have identified the grouping structure in our data that might influence the effect of interest (Age).\nWhat we believe is that the effect of Age on the protein expression is constant in each of the groups. However, due to either technical issues with the instruments or even demographics, each group might have different baselines (starting point) for the protein expression. This gives us a hint that we might have to model intercept differently for each group. This decision gives us two choices, model this intercept as fixed or random effect.\nMe as a researcher tell you in addition to the effect of age, I would like to know how much differences exactly are between each of these clinics. Therefore i want to use fixed effect to exactly pinpoint the estimated baseline for each of the clinic and compare them against each other.\nYou however, argue that there is no reason to do that. We just picked a few random clinics, there is no point in knowing by how much clinic 1 is different to clinic 2 because we could have selected and other random clinics in the world. So let’s model this as random effect to get the variance of the intercepts across clinics. By modeling the intercept as a random effect, we acknowledge that each clinic has its own baseline level of protein expression due to various unmeasured factors such as technical differences in equipment or demographic variations. This approach allows us to account for the inherent variability between clinics without focusing on the specific differences between any two clinics. Instead, we aim to understand the general variability of baseline protein expression levels across clinics.\nWe start using the classical approach (as we saw in the previous chapter) using lm function with the following formula y ~ 1+x+group:\n\n\n\n\n\n\nNote\n\n\n\nWe have included 1 in the model just to stress that there is an intercept in the model. Obviously, y ~ 1+x+group is identical to y ~ x+group\n\n\n\n\nCode\n# Fit a linear regression model\nmodel_lm &lt;- lm(y ~ 1+x+group, data = data_int)\nsummary(model_lm)\n\n\n\nCall:\nlm(formula = y ~ 1 + x + group, data = data_int)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.7577 -3.2657 -0.1872  1.4925 10.7686 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   58.798     15.341   3.833 0.000505 ***\nx              2.213      1.136   1.949 0.059406 .  \ngroup2       -23.072      3.133  -7.364 1.30e-08 ***\ngroup3       -31.187      6.159  -5.064 1.32e-05 ***\ngroup4       -34.509      8.716  -3.959 0.000351 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.625 on 35 degrees of freedom\nMultiple R-squared:  0.786, Adjusted R-squared:  0.7615 \nF-statistic: 32.13 on 4 and 35 DF,  p-value: 2.832e-11\n\n\nWe saw this result before. This model has four different intercepts, one for each group and a single slope that is identical for each group.\nGiven this result, we have estimated the group coefficients and we can go ahead with comparing the groups or do whatever we wanted to do with these estimates. It is important that here we don’t have a global slope. We have four slopes, one for each group.\nNow it is time to do the same analysis using the mixed model approach. The model that we are going fit using lmer function from lme4 is of form y ~ 1+x+(1|group). This part of the model, y ~ 1+x, we already know what it is. However, what this part (1|group) is new. The right part of | shows what variable we want to use as grouping factor by which we are going to fit our random effect (That is the group in our case). The left part of | is the effect we want to consider random. The left part is like a formula, similar to classical regression. which in this case, we just used 1. This means for our main model, y ~ 1+x which has an intercept (1) and a slope x, we would like to consider the intercept as random effect so we use 1.\nNow that we know the structure of the model, let’s go ahead and do the modeling:\n\n\nCode\nlibrary(lme4)\n# Fit a mixed linear regression model\nmodel_lmm &lt;- lmer(y ~ 1+x+(1|group), data = data_int)\nsummary(model_lmm)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ 1 + x + (1 | group)\n   Data: data_int\n\nREML criterion at convergence: 244.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.73343 -0.61480 -0.00871  0.37734  2.22117 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n group    (Intercept) 184.10   13.568  \n Residual              21.57    4.645  \nNumber of obs: 40, groups:  group, 4\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   50.205     18.893   2.657\nx              1.418      1.030   1.377\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.932\n\n\nThis output has some key differences to the previous approach. The first thing is in the fixed effect part:\n\n\nCode\nas.list(summary(model_lmm))[['coefficients']]\n\n\n             Estimate Std. Error  t value\n(Intercept) 50.205206   18.89260 2.657401\nx            1.417913    1.02998 1.376642\n\n\nUnlike lm, here there is no estimate for each group. There is a single intercept and a single slope. These are global trends in the data with their corresponding Std. Error and t value. But what happen with random effect and grouping of the data. This is what we can see in the Random effects part of the output:\n\n\nCode\nas.list(summary(model_lmm))[['varcor']]\n\n\n Groups   Name        Std.Dev.\n group    (Intercept) 13.5684 \n Residual              4.6447 \n\n\nDepending of what type of random effect we have been fitting this table can have different headers. But generally, Groups, Name, Variance, Std.Dev. and Corr are part of it.\nEach row of this table correspond to a source of variation in our data that is random because of a certain reason. In our model, we decided to consider only Intercept as a random effect for group. Therefore, the first row of the table tells us that there is random variability in Intercept between different group. This variability is quantified by a variance of 184.101 and a standard deviation of 13.568. This means that the average effect (or intercept) across groups can deviate from the overall (global) intercept by about 13.568 units, indicating substantial between-group variability. This is important to note that, we have actually never estimated any intercept for each group directly. But we have estimated the variability of intercepts across different groups, which allows us to account for the fact that each group may have a different starting point or baseline level in the response variable, even though these specific baseline levels are not directly calculated. You can think about this as the variance of the distribution of intercepts across the groups. Essentially, we’re modeling the distribution from which each group’s intercept is drawn.\nThe second row, labeled Residual, refers to the variability within each group that is not explained by the model predictors (because of random sampling). The variance here is 21.574, with a standard deviation of 4.645. This residual variance represents the individual differences or noise within groups after accounting for the modeled effects, including the random intercepts.\nSo before moving on let’s summarize what get got so far.\nWe used mixed models with a random intercept to model the effect of age on protein expression. We got an estimated global intercept (baseline) and slope (main effect of interest). We took care of the grouping structure of our data without directly estimating any additional parameters. In fact, what we estimated was intercept, slope, variance of Intercept and variance of residuals. One of the coolest thing here is that we would have estimated the exact same number of parameters if we our grouping structure had a lot more levels. But if we want to use grouping as covariate (in a classic model), we should modeled all of these levels!\n\n\nDespite that we have not estimated the intercept for each of the group, we can still predict what would the intercept be for each them. We are going to see in the math section how that can be done, but for now, we leave the details and directly use the function ranef from lme4 package.\n\n\nCode\nprint(ranef(model_lmm))\n\n\n$group\n  (Intercept)\n1   19.059412\n2   -2.116552\n3   -7.752078\n4   -9.190783\n\nwith conditional variances for \"group\" \n\n\nIn this particular case, it gave us the differences from the overall intercept estimated in the mixed effects model. By extracting the random effects using the ranef function, we obtain insights into the specific adjustments needed for each group. We can now use the global intercept and these adjustments to visualize the predicted intercept.\n\n\nCode\n# Plotting the initial scatter plot with points colored by group and shape determined by group\nplot(data_int$x, data_int$y, xlab=\"Age\", ylab=\"Protein expression\",\n     col=data_int$group, pch=as.numeric(data_int$group))\n\n# Looping through each unique group in the dataset\nfor (group in unique(data_int$group)) {\n  # Subset data for the current group\n  group_data &lt;- data_int[data_int$group == group,]\n\n  # Calculate the intercept for the group by adding the fixed intercept to the group's random effect\n  intercept &lt;- fixef(model_lmm)[1] + ranef(model_lmm)[[1]][group, 1]\n\n  # Extract the fixed effect slope from the model\n  slope &lt;- fixef(model_lmm)[2]\n  # Draw a line for the group using the calculated intercept and slope\n  lines(y = intercept + slope * group_data$x, x = group_data$x, col = \"red\", lty = 1)\n\n}\n\n  # Draw a global effect line using the fixed effect intercept and slope from the model\n  abline(a = fixef(model_lmm)[1], b = fixef(model_lmm)[2], col = \"blue\")\n  \n# Adding a legend to the plot\nlegend_labels &lt;- c(\"Global Effect\", \"Random Intercept\")\nlegend(\"topright\", legend = legend_labels, col = c(\"blue\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\nFigure 2: Scatter plot of Age vs. Protein expression per group with random intercept\n\n\n\n\n\nHere we have plotted predicted best unbiased estimated trend per group and also the global trend. We can see that all the lines are parallel (as we still did not model slope per group) and there is a deviation of slope in each of the groups compared to the global trend.\nLooking at Figure 2"
  },
  {
    "objectID": "lmm.html#why-mixed",
    "href": "lmm.html#why-mixed",
    "title": "Mixed models",
    "section": "",
    "text": "Mixed models get their name because they combine two types of effects: fixed effects and random effects. This combination is what makes them so valuable for analyzing complex data.\n\n\nThe statistical models you might be familiar with, like linear regression or ANOVA, focus on “fixed effects.” These are the direct relationships between your chosen variables and the outcome you’re interested in. Fixed effects are the factors you control or are primarily interested in. If you’re studying the effect of age on protein expression, age is your fixed effect. A fixed effect will contain all possible levels of a factor in the experiment, measuring a few specific instances of interest. The key premise of the fixed-effect model is that there is one true effect size that is common to all the studies being analyzed. This model assumes that any observed variation among study results is solely due to sampling error within studies, and not due to actual differences in effect sizes across studies. Therefore, the goal of a fixed-effect analysis is to estimate this common true effect size.\nLet’s think about a research question asking if a specific vitamin supplement reduces the duration of common cold symptoms. Multiple clinical trials have been conducted to evaluate the effect of this vitamin supplement on the duration of common cold symptoms. Each trial uses the same dosage of the supplement and has similar participant demographics (e.g., age, health status). The researchers conducting the analysis believe that, given the standardized intervention and population, the supplement should have a consistent effect across all trials.\nStudies Reviewed: - Study 1: 100 participants, average reduction in symptom duration of 1.5 days. - Study 2: 150 participants, average reduction in symptom duration of 1.4 days. - Study 3: 120 participants, average reduction in symptom duration of 1.6 days. - Study 4: 130 participants, average reduction in symptom duration of 1.3 days.\nFixed-Effect Model Assumptions in this Example:\n\nSingle True Effect Size: The assumption here is that there is one true effect size reflecting the average reduction in the duration of common cold symptoms due to the vitamin supplement. This is based on the controlled administration of the supplement and the homogeneous nature of the participant groups across the trials.\nVariation Due to Sampling Error: The slight differences in observed effects among the studies (e.g., some showing a 1.3-day reduction while others show a 1.6-day reduction) are attributed to sampling error—random variation due to the different samples of participants in each study.\nGoal: To estimate the common true effect size of the vitamin supplement on reducing the duration of cold symptoms, a fixed-effect analysis is employed. This involves calculating a weighted average of the effect sizes from the individual trials, with greater weight given to larger trials since they are less prone to sampling error.\n\nThrough the fixed-effect model analysis, it might be concluded that the specific vitamin supplement leads to an average reduction of approximately 1.4 days in the duration of common cold symptoms. This conclusion is based on the premise that the variation in results across the trials is solely due to sampling error, without accounting for potential underlying differences in participant response to the supplement, as such variability is assumed to be negligible given the standardized conditions of the trials.\nThe primary limitation of using a fixed-effect model, lies in its underlying assumptions about the homogeneity of effect sizes across studies. This model assumes that there is one true effect size that applies to all studies, and any observed variability in outcomes is solely due to sampling error. However, this assumption can be overly simplistic and may not always hold true, especially in biological and medical research where variability is the norm rather than the exception.\n\n\n\nThe random-effects offer a more flexible and realistic approach, especially in contexts where between-study variability is expected or observed. The defining feature of the random-effects model is the assumption that there is a distribution of true effect sizes across studies, and the goal is to estimate the mean of this distribution. This approach accounts for variation not only within studies (due to sampling error) but also between studies, recognizing that different studies may inherently have different true effect sizes due to various factors (e.g., differences in populations, interventions, outcomes measured).\nReturning to our scenario of evaluating the impact of a vitamin supplement on the duration of common cold symptoms, let’s consider that the clinical trials are conducted across various geographical locations, each with a distinct demographic and environmental profile. These location-based groups could inherently influence how participants respond to the supplement, independent of the supplement’s effect itself. We would introduce random intercepts for each geographical location. This means that while we are still interested in estimating the overall effect of the vitamin supplement on symptom duration, we also acknowledge that each location starts from a different baseline in terms of average symptom duration without the supplement. However, we actually don’t directly estimate these baselines but rather assume that these baselines are randomly samples from a distribution of baselines in the population with certain statistical properties. Therefore, we are not interested in each single baseline but we want to know statistical properties of the distribution of baselines.\nIn summary, the fixed-effect model’s generalization is limited to the “levels” within the experiment, meaning it applies to populations or conditions that exactly match those of the included studies. It treats the effect as if it were a fixed property of the specific scenarios tested. In contrast, the random-effects model allows for generalization to “levels beyond those that took part in the study,” acknowledging that the effect sizes are part of a distribution across a wider population. This model treats the effect as a variable property reflecting a range of possible scenarios, including those not directly observed in the analysis.\nThis might sounds too complicated or abstract. We are going to clarify this with a few examples."
  },
  {
    "objectID": "lmm.html#ingredients",
    "href": "lmm.html#ingredients",
    "title": "Mixed models",
    "section": "",
    "text": "In this section, we will try to clarify what information do we need in order to fit a linear model.\n\n\nThe first and most important thing is to understand the data we are dealing with. Before we can make any attempt to use mixed models, we need to know what kind of grouping structure we want to capture. Mixed models are particularly useful when data is collected in groups or clusters, such as measurements from different subjects, schools, geographical locations, or time points within subjects. Identifying the hierarchy or nested structure where observations are grouped within higher-level units is crucial. This grouping structure can significantly influence the correlations among observations, as data points within the same group are likely to be more similar to each other than to data points in different groups. Understanding this structure allows us to correctly model the random effects, which account for the variability at different levels of the data hierarchy, thereby improving the accuracy and interpretability of our results. By incorporating these random effects, mixed models enable us to make inferences about both the fixed effects, which are consistent across groups, and the random effects, which vary across groups.\nWithout having this information, we cannot use standard mixed models. This grouping must be categorical, not continuous, as mixed models rely on categorical variables to define the levels of the hierarchy or nested structure within the data. Categorical grouping allows us to classify observations into distinct categories or groups that share common characteristics.\nIt is obvious but worth mentioning that these grouping should not be confused with random effects.Random effects are calculated based on the assumption that data points within the same group may share more similarities with each other than with data points from different categories, thus accounting for the within-group correlation. Therefore, the identification of categorical groupings is not just a step in the analysis but a prerequisite for accurately calculating and interpreting the random effects that are fundamental to the mixed model’s approach to data analysis.\n\n\n\n\n\n\nImportant\n\n\n\nYou cannot use continuous data as grouping if it does not have levels. Without these discrete levels, it becomes impossible to define the clusters or hierarchies necessary for calculating random effects. To effectively employ mixed models, one must either use inherently categorical variables or discretize continuous variables into meaningful categories.\n\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes it is not clear what grouping of data exist. Or even what should be used as grouping. In often (but not always), the grouping is something different than what we are interested in. For example, let’s say, we have gathered data from patients with a disease and attempted to find a matched control based on certain characteristics such as age, BMI etc. The group of interest here is disease vs. healthy and a possible grouping variable for random effect would be a match ID for the a each disease and its matched control.\n\n\nThe last thing i would like to mention is that, how many levels the random effect grouping should have in order to be included in the model. Although there is no golden consensus about it but most people tend to agree that the number levels of the group factor should be more than five to be accurately modeled by mixed models. Although some argue that with even two levels, mixed model will be like classical linear regression one should be extra cautious about it.\n\n\n\nNow that we have identified the grouping structure of the data, we need to decide what effects are fixed and what effects are random with respect to the grouping structure of the data. This is again might be confusing a bit but if we start asking ourselves a few questions, it might clarify the whole concept.\nIt is important to stress that the main reason we have chosen to go with mixed models is that we believe there is a grouping in our data that somehow affects some of the effects in our study. And by saying effect we mean the influence or impact that a particular variable has on the outcome of interest.\nSo here comes the first and a key question:\n\nDo we believe a particular effect varies across the grouping structure of the data? if we do not believe that a particular effect varies across the grouping structure of the data, then we may consider modeling this effect as a fixed effect without taking care of the grouping structure.\nIf the answer to this question is yes. Then we ask ourselves another key question.\nAre we interested in estimating the effect specific to the groups? Now that we have decided to take care of the grouping structure. We might be interested in estimating the effect of interest in each of groups or even compare them across the groups. You might want to even calculate p-values for these. If this is so then you might want to model these as fixed effect for example by including them as covariate (or blocking/control variable or even interaction) in the model. Doing so however, will force the model to estimate these effect thus spending degrees of freedom.\nIf however, we are still interested in taking care of the grouping but instead of estimating the effect in each group, we are happy just to know how much variability is in the effect of interest across the groups, then we are going to consider modeling that effect as random. This will instruct the modeling application to instead of estimating effects for each group (could be hundred of groups), just come up with estimates of few parameters showing the variability. Please note that these effects can be anything from baseline, slopes, interaction etc.\nDo we have a large number of groups, and are some of these groups potentially unobserved? Random effects models are particularly useful when dealing with a large number of groups, especially when some groups in the population might not be represented in the sample. Random effects assume that the observed groups are a random sample from a larger population of groups, allowing for generalization beyond the specific groups in the study.\nHowever, as said before, instead of estimating parameters for each group, then calculate variance across the groups. If the number of levels in each group is too few (like two or something like that), the estimated variance is not going to be accurate. So you might want to actually get back and consider using classical models.\n\nBy asking these questions, we can better clarify the roles of fixed and random effects. These things are going to be further clarified when we start working on a few examples."
  },
  {
    "objectID": "lmm.html#examples-mixed-models-in-r",
    "href": "lmm.html#examples-mixed-models-in-r",
    "title": "Mixed models",
    "section": "",
    "text": "We get back our simulated data in the previous chapter and try to model it using mixed models. Before proceeding make sure that you have installed lme4 package as this is what we are going to use throughout this section.\nJust to remind you, the idea was to examine the relationship between Age and Protein expression.\n\n\nCode\nsimulate_grouped_trend &lt;- function(group_count = 5, points_per_group = 10, global_slope = -10, global_intercept = 30, group_slope = 2, noise_sd = 50) {\n  set.seed(123) # Setting a seed for reproducibility\n  \n  # Initialize an empty data frame to store the simulated data\n  data &lt;- data.frame(x = numeric(), y = numeric())\n  \n  # Loop to create each group\n  for (i in 1:group_count) {\n    x_start &lt;- 12 + (i - 1) * (10 / group_count) # Stagger the start of x for each group\n    x &lt;- runif(points_per_group, min = x_start, max = x_start + (10 / group_count))\n    \n    # Apply a local positive trend within the group, but maintain the global negative trend\n    local_intercept &lt;- global_intercept + global_slope * (x_start + (10 / (2 * group_count))) + rnorm(1, mean = 0, sd = noise_sd)\n    y &lt;- local_intercept + group_slope * (x - x_start) + rnorm(points_per_group, mean = 0, sd = noise_sd)\n    \n    # Combine this group with the overall dataset\n    group_data &lt;- data.frame(x = x, y = y,group=i)\n    data &lt;- rbind(data, group_data)\n  }\n  \n  return(data)\n}\n\n# generate simulated data\ndata_int &lt;- simulate_grouped_trend(group_count = 4,points_per_group = 10,global_slope = -2,global_intercept = 100,group_slope = 4,noise_sd = 5)\n\n# set group to factor\n\ndata_int$group &lt;- factor(data_int$group)\n# plot the data\nplot(data_int$x,data_int$y,xlab=\"Age\",ylab=\"Protein expression\",col=data_int$group,pch=as.numeric(data_int$group))\n\n\n\n\n\n\n\n\nFigure 1: A scatter plot of Age vs. Protein expression\n\n\n\n\n\nBecause there are some grouping structure in data, we decide to take care of it. Let’s say that each group is a random clinic that we decided to get some sample from. So we now have identified the grouping structure in our data that might influence the effect of interest (Age).\nWhat we believe is that the effect of Age on the protein expression is constant in each of the groups. However, due to either technical issues with the instruments or even demographics, each group might have different baselines (starting point) for the protein expression. This gives us a hint that we might have to model intercept differently for each group. This decision gives us two choices, model this intercept as fixed or random effect.\nMe as a researcher tell you in addition to the effect of age, I would like to know how much differences exactly are between each of these clinics. Therefore i want to use fixed effect to exactly pinpoint the estimated baseline for each of the clinic and compare them against each other.\nYou however, argue that there is no reason to do that. We just picked a few random clinics, there is no point in knowing by how much clinic 1 is different to clinic 2 because we could have selected and other random clinics in the world. So let’s model this as random effect to get the variance of the intercepts across clinics. By modeling the intercept as a random effect, we acknowledge that each clinic has its own baseline level of protein expression due to various unmeasured factors such as technical differences in equipment or demographic variations. This approach allows us to account for the inherent variability between clinics without focusing on the specific differences between any two clinics. Instead, we aim to understand the general variability of baseline protein expression levels across clinics.\nWe start using the classical approach (as we saw in the previous chapter) using lm function with the following formula y ~ 1+x+group:\n\n\n\n\n\n\nNote\n\n\n\nWe have included 1 in the model just to stress that there is an intercept in the model. Obviously, y ~ 1+x+group is identical to y ~ x+group\n\n\n\n\nCode\n# Fit a linear regression model\nmodel_lm &lt;- lm(y ~ 1+x+group, data = data_int)\nsummary(model_lm)\n\n\n\nCall:\nlm(formula = y ~ 1 + x + group, data = data_int)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.7577 -3.2657 -0.1872  1.4925 10.7686 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   58.798     15.341   3.833 0.000505 ***\nx              2.213      1.136   1.949 0.059406 .  \ngroup2       -23.072      3.133  -7.364 1.30e-08 ***\ngroup3       -31.187      6.159  -5.064 1.32e-05 ***\ngroup4       -34.509      8.716  -3.959 0.000351 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.625 on 35 degrees of freedom\nMultiple R-squared:  0.786, Adjusted R-squared:  0.7615 \nF-statistic: 32.13 on 4 and 35 DF,  p-value: 2.832e-11\n\n\nWe saw this result before. This model has four different intercepts, one for each group and a single slope that is identical for each group.\nGiven this result, we have estimated the group coefficients and we can go ahead with comparing the groups or do whatever we wanted to do with these estimates. It is important that here we don’t have a global slope. We have four slopes, one for each group.\nNow it is time to do the same analysis using the mixed model approach. The model that we are going fit using lmer function from lme4 is of form y ~ 1+x+(1|group). This part of the model, y ~ 1+x, we already know what it is. However, what this part (1|group) is new. The right part of | shows what variable we want to use as grouping factor by which we are going to fit our random effect (That is the group in our case). The left part of | is the effect we want to consider random. The left part is like a formula, similar to classical regression. which in this case, we just used 1. This means for our main model, y ~ 1+x which has an intercept (1) and a slope x, we would like to consider the intercept as random effect so we use 1.\nNow that we know the structure of the model, let’s go ahead and do the modeling:\n\n\nCode\nlibrary(lme4)\n# Fit a mixed linear regression model\nmodel_lmm &lt;- lmer(y ~ 1+x+(1|group), data = data_int)\nsummary(model_lmm)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ 1 + x + (1 | group)\n   Data: data_int\n\nREML criterion at convergence: 244.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.73343 -0.61480 -0.00871  0.37734  2.22117 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n group    (Intercept) 184.10   13.568  \n Residual              21.57    4.645  \nNumber of obs: 40, groups:  group, 4\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   50.205     18.893   2.657\nx              1.418      1.030   1.377\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.932\n\n\nThis output has some key differences to the previous approach. The first thing is in the fixed effect part:\n\n\nCode\nas.list(summary(model_lmm))[['coefficients']]\n\n\n             Estimate Std. Error  t value\n(Intercept) 50.205206   18.89260 2.657401\nx            1.417913    1.02998 1.376642\n\n\nUnlike lm, here there is no estimate for each group. There is a single intercept and a single slope. These are global trends in the data with their corresponding Std. Error and t value. But what happen with random effect and grouping of the data. This is what we can see in the Random effects part of the output:\n\n\nCode\nas.list(summary(model_lmm))[['varcor']]\n\n\n Groups   Name        Std.Dev.\n group    (Intercept) 13.5684 \n Residual              4.6447 \n\n\nDepending of what type of random effect we have been fitting this table can have different headers. But generally, Groups, Name, Variance, Std.Dev. and Corr are part of it.\nEach row of this table correspond to a source of variation in our data that is random because of a certain reason. In our model, we decided to consider only Intercept as a random effect for group. Therefore, the first row of the table tells us that there is random variability in Intercept between different group. This variability is quantified by a variance of 184.101 and a standard deviation of 13.568. This means that the average effect (or intercept) across groups can deviate from the overall (global) intercept by about 13.568 units, indicating substantial between-group variability. This is important to note that, we have actually never estimated any intercept for each group directly. But we have estimated the variability of intercepts across different groups, which allows us to account for the fact that each group may have a different starting point or baseline level in the response variable, even though these specific baseline levels are not directly calculated. You can think about this as the variance of the distribution of intercepts across the groups. Essentially, we’re modeling the distribution from which each group’s intercept is drawn.\nThe second row, labeled Residual, refers to the variability within each group that is not explained by the model predictors (because of random sampling). The variance here is 21.574, with a standard deviation of 4.645. This residual variance represents the individual differences or noise within groups after accounting for the modeled effects, including the random intercepts.\nSo before moving on let’s summarize what get got so far.\nWe used mixed models with a random intercept to model the effect of age on protein expression. We got an estimated global intercept (baseline) and slope (main effect of interest). We took care of the grouping structure of our data without directly estimating any additional parameters. In fact, what we estimated was intercept, slope, variance of Intercept and variance of residuals. One of the coolest thing here is that we would have estimated the exact same number of parameters if we our grouping structure had a lot more levels. But if we want to use grouping as covariate (in a classic model), we should modeled all of these levels!\n\n\nDespite that we have not estimated the intercept for each of the group, we can still predict what would the intercept be for each them. We are going to see in the math section how that can be done, but for now, we leave the details and directly use the function ranef from lme4 package.\n\n\nCode\nprint(ranef(model_lmm))\n\n\n$group\n  (Intercept)\n1   19.059412\n2   -2.116552\n3   -7.752078\n4   -9.190783\n\nwith conditional variances for \"group\" \n\n\nIn this particular case, it gave us the differences from the overall intercept estimated in the mixed effects model. By extracting the random effects using the ranef function, we obtain insights into the specific adjustments needed for each group. We can now use the global intercept and these adjustments to visualize the predicted intercept.\n\n\nCode\n# Plotting the initial scatter plot with points colored by group and shape determined by group\nplot(data_int$x, data_int$y, xlab=\"Age\", ylab=\"Protein expression\",\n     col=data_int$group, pch=as.numeric(data_int$group))\n\n# Looping through each unique group in the dataset\nfor (group in unique(data_int$group)) {\n  # Subset data for the current group\n  group_data &lt;- data_int[data_int$group == group,]\n\n  # Calculate the intercept for the group by adding the fixed intercept to the group's random effect\n  intercept &lt;- fixef(model_lmm)[1] + ranef(model_lmm)[[1]][group, 1]\n\n  # Extract the fixed effect slope from the model\n  slope &lt;- fixef(model_lmm)[2]\n  # Draw a line for the group using the calculated intercept and slope\n  lines(y = intercept + slope * group_data$x, x = group_data$x, col = \"red\", lty = 1)\n\n}\n\n  # Draw a global effect line using the fixed effect intercept and slope from the model\n  abline(a = fixef(model_lmm)[1], b = fixef(model_lmm)[2], col = \"blue\")\n  \n# Adding a legend to the plot\nlegend_labels &lt;- c(\"Global Effect\", \"Random Intercept\")\nlegend(\"topright\", legend = legend_labels, col = c(\"blue\", \"red\"), lty = 1)\n\n\n\n\n\n\n\n\nFigure 2: Scatter plot of Age vs. Protein expression per group with random intercept\n\n\n\n\n\nHere we have plotted predicted best unbiased estimated trend per group and also the global trend. We can see that all the lines are parallel (as we still did not model slope per group) and there is a deviation of slope in each of the groups compared to the global trend.\nLooking at Figure 2"
  },
  {
    "objectID": "lm.html",
    "href": "lm.html",
    "title": "Simple linear regression",
    "section": "",
    "text": "Simple linear regression\nImagine we are given a dataset of only two variables (measurements) for a number of people. This could be the expression of a single gene or a protein, body mass index (BMI), blood pressure, age, or any other quantifiable trait.\n\n\nCode\nsimulate_grouped_trend &lt;- function(group_count = 5, points_per_group = 10, global_slope = -10, global_intercept = 30, group_slope = 2, noise_sd = 50) {\n  set.seed(123) # Setting a seed for reproducibility\n  \n  # Initialize an empty data frame to store the simulated data\n  data &lt;- data.frame(x = numeric(), y = numeric())\n  \n  # Loop to create each group\n  for (i in 1:group_count) {\n    x_start &lt;- 12 + (i - 1) * (10 / group_count) # Stagger the start of x for each group\n    x &lt;- runif(points_per_group, min = x_start, max = x_start + (10 / group_count))\n    \n    # Apply a local positive trend within the group, but maintain the global negative trend\n    local_intercept &lt;- global_intercept + global_slope * (x_start + (10 / (2 * group_count))) + rnorm(1, mean = 0, sd = noise_sd)\n    y &lt;- local_intercept + group_slope * (x - x_start) + rnorm(points_per_group, mean = 0, sd = noise_sd)\n    \n    # Combine this group with the overall dataset\n    group_data &lt;- data.frame(x = x, y = y,group=i)\n    data &lt;- rbind(data, group_data)\n  }\n  \n  return(data)\n}\n\n# generate simulated data\ndata_int &lt;- simulate_grouped_trend(group_count = 4,points_per_group = 10,global_slope = -2,global_intercept = 100,group_slope = 4,noise_sd = 5)\n\n# plot the data\nplot(data_int$x,data_int$y,xlab=\"Age\",ylab=\"Protein expression\")\n\n\n\n\n\n\n\n\nFigure 1: A scatter plot of Age vs. Protein expression\n\n\n\n\n\nOur aim here is to examine the relationship between Age and Protein expression, we would apply a simple linear regression model. This statistical method allows us to quantify the relationship between an independent variable (Age) and a dependent variable (Protein expression), assuming a linear relationship between them. The goal here is to determine how Protein expression changes with Age.\n\n\nCode\n# Fit a linear regression model\nmodel &lt;- lm(y ~ x, data = data_int)\n\n# Plot the data with the regression line\nplot(data_int$x, data_int$y, xlab=\"Age\", ylab=\"Protein expression\", main=\"Linear Regression: Age vs. Protein Expression\")\nabline(model, col=\"red\") # Add the regression line in red\n\n\n\n\n\n\n\n\nFigure 2: Scatter plot of Age vs. Protein expression including the regression line\n\n\n\n\n\nThe lm() function fits a linear model to the data, allowing us to see the relationship between Age and Protein expression. What this plot suggests is that there is a global decrease in Protein expression as Age increases, which is depicted by the negative slope of the red regression line. The scatter of points around the line indicates variability in Protein expression that is not explained by Age alone.\nBased on the output from the lm() function, we can extract the coefficients and examine the fit of our linear regression model.\n\n\nCode\n# Summary of the model to see the coefficients\nsummary(model)\n\n\n\nCall:\nlm(formula = y ~ x, data = data_int)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.7166  -6.2725   0.7094   5.0500  17.9565 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 106.6762     7.2948   14.62  &lt; 2e-16 ***\nx            -1.8836     0.4204   -4.48 6.63e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.761 on 38 degrees of freedom\nMultiple R-squared:  0.3457,    Adjusted R-squared:  0.3285 \nF-statistic: 20.07 on 1 and 38 DF,  p-value: 6.627e-05\n\n\nThe intercept, estimated at 106.6762, represents the expected Protein expression when Age is zero. Although having an age of zero is not meaningful in this context, the intercept gives us a starting reference point for the model.\nThe slope coefficient for Age (x) is -1.8836, indicating that for each one-unit increase in Age, Protein expression is expected to decrease by approximately 1.88 units. This negative association is significant, as indicated by the p-value of 6.63e-05, which is much lower than the conventional threshold of 0.05 for statistical significance.\nThe standard error of the estimate for the slope tells us the average amount that the coefficient estimates vary from the actual average value of our response variable. Here, the standard error for the slope is 0.4204, which is relatively small compared to the estimate itself, suggesting a precise estimate of the slope. So we are pretty confident in what we have estimated. However, while the regression line provides a clear visual indication of the overall trend, it does not account for the potential clustering of data points.\nIf you look more closely, you can see four different clusters of the data.\n\n\nCode\n# Plot the data with different colors for each group\nplot(data_int$x, data_int$y, xlab=\"Age\", ylab=\"Protein expression\", main=\"Linear Regression: Age vs. Protein Expression\", col=data_int$group)\n\n# Add a linear regression line\nabline(lm(y ~ x, data = data_int), col=\"red\")\n\n# Function to draw ellipse\ndraw_ellipse &lt;- function(x, y, level = 0.95, col){\n  # Calculate the means of x and y\n  xbar &lt;- mean(x)\n  ybar &lt;- mean(y)\n  \n  # Calculate the standard deviations\n  std_x &lt;- sd(x)*2\n  std_y &lt;- sd(y)*2\n  \n  # Calculate the correlation\n  correlation &lt;- cor(x, y)\n  \n  # Create a sequence of angles\n  t &lt;- seq(0, 2*pi, length.out = 100)\n  \n  # Calculate the ellipse points\n  a &lt;- std_x * sqrt(1 + correlation)\n  b &lt;- std_y * sqrt(1 - correlation)\n  ellipse_x &lt;- xbar + a * cos(t)\n  ellipse_y &lt;- ybar + b * sin(t)\n  \n  # Draw the ellipse\n  lines(ellipse_x, ellipse_y, col=col, lwd=2)\n}\n\n# Draw ellipses for each group\nunique_groups &lt;- unique(data_int$group)\ncolors &lt;- rainbow(length(unique_groups))\nfor (group in unique_groups) {\n  group_data &lt;- data_int[data_int$group == group, ]\n  draw_ellipse(group_data$x, group_data$y, col=group)\n}\n\n\n\n\n\n\n\n\nFigure 3: Scatter plot of Age vs. Protein expression including the regression line. The clusters are shown by colors\n\n\n\n\n\nWhile the global pattern seems decreasing, the local pattern per cluster of data increases with Age.\n\n\nCode\n# Plot the data with different colors for each group\nplot(data_int$x, data_int$y, xlab=\"Age\", ylab=\"Protein expression\", main=\"Linear Regression: Age vs. Protein Expression\", pch=19, col=data_int$group)\n\n# Draw ellipses and regression lines for each group\nunique_groups &lt;- unique(data_int$group)\ncolors &lt;- rainbow(length(unique_groups))\n\nfor (group in unique_groups) {\n  group_data &lt;- data_int[data_int$group == group, ]\n  # Draw ellipse\n  draw_ellipse(group_data$x, group_data$y, col=group)\n  \n  # Fit linear model for the group\n  group_model &lt;- lm(y ~ x, data=group_data)\n  \n  # Get regression line points\n  reg_line_x &lt;- range(group_data$x)\n  reg_line_y &lt;- predict(group_model, newdata=data.frame(x=reg_line_x))\n  \n  # Draw the regression line for the group\n  lines(reg_line_x, reg_line_y, col=group, lwd=2)\n}\n\n\n\n\n\n\n\n\nFigure 4: Scatter plot of Age vs. Protein expression including the regression line. The clusters are shown by colors\n\n\n\n\n\nMore precisely, there is a positive trend in each of the clusters that has not been captured by the model. Instead the model focused on the global pattern. The example that we just saw was a case when both Protein expression and Age are correlated within each cluster.\nYou could of course fit single model for each of the group or even using the groups as covariate in the model. Both of these options can be OK in circumstances that we don’t have too many groups and we have enough data in each of groups. Below we try to use the cluster as covariate to see what would happen to the coeffcients.\n\n\nCode\n# Fit a linear regression model\ndata_int$group &lt;- as.factor(data_int$group)\nmodel_cov &lt;- lm(y ~ x+group, data = data_int)\n\n\n# Get the predictions\ndata_int$predicted &lt;- predict(model_cov)\n\n# Plotting\nplot(data_int$x, data_int$y, xlab=\"Age\", ylab=\"Protein expression\", main=\"Linear Regression: Age vs. Protein Expression\", pch=19, col=data_int$group)\n\n\n# Add predicted lines\nfor(i in seq_along(unique(data_int$group))){\n  group_subset &lt;- data_int[data_int$group == unique_groups[i], ]\n  lines(group_subset$x, group_subset$predicted, col = i, lwd = 2)\n}\n\n\n\n\n\n\n\n\nFigure 5: Scatter plot of Age vs. Protein expression including the regression line including group covariate\n\n\n\n\n\nAs it is clear in the figure, the function lm(y ~ x+group, data = data_int) has fitted four different lines (one for each group). These lines are parallel to each other while each group has a different intercept.\nTo make this even more clear we can have a look at the coeffcients of the the model:\n\n\nCode\nprint(model_cov)\n\n\n\nCall:\nlm(formula = y ~ x + group, data = data_int)\n\nCoefficients:\n(Intercept)            x       group2       group3       group4  \n     58.798        2.213      -23.072      -31.187      -34.509  \n\n\nStarting with the intercept, which is 58.798, this tells us the expected value of y when x is at 0 and when we are considering the baseline group (group1). It’s like saying, “If we don’t move x and if we’re looking at group1, y would be around 58.798.”\nNext, we have the coefficient for x, which is 2.213. This number shows how y changes with a one-unit increase in x, across all groups. It’s a universal slope, implying that regardless of which group we’re looking at, if x goes up by one, y tends to increase by about 2.213 units, holding everything else constant.\nNow, let’s talk about the interesting part - the different intercepts for each group beyond the first. The coefficients for group2 (-23.072), group3 (-31.187), and group4 (-34.509) adjust the baseline level of y for each group compared to group1. What this means is that if we’re in group2 and x is 0, y is expected to be 23.072 units lower than the baseline of group1. Similarly, for group3 and group4, y would be 31.187 and 34.509 units lower than group1’s baseline, respectively, when x is 0.\nThese differences highlight that our groups have unique starting points for y, which aren’t explained by x alone. There are underlying characteristics or conditions specific to each group that affect y, independent of x. By including these group differences in our model, we can accurately capture the unique relationship between x and y for each group, showing that not every group begins from the same place.\nHaving explored how different groups can have unique starting points or intercepts in our model, it’s natural to wonder if the relationship between x and y—our slope—might also vary across these groups. Until now, we’ve assumed that a one-unit increase in x has the same impact on y for all groups. But what if this isn’t the case? What if the effect of x on y is stronger for some groups than for others?\nTo explore this possibility, we introduce the concept of interaction terms in our model. By fitting a new model, model_cov &lt;- lm(y ~ x*group, data = data_int), we’re not just looking at how x and group independently predict y, but also how the effect of x on y might change depending on the group. This model allows x to have a different slope for each group, providing a richer understanding of our data. It’s a step towards uncovering more complex patterns and interactions that might exist, shedding light on the nuanced ways in which our variables interplay.\n\n\nCode\n# Fit a linear regression model\ndata_int$group &lt;- as.factor(data_int$group)\nmodel_cov &lt;- lm(y ~ x*group, data = data_int)\n\n\n# Get the predictions\ndata_int$predicted &lt;- predict(model_cov)\n\n# Plotting\nplot(data_int$x, data_int$y, xlab=\"Age\", ylab=\"Protein expression\", main=\"Linear Regression: Age vs. Protein Expression\", pch=19, col=data_int$group)\n\n\n# Add predicted lines\nfor(i in seq_along(unique(data_int$group))){\n  group_subset &lt;- data_int[data_int$group == unique_groups[i], ]\n  lines(group_subset$x, group_subset$predicted, col = i, lwd = 2)\n}\n\nprint(model_cov)\n\n\n\nCall:\nlm(formula = y ~ x * group, data = data_int)\n\nCoefficients:\n(Intercept)            x       group2       group3       group4     x:group2  \n    46.9913       3.0911     -18.8314     -30.0940      24.0594      -0.3905  \n   x:group3     x:group4  \n    -0.3006      -3.1154  \n\n\n\n\n\n\n\n\nFigure 6: Scatter plot of Age vs. Protein expression including the regression line with interaction\n\n\n\n\n\nThe interaction terms in our new model model_cov &lt;- lm(y ~ x*group, data = data_int) reveal yet another layer of complexity. Now, not only do we recognize that each group starts at a different point, but we also see that the relationship between x and y—how y changes as x increases—is not a one-size-fits-all across the groups.\nNow, let’s interpret the coefficients:\n\n(Intercept) 46.9913: This is the expected protein expression when age is zero for the baseline group (group1).\nx 3.0911: This coefficient represents the slope of the line for the baseline group. For each additional year of age, the protein expression is expected to increase by 3.0911 units for group1.\ngroup2 -18.8314: This coefficient adjusts the intercept for group2. So, the expected starting point for protein expression in group2, when age is zero, is 46.9913 (the baseline intercept) minus 18.8314, giving us a starting point of 28.1599.\ngroup3 -30.0940: Similarly, for group3, the expected starting point for protein expression when age is zero is 46.9913 minus 30.0940, resulting in 16.8973.\ngroup4 24.0594: For group4, the expected starting point is 46.9913 plus 24.0594, which is 71.0507, indicating that group4 has a higher starting point than group1.\nx:group2 -0.3905: This is the interaction term for age and group2, indicating how the slope for group2 differs from the baseline group’s slope. The slope for group2 is 3.0911 (baseline slope) minus 0.3905, which is approximately 2.7006. This suggests that for group2, for each additional year of age, the protein expression increases by about 2.7006 units, which is less than the increase for group1.\nx:group3 -0.3006: For group3, the slope is 3.0911 minus 0.3006, approximately 2.7905. So the rate of increase in protein expression per year of age for group3 is slightly less than for group1 but not as much less as for group2.\nx:group4 -3.1154: This interaction term indicates a significant change in the slope for group4 compared to group1. The slope for group4 is 3.0911 minus 3.1154, which is approximately -0.0243. This would suggest a slight decrease in protein expression with age for group4, which is contrary to the positive association seen in the other groups.\n\nThe interaction terms, specifically the coefficients for x:group2, x:group3, and x:group4, tell us just how much the slope of the relationship between x and y differs for each group compared to our baseline group1. For example, group2 has a slightly lesser slope than group1, indicating that as x increases, y increases but not as rapidly as in group1. Group3 follows a similar pattern, albeit the difference is more modest.\nMost intriguing is group4, where the interaction term is quite pronounced, suggesting that the increase in y with x is significantly less for group4 compared to group1 in fact, it’s almost negligible, indicating a nearly flat line. This could mean that for individuals in group4, changes in x do not translate into changes in y as they do for the other groups.\nWhat we just saw was a simple example of how to handle clustered data in a simple linear regression. Without accounting for these cluster-related differences, the assumption of independence among observations—a cornerstone of linear regression—is violated, as the outcome within a cluster are inherently correlated. This correlation persists even when attempting to control for the cluster’s effect, due to other unobservable factors.\nThe consequence of ignoring cluster-level effects might lead to incorrect inferences, as standard errors are usually underestimated, making them appear more precise than they actually are. While the regression coefficients themselves might remain unbiased, the underestimation of standard errors can falsely enhance the significance of findings. Furthermore, we often ten to overestimate the degrees of freedom leading to inflated p-values.\nAddressing this issue often involves introducing indicator variables (similar to what we have done above) to account for cluster effects, allowing for different intercepts or even slope for each cluster. However, this approach becomes impractical with a large number of clusters or when dealing with longitudinal data. Furthermore, we might not even be interested in modeling clusters parameters, we are just interested in some specific questions. Think about the simple dataset we just analyzed, the main effect of interest was Age, however, we had to estimate a lot more parameters in the model to account for the cluster effect. More specifically, in lm(y~x+group) we estimate five parameters and in lm(y~x*group,data_int) we had eight parameters while we basically needed two. These parameters are all degrees of freedom that we spend while we might not even be interested in the cluster level parameters. Now imagine we might have hundreds of nested clustered with limited data, such modeling attemp will quickly become .infeasible and may lead to overfitting. In such cases, the model would consume degrees of freedom excessively, reducing the power of statistical tests and potentially leading to unreliable estimates due to the curse of dimensionality. Moreover, with a high number of parameters relative to the number of observations, standard errors of the estimates increase, which can make the model’s predictions less precise.\nTo address these issues, we often resort to mixed-effects models which is the topic of the next chapter."
  }
]