---
title: "Linear mixed models"
# author: Olga Dethlefsen
format: 
  revealjs:
    slide-number: true
    theme: [default, custom.scss]
    chalkboard: 
      buttons: true
  html:
    code-fold: false
editor_options: 
  chunk_output_type: console
bibliography: references.bib
---

```{r}
#| message: false
#| warning: false
#| include: false

# load libraries
library(tidyverse)
library(magrittr)
library(faraway)
library(kableExtra)
library(ggplot2)
library(rmarkdown)
library(gridExtra)
library(ggiraphExtra)
library(latex2exp)

font.size <- 16
col.blue.light <- "#a6cee3"
col.blue.dark <- "#1f78b4"
my.ggtheme <- 
  theme_bw() + 
  theme(axis.title = element_text(size = font.size), 
        axis.text = element_text(size = font.size), 
        legend.text = element_text(size = font.size), 
        legend.title = element_blank(), 
        legend.position = "top") 
        

# add obesity and diabetes status to diabetes faraway data
inch2m <- 2.54/100
pound2kg <- 0.45
data_diabetes <- diabetes %>%
  mutate(height  = height * inch2m, height = round(height, 2)) %>% 
  mutate(waist = waist * inch2m) %>%  
  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%
  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% 
  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c("No", "Yes"))) %>% 
  mutate(diabetic = ifelse(glyhb > 7, "Yes", "No"), diabetic = factor(diabetic, levels = c("No", "Yes"))) %>%
  na.omit()

```

# Outline

- Introduction (ca. 15 min)
- Mathematical details (15 - 30 min)
- Examples in R (1h)
- Q&A session (15 min)

## Introduction
*Why linear models?*

. . .

::: columns
::: {.column width="50%"}
With linear models we can answer questions:

::: incremental
-   is there a relationship between exposure and outcome, e.g. height and weight?
-   how strong is the relationship between the two variables?
-   what will be a predicted value of the outcome given a new set of exposure values?
-   which variables are associated with the response, e.g. is it height that dictates the weight or height and age?
:::
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
```{r}
#| fig-width: 6
#| fig-height: 6
data_diabetes %>%
  ggplot(aes(x = height, y = weight)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) + 
  my.ggtheme + 
  xlab("height [m]") + 
  ylab("weight [kg]")
```
:::
:::




## What linear models are

*definition*

<br>

::: columns
::: {.column width="60%"}
::: incremental
- In an linear model we model the relationship between a single continuous variable $Y$ and one or more variables $X$.
- One very general form for the model would be: $$Y = f(X_1, X_2, \dots X_p) + \epsilon$$ where $f$ is some unknown function and $\epsilon$ is the error in this representation.
- The $X$ variables can be numerical, categorical or a mixture of both.
- Formally, linear models are a way of describing a response variable in terms of **linear combination** of predictor variables
:::
:::

::: {.column width="5%"}
:::

::: {.column width="35%"}
```{r}
#| fig-width: 6
#| fig-height: 6
data_diabetes %>%
  ggplot(aes(x = height, y = weight)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) + 
  my.ggtheme + 
  xlab("height [m]") + 
  ylab("weight [kg]")
```
:::
:::


<!-- ## Simple linear regression -->

<!-- ```{r} -->
<!-- #| fig-width: 10 -->
<!-- #| fig-height: 4 -->
<!-- #| fig-align: center -->

<!-- par(mfrow=c(1,2)) -->
<!-- x <- 1:100 -->
<!-- y <- x + rnorm(length(x), 0, 10) -->
<!-- plot(x,y, pch = 19, las = 1) -->

<!-- x <- 1:100 -->
<!-- y <- (x - rnorm(length(x), 0, 10))*(-1) -->
<!-- plot(x,y, pch = 19, las = 1) -->

<!-- ``` -->

<!-- ::: incremental -->
<!-- -   It is used to check the association between **the numerical outcome and one numerical explanatory variable** -->
<!-- -   In practice, we are finding the best-fitting straight line to describe the relationship between the outcome and exposure -->
<!-- ::: -->

<!-- ## Simple linear regression -->

<!-- ::: {#exm-simple-lm} -->
<!-- ## Weight and plasma volume -->

<!-- Let's look at the example data containing body weight (kg) and plasma volume (liters) for eight healthy men to see what the best-fitting straight line is. -->

<!-- ```{r} -->
<!-- #| code-fold: false -->
<!-- weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) -->
<!-- plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) -->

<!-- ``` -->

<!-- ``` r -->
<!-- weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) -->
<!-- plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) -->
<!-- ``` -->
<!-- ::: -->

<!-- ```{r} -->
<!-- #| label: fig-lm-intro-example -->
<!-- #| fig-cap: "Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*." -->
<!-- #| fig-cap-location: margin -->
<!-- #| echo: false -->
<!-- #| fig-width: 4 -->
<!-- #| fig-heigth: 4 -->
<!-- #| include: true -->

<!-- plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]",  panel.first = grid()) -->

<!-- ``` -->

## Simple linear regression

```{r}
#| label: fig-lm-01
#| include: false
#| fig-width: 6
#| fig-height: 6

weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)
plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)
plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]",  panel.first = grid())

```

```{r}
#| label: fig-lm-02
#| include: false
#| fig-width: 6
#| fig-height: 6

plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]", panel.first = grid())

reg1 <- lm(plasma ~ weight)
a <- reg1$coefficients[1]
b <- reg1$coefficients[2]

#abline(a=a+0.1 , b + 0.001, col="gray")
#abline(a=a+0.1 , b + 0.0001, col="gray")
#abline(a=a , b + 0.00015, col="gray")
#abline(a=a+0.1 , b + 0.002, col="gray")
# abline(a=a+0.1 , b - 0.002, col="gray")
# abline(a=a+0.1 , b - 0.002, col="gray")
# abline(a=a+0.1 , b - 0.001, col="gray")
# abline(a=a, b - 0.001, col="gray")
 abline(a=a+0.5 , b , col="gray")
# abline(a=a-0.5 , b , col="gray")

#abline(lm(plasma~weight), col="red") # regression line
points(weight, plasma, pch=19)

```

```{r}
#| label: fig-lm-03
#| include: false
#| fig-width: 6
#| fig-height: 6


plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]", panel.first = grid())

reg1 <- lm(plasma ~ weight)
a <- reg1$coefficients[1]
b <- reg1$coefficients[2]

abline(a=a+0.1 , b + 0.001, col="gray")
#abline(a=a+0.1 , b + 0.0001, col="gray")
# abline(a=a , b + 0.00015, col="gray")
# abline(a=a+0.1 , b + 0.002, col="gray")
# abline(a=a+0.1 , b - 0.002, col="gray")
# abline(a=a+0.1 , b - 0.002, col="gray")
# abline(a=a+0.1 , b - 0.001, col="gray")
# abline(a=a, b - 0.001, col="gray")
abline(a=a+0.5 , b , col="gray")
# abline(a=a-0.5 , b , col="gray")

#abline(lm(plasma~weight), col="red") # regression line
points(weight, plasma, pch=19)

```

```{r}
#| label: fig-lm-03b
#| include: false
#| fig-width: 6
#| fig-height: 6


plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]", panel.first = grid())

reg1 <- lm(plasma ~ weight)
a <- reg1$coefficients[1]
b <- reg1$coefficients[2]

abline(a=a+0.1 , b + 0.001, col="gray")
#abline(a=a+0.1 , b + 0.0001, col="gray")
# abline(a=a , b + 0.00015, col="gray")
#abline(a=a+0.1 , b + 0.002, col="gray")
# abline(a=a+0.1 , b - 0.002, col="gray")
# abline(a=a+0.1 , b - 0.002, col="gray")
 abline(a=a+0.1 , b - 0.001, col="gray")
#abline(a=a, b - 0.001, col="gray")
abline(a=a+0.5 , b , col="gray")
#abline(a=a-0.5 , b , col="gray")

#abline(lm(plasma~weight), col="red") # regression line
points(weight, plasma, pch=19)

```

```{r}
#| label: fig-lm-04
#| include: false
#| fig-width: 6
#| fig-height: 6


plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]", panel.first = grid())

reg1 <- lm(plasma ~ weight)
a <- reg1$coefficients[1]
b <- reg1$coefficients[2]

abline(a=a+0.1 , b + 0.001, col="gray")
abline(a=a+0.1 , b + 0.0001, col="gray")
#abline(a=a , b + 0.00015, col="gray")
abline(a=a+0.1 , b + 0.002, col="gray")
abline(a=a+0.1 , b - 0.002, col="gray")
abline(a=a+0.1 , b - 0.002, col="gray")
abline(a=a+0.1 , b - 0.001, col="gray")
abline(a=a, b - 0.001, col="gray")
abline(a=a+0.5 , b , col="gray")
abline(a=a-0.5 , b , col="gray")

#abline(lm(plasma~weight), col="red") # regression line
points(weight, plasma, pch=19)

```

```{r}
#| label: fig-lm-05
#| include: false
#| fig-width: 6
#| fig-height: 6

plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]", panel.first = grid())

reg1 <- lm(plasma ~ weight)
a <- reg1$coefficients[1]
b <- reg1$coefficients[2]

abline(a=a+0.1 , b + 0.001, col="gray")
abline(a=a+0.1 , b + 0.0001, col="gray")
#abline(a=a , b + 0.00015, col="gray")
abline(a=a+0.1 , b + 0.002, col="gray")
abline(a=a+0.1 , b - 0.002, col="gray")
abline(a=a+0.1 , b - 0.002, col="gray")
abline(a=a+0.1 , b - 0.001, col="gray")
abline(a=a, b - 0.001, col="gray")
abline(a=a+0.5 , b , col="gray")
abline(a=a-0.5 , b , col="gray")

abline(lm(plasma~weight), col="red") # regression line
points(weight, plasma, pch=19)
```

::: r-stack
![](session-lm-presentation_files/figure-revealjs/fig-lm-01-1.png){.fragment width="600" height="600"}

![](session-lm-presentation_files/figure-revealjs/fig-lm-02-1.png){.fragment width="600" height="600"}

![](session-lm-presentation_files/figure-revealjs/fig-lm-03-1.png){.fragment width="600" height="600"}

![](session-lm-presentation_files/figure-revealjs/fig-lm-03b-1.png){.fragment width="600" height="600"}

![](session-lm-presentation_files/figure-revealjs/fig-lm-04-1.png){.fragment width="600" height="600"}

![](session-lm-presentation_files/figure-revealjs/fig-lm-05-1.png){.fragment width="600" height="600"}
:::

## Simple linear regression 

:::: {.columns}

::: {.column width="50%"}

The equation for the red line is: $$Y_i=0.086 +  0.044 \cdot x_i \quad for \;i = 1 \dots 8$$

and in general:  $$Y_i = \alpha + \beta \cdot x_i + \epsilon_i$$ {#eq-lm} where:

-   $x$: is called: exposure variable, explanatory variable, independent variable, predictor, covariate
-   $y$: is called: response, outcome, dependent variable
-   $\alpha$ and $\beta$ are **model coefficients**
-   and $\epsilon_i$ is an **error terms**
:::

::: {.column width="5%"}

:::

::: {.column width="45%"}
```{r}
#| label: fig-lm-example-reg
#| fig-cap: "Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable"
#| fig-cap-location: margin
#| echo: false
#| fig-width: 5
#| fig-heigth: 5

#par(mfrow=c(1,2))
#plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]",  panel.first = grid())

plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]", panel.first = grid())

reg1 <- lm(plasma ~ weight)
a <- reg1$coefficients[1]
b <- reg1$coefficients[2]

abline(a=a+0.1 , b + 0.001, col="gray")
abline(a=a+0.1 , b + 0.0001, col="gray")
#abline(a=a , b + 0.00015, col="gray")
abline(a=a+0.1 , b + 0.002, col="gray")
abline(a=a+0.1 , b - 0.002, col="gray")
abline(a=a+0.1 , b - 0.002, col="gray")
abline(a=a+0.1 , b - 0.001, col="gray")
abline(a=a, b - 0.001, col="gray")
abline(a=a+0.5 , b , col="gray")
abline(a=a-0.5 , b , col="gray")

abline(lm(plasma~weight), col="red") # regression line
points(weight, plasma, pch=19)


```
:::

::::



## Least squares {.smaller}

*estimating model coefficients*

::: columns
::: {.column width="50%"}
Let $\hat{y_i}=\hat{\alpha} + \hat{\beta}x_i$ be the prediction $y_i$ based on the $i$-th value of $x$:

-   Then $\epsilon_i = y_i - \hat{y_i}$ represents the $i$-th **residual**, i.e. the difference between the $i$-th observed response value and the $i$-th response value that is predicted by the linear model
-   RSS, the **residual sum of squares** is defined as: $$RSS = \epsilon_1^2 + \epsilon_2^2 + \dots + \epsilon_n^2$$ or equivalently as: $$RSS=(y_1-\hat{\alpha}-\hat{\beta}x_1)^2+(y_2-\hat{\alpha}-\hat{\beta}x_2)^2+...+(y_n-\hat{\alpha}-\hat{\beta}x_n)^2$$
-   the least squares approach chooses $\hat{\alpha}$ and $\hat{\beta}$ **to minimize the RSS**.
:::

::: {.column width="10%"}
:::

::: {.column width="40%"}
```{r}
#| label: fig-reg-errors
#| fig-cap: "Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice versa*. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line"
#| fig-cap-location: margin
#| echo: false
#| warning: false
#| message: false
#| fig-width: 8
#| fig-height: 8

data.reg <- data.frame(plasma=plasma, weight=weight)
fit.reg <- lm(plasma~weight, data=data.reg)
data.reg$predicted <- predict(fit.reg)
data.reg$residuals <- residuals((fit.reg))

ggplot(data.reg, aes(x=weight, plasma)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "firebrick") +
  geom_segment(aes(xend = weight, yend = predicted), color="blue") +
  geom_point(aes(y = predicted), shape = 1) +
  geom_point(aes(y = predicted), shape = 1) +
  xlab("body weight [kg]") + ylab("plasma volume [liters]") + 
  my.ggtheme

```
:::
:::

<!-- ## Least squares -->

<!-- ::: {#thm-lss} -->

<!-- ## Least squares estimates for a simple linear regression -->

<!-- $$\hat{\beta} = \frac{S_{xy}}{S_{xx}}$$ $$\hat{\alpha} = \bar{y}-\frac{S_{xy}}{S_{xx}}\cdot \bar{x}$$ -->

<!-- where: -->

<!-- -   $\bar{x}$: mean value of $x$ -->

<!-- -   $\bar{y}$: mean value of $y$ -->

<!-- -   $S_{xx}$: sum of squares of $X$ defined as $S_{xx} = \displaystyle \sum_{i=1}^{n}(x_i-\bar{x})^2$ -->

<!-- -   $S_{yy}$: sum of squares of $Y$ defined as $S_{yy} = \displaystyle \sum_{i=1}^{n}(y_i-\bar{y})^2$ -->

<!-- -   $S_{xy}$: sum of products of $X$ and $Y$ defined as $S_{xy} = \displaystyle \sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})$ -->

<!-- ::: -->

<!-- ## Least squares -->

<!-- *Live demo* -->

<!-- ::: {#exm-lss} -->

<!-- ## Least squares -->

<!-- Let's try least squares method to find coefficient estimates in the **"body weight and plasma volume example"** -->

<!-- ``` r -->

<!-- # initial data -->

<!-- weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) -->

<!-- plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) -->

<!-- # rename variables for convenience -->

<!-- x <- weight -->

<!-- y <- plasma -->

<!-- # mean values of x and y -->

<!-- x.bar <- mean(x) -->

<!-- y.bar <- mean(y) -->

<!-- # Sum of squares -->

<!-- Sxx <-  sum((x - x.bar)^2) -->

<!-- Sxy <- sum((x-x.bar)*(y-y.bar)) -->

<!-- # Coefficient estimates -->

<!-- beta.hat <- Sxy / Sxx -->

<!-- alpha.hat <- y.bar - Sxy/Sxx*x.bar -->

<!-- # Print estimated coefficients alpha and beta -->

<!-- print(alpha.hat) -->

<!-- print(beta.hat) -->

<!-- ``` -->

<!-- ```{r} -->

<!-- #| code-fold: false -->

<!-- # initial data -->

<!-- weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg) -->

<!-- plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters) -->

<!-- # rename variables for convenience -->

<!-- x <- weight -->

<!-- y <- plasma -->

<!-- # mean values of x and y -->

<!-- x.bar <- mean(x) -->

<!-- y.bar <- mean(y) -->

<!-- # Sum of squares -->

<!-- Sxx <-  sum((x - x.bar)^2) -->

<!-- Sxy <- sum((x-x.bar)*(y-y.bar)) -->

<!-- # Coefficient estimates -->

<!-- beta.hat <- Sxy / Sxx -->

<!-- alpha.hat <- y.bar - Sxy/Sxx*x.bar -->

<!-- # Print estimated coefficients alpha and beta -->

<!-- print(alpha.hat) -->

<!-- print(beta.hat) -->

<!-- ``` -->

<!-- ::: -->

## Slope

$plasma = 0.0857 + 0.0436 * weight$

Linear regression gives us estimates of model coefficient $Y_i = \alpha + \beta x_i + \epsilon_i$

```{r}
#| fig-align: center

model <- lm(plasma ~ weight)
alpha <- model$coefficients[1]
beta <- model$coefficients[2]

my.s <- 2
par(mfcol=c(1,2))

# Beta 1 example a
plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]",  panel.first = grid())
abline(lm(plasma~weight), col="red") # regression line
abline(h = alpha + beta * 65, col = "blue",  lty = 3)
abline(h = alpha + beta * 70, col = "blue",  lty = 3)
segments(x0=65, y0=alpha + beta * 65, x1=70, y1=alpha + beta * 65, col="blue")
segments(x0=70, y0=alpha + beta * 65, x1=70, y1=alpha + beta * 70, col="blue")
text(72, 2.95, expression(beta), cex=1.2, col="blue")
text(60, alpha + beta * 65 + 0.05, round(alpha + beta * 65, 2), cex=1.2, col="blue")
text(60, alpha + beta * 70 + 0.05, round(alpha + beta * 70, 2), cex=1.2, col="blue")

# Beta 1 example b
plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]",  panel.first = grid(), xlim=c(60, 70), ylim=c(2.8, 3.2))
abline(lm(plasma~weight), col="red") # regression line
abline(h = alpha + beta * 65, col = "blue",  lty = 3)
abline(h = alpha + beta * 66, col = "blue",  lty = 3)
segments(x0=65, y0=alpha + beta * 65, x1=66, y1=alpha + beta * 65, col="blue")
segments(x0=66, y0=alpha + beta * 65, x1=66, y1=alpha + beta * 66, col="blue")
text(67, 2.94, expression(beta), cex=1.2, col="blue")
text(61, alpha + beta * 65 - 0.02, round(alpha + beta * 65, 2), cex=1.2, col="blue")
text(61, alpha + beta * 66 + 0.02, round(alpha + beta * 66, 2), cex=1.2, col="blue")

```

*Increasing weight by 5 kg corresponds to* $3.14 - 2.92 = 0.22$ increase in plasma volume. Increasing weight by 1 kg corresponds $2.96 - 2.92 = 0.04$ increase in plasma volume

## Intercept

$plasma = 0.0857 + 0.0436 * weight$

Linear regression gives us estimates of model coefficient $Y_i = \alpha + \beta x_i + \epsilon_i$

```{r}
#| fig-align: center

# Values from regression model: plasma_volume = 0.0857 + 0.043615*x
par(mfcol=c(1,2))

# Fitted line
plot(weight, plasma, cex.main = my.s, cex.main=my.s, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]",  panel.first = grid())
abline(lm(plasma~weight), col="red") # regression line
text(65, 3.3, "plasma = 0.0857 + 0.0436 * weight", cex=1)

# Beta 0 example a
plot(weight, plasma, pch=19, las=1, xlab = "body weight [kg]", ylab="plasma volume [l]",  panel.first = grid(), xlim=c(-20, 80), ylim=c(0, 5))
abline(lm(plasma~weight), col="red") # regression line
abline(h=alpha, col="blue", lty = 3) # regression line
segments(x0=65, y0=2.92, x1=66, y1=2.92, col="blue")
segments(x0=66, y0=2.92, x1=66, y1=2.964, col="blue")
text(15, 0.4, expression(alpha), cex=1.2, col="blue")
text(33, 0.5, paste("= ", round(alpha,3), sep=""), cex=1.2, col="blue")

```

*Intercept value corresponds to expected outcome when the explanatory variable value equals to zero. It is not always meaningful*

## Hypothesis testing {.smaller}

**Is there a relationship between the response and the predictor?**


- The calculated $\hat{\alpha}$ and $\hat{\beta}$ are **estimates of the population values** of the intercept and slope and are therefore subject to **sampling variation**. Their precision is measured by their **estimated standard errors**, `e.s.e`($\hat{\alpha}$) and `e.s.e`($\hat{\beta}$), used in **hypothesis testing**

. . .


**The most common hypothesis test** involves testing the `null hypothesis` of:

-   $H_0:$ There is no relationship between $X$ and $Y$
-   versus the `alternative hypothesis` $H_a:$ there is some relationship between $X$ and $Y$

. . .

**Mathematically**, this corresponds to testing:

-   $H_0: \beta=0$
-   versus $H_a: \beta\neq0$
-   since if $\beta=0$ then the model $Y_i=\alpha+\beta x_i + \epsilon_i$ reduces to $Y=\alpha + \epsilon_i$

. . .

**Under the null hypothesis** $H_0: \beta = 0$ <!-- we have: $$\frac{\hat{\beta}-\beta}{e.s.e(\hat{\beta})} \sim t(n-p)$$  --> ![](images/lm-tstatistics.png)

-   $n$ is number of observations and $p$ is number of model parameters
-   and the `t-statistics` follows Student's t distribution with $n-p$ degrees of freedom


## Vector-matrix notations {.smaller}

::: {#def-vector-matrix-lm}
## vector matrix form of the linear model

The vector-matrix representation of a linear model can be written as $$\mathbf{Y} = \mathbf{X}\boldsymbol\beta + \boldsymbol\epsilon$$

where:

-   $\mathbf{Y}$ is $n \times1$ vector of observations
-   $\mathbf{X}$ is $n \times p$ **design matrix**
-   $\boldsymbol\beta$ is $p \times1$ vector of parameters
-   $\boldsymbol\epsilon$ is $n \times1$ vector of vector of random errors, independent and identically distributed (i.i.d) N(0, $\sigma^2$)

In full, the above vectors and matrix have the form:

$\mathbf{Y}=\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_{n} \end{bmatrix}$ $\boldsymbol\beta=\begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_{p} \end{bmatrix}$ $\boldsymbol\epsilon=\begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_{n} \end{bmatrix}$ $\mathbf{X}=\begin{bmatrix} 1 & x_{1,1} & \dots & x_{1,p} \\ 1 & x_{2,1} & \dots & x_{2,p} \\ \vdots & \vdots & \vdots & \vdots \\ 1 & x_{n,1} & \dots & x_{n,p} \end{bmatrix}$

:::

<br>

. . .

The least squares estimates is given by: $$\hat{\mathbf{\beta}}= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$$

## Linear mixed models

*grouped data*

:::: {.columns}

::: {.column width="50%"}

<br>

- Data often come from grouped, hierarchical or nested structures, such as repeated measurements from the same subjects, patients clustered within hospitals, or plants grouped within plots.
- All such data share the common feature of **correlation of observations within the same group** and linear model analyses assuming independence of the observations become inappropriate. 


:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
```{r}
#| fig-width: 6
#| fig-height: 6

simulate_grouped_trend <- function(group_count = 5, points_per_group = 10, global_slope = -10, global_intercept = 30, group_slope = 2, noise_sd = 50) {
  set.seed(123) # Setting a seed for reproducibility
  
  # Initialize an empty data frame to store the simulated data
  data <- data.frame(x = numeric(), y = numeric())
  
  # Loop to create each group
  for (i in 1:group_count) {
    x_start <- 12 + (i - 1) * (10 / group_count) # Stagger the start of x for each group
    x <- runif(points_per_group, min = x_start, max = x_start + (10 / group_count))
    
    # Apply a local positive trend within the group, but maintain the global negative trend
    local_intercept <- global_intercept + global_slope * (x_start + (10 / (2 * group_count))) + rnorm(1, mean = 0, sd = noise_sd)
    y <- local_intercept + group_slope * (x - x_start) + rnorm(points_per_group, mean = 0, sd = noise_sd)
    
    # Combine this group with the overall dataset
    group_data <- data.frame(x = x, y = y,group=i)
    data <- rbind(data, group_data)
  }
  
  return(data)
}

# generate simulated data
data_int <- simulate_grouped_trend(group_count = 4,points_per_group = 10,global_slope = -2,global_intercept = 100,group_slope = 4,noise_sd = 5)

# plot the data
#plot(data_int$x,data_int$y,xlab="Age",ylab="Protein expression")

# Plot the data with different colors for each group
plot(data_int$x, data_int$y, xlab="Age", ylab="Protein expression", main="", col=data_int$group)

# Add a linear regression line
abline(lm(y ~ x, data = data_int), col="red")

# Function to draw ellipse
draw_ellipse <- function(x, y, level = 0.95, col){
  # Calculate the means of x and y
  xbar <- mean(x)
  ybar <- mean(y)
  
  # Calculate the standard deviations
  std_x <- sd(x)*2
  std_y <- sd(y)*2
  
  # Calculate the correlation
  correlation <- cor(x, y)
  
  # Create a sequence of angles
  t <- seq(0, 2*pi, length.out = 100)
  
  # Calculate the ellipse points
  a <- std_x * sqrt(1 + correlation)
  b <- std_y * sqrt(1 - correlation)
  ellipse_x <- xbar + a * cos(t)
  ellipse_y <- ybar + b * sin(t)
  
  # Draw the ellipse
  lines(ellipse_x, ellipse_y, col=col, lwd=2)
}

# Draw ellipses for each group
unique_groups <- unique(data_int$group)
colors <- rainbow(length(unique_groups))
for (group in unique_groups) {
  group_data <- data_int[data_int$group == group, ]
  draw_ellipse(group_data$x, group_data$y, col=group)
}
```
:::

::::


## Linear mixed models

*mixed: fixed and random effects*

<br>

::: incremental

- A **fixed effect** is an unknown constant that we try to estimate from the data
  - These are the direct relationships between your chosen variables and the outcome we are interested in. Fixed effects are the factors you control or are primarily interested in, e.g. when studying the effect of age on protein expression, age is a fixed effect. 
- A **random effect** is convenient way to model grouping structure. A random effect is a random variable, and we try to **estimate the parameters that describe the distribution of this random effect**. 
  - For instance, consider an experiment to investigate the effects of several drug treatments on a sample of patients. Typically, we are interested in specific drug treatments and so we would treat the drug effects as fixed. However, it makes sense to treat the patients effects as random. 

:::

# Mathematical details{.smaller}

# Examples in R {.smaller}

# Q&A session


